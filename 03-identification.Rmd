---
output:
  pdf_document: default
  html_document: default
---

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(knitr)
library(dplyr)
library(tidyr)
proj_dir = getwd()
knitr::opts_chunk$set(fig.pos = 'H')
options(digits=3)
```

# Model Identification {#identification}

This chapter provides the necessary identification conditions for the learning through practice (LTP) model and the sufficient identification conditions for the Bayesian Knowledge Tracing (BKT) model. Because the LTP model admits an arbitrary number of states for latent mastery and observed responses, the necessary identification conditions serve as an upper limit on the model complexity.

Define identification as the following [@greene2003econometric]:

```{definition}
The parameter vector $\theta$ is identified, if for any other parameter vector $\theta^*$, for some data, $L(\theta^*|y)\neq L(\theta|y)$.
```


## The Literature Review

Because the LTP model is an application of the hidden Markov model (HMM), if identification conditions for the HMM are established, identification conditions of the LTP model can be derived from it. Ephraim and Merhav [-@ephraim2002hidden] provided a literature review of researches on model identification, from which emerges two promising perspectives. The first perspective of proof comes from Leroux's observation [-@leroux1992maximum] that the likelihood of HMM can be viewed as a finite mixture of the product densities. He proved that the HMM is identified under loose conditions if the observed response is drawn from Poisson distribution, Gaussian distribution with fixed variance, Exponential distribution, and negative Exponential distribution. The second perspective of proof comes from Ito et al[-@ito1992identifiability], who took an algebraic approach to study the equivalence of different discrete Markov processes. Their approach requires the mapping between the latent variable and the observed variable to be a proper function, i.e. one latent state can only map to one value of the observed variable. Because observed responses of the LTP model follow the multinomial distribution, Leroux's theorem does not apply. Because the mapping between the latent mastery and the observed response of the LTP model is not a proper function, Itoâ€˜s result does not apply. In short, both perspectives are not conducive to identification proof of the LTP model.

As for the literature on the Bayesian Knowledge Tracing model, the identification (or lack thereof) is an important topic. Beck and Chang [-@beck2007identifiability] first raised the issue of model identification after correctly observing that multiple parameter sets with very different interpretations of the learning process lead to an identical learning curve. They subsequently (incorrectly) concluded that the BKT model is not identified. This conclusion was later echoed by Pardos and Hefferman [-@pardos2010navigating] and Van De Sande [-@van2013properties]. The "lack" of identifiability is one of reasons for the literature to move toward individualized model parameters [@d2008more;@pardos2010modeling]. However, the identification conditions of the individualization strategy are never formally proved. In fact, it is a counter-intuitive strategy because more, not fewer, parameters are introduced to solve the identifiability problem with the same dataset and essentially the same structural representation. Rai et al [-@rai2009using] introduce the Dirichlet prior to confining the EM algorithm within a certain parameter space. This is a reasonable strategy if the prior is correctly specified, which is not easy to guarantee. In short, the identification of the BKT model is not properly addressed in the learning analytics literature.

This chapter attempts to study the identification of the LTP model from a new perspective by using the moments of the joint distribution of item assignments, observed responses, stop decisions, and effort decisions. This identification strategy is inspired by Blackwell and Koopmans's work [-@blackwell1957identifiability] to prove the identification for a special hidden Markov model. In addition, the perspective connects the identification of the LTP model to the literature on the identification of Generalized Method of Moments [@hansen1982large].


## Sufficient Statistics of the LTP Model

This section reparametrizes data generating proces assumed by the LTP model as a multinomial distribution. The sample frequencies of the joint variable are sufficient statistics for the multinomial distribution. By drawing the analogy, it is can be proven that the sample frequencies of the joint distribution of item assignments, observed responses, stop decisions, and effort decisions are the sufficient statistics of the LTP model.

To review, $\mathbf{A}$, is the joint item assignments of the sequence. $\mathbf{Y}$ is the joint responses. $\mathbf{H}$ is the joint stop decisions. $\mathbf{E}$ is the joint effort decisions. $\mathbf{a},\mathbf{y},\mathbf{h},\mathbf{e}$ are the realized value. Because $\mathbf{Y},\mathbf{A}, \mathbf{E},\mathbf{H}$ are all discrete variables, the sample space of their joint distribution is countable. Define a mapping function $G(\mathbf{y},\mathbf{a},\mathbf{e},\mathbf{h})=\omega$ that projects each distinct combinations of $\mathbf{Y},\mathbf{A}, \mathbf{E},\mathbf{H}$ to a distinct $\omega$. Define a new random variable $\Omega$ that follows a multinomial distribution whose probability mass function is 

$$
P(\Omega=\omega) = P(\mathbf{Y}=\mathbf{y},\mathbf{A}=\mathbf{a}, \mathbf{E}=\mathbf{e},\mathbf{H}=\mathbf{h})
$$

```{example}
For the BKT model, if only two responses are observed, the mapping function can be defined as

$$
G(Y_1,Y_2)=10Y_1+Y_2
$$

Consequently, $\Omega=\{0,1,10,11\}$. For instances, $P(\Omega=1)=P(Y_1=0,Y_2=1)$,$P(\Omega=10)=P(Y_1=1,Y_2=0)$. 
```

Let $N_{\Omega}$ be the cardinality of the sample space of $\Omega$. Define $n_\omega=\sum_{i=1}^N I(G(\mathbf{y}^i,\mathbf{a}^i,\mathbf{e}^i,\mathbf{h}^i)=\omega)$ where $i$ is the learner id and $N$ is the number of learners.

```{theorem}
$\{ n_1,\dots\,n_{N_{\Omega}}\}$ are sufficient statistics of the joint distribution $(\mathbf{Y},\mathbf{A}, \mathbf{E},\mathbf{H})$ 
```

```{proof}
The total likelihood function is 

$$
L = \prod_{i=1}^NP(\mathbf{Y}=\mathbf{y}^i,\mathbf{A}=\mathbf{a}^i, \mathbf{E}=\mathbf{e}^i,\mathbf{H}=\mathbf{h}^i)
$$

Apply the mapping function, the total likelihood is equivalent to 

$$
\begin{aligned}
L &= \prod_{i=1}^NP(\Omega=G(\mathbf{y}^i,\mathbf{a}^i,\mathbf{e}^i,\mathbf{h}^i))\\
  &= \prod_{\omega=1}^{N_{\Omega}} P(\Omega=\omega)^{\sum_{i=1}^N I(G(\mathbf{y}^i,\mathbf{a}^i,\mathbf{e}^i,\mathbf{h}^i)=\omega)}\\
  &= \prod_{\omega=1}^{N_{\Omega}} P(\Omega=\omega)^{N_{\omega}}
\end{aligned}
$$

Since the likelihood function is expressed as a multinomial distribution, it is easy to see that the sample frequencies of the joint distribution of item assignments, observed responses, stop decisions and effort decisions ($\{n_1,\dots,n_{N_{\Omega}}\}$)  are sufficient statistics.
```


## Identification of the Learning Through Practice Model

In the reparametrized the likelihood, $P(\Omega=\omega)$ is a function of the original model parameters. 

```{example}
Follow the example 3.1, 

$$
\begin{aligned}
P(\Omega=1) &= P(Y_1=0,Y_2=1)\\
&= (1-\pi)(1-\ell)(1-c^{1,0})c^{1,0}+(1-\pi)\ell (1-c^{1,0})c^{1,1} + \pi c^{1,1}(1-c^{1,1})
\end{aligned}
$$
```

Because all other combinations of item assignments, observed responses, stop decisions and effort decisions are a collection of elements in $\Omega$, all other first order moments are linear combinations of $\{n_1,\dots,n_{N_{\Omega}}\}$. Therefore, Theorem 3.1 provides $N_{\Omega}-1$ moment conditions that are nonlinear functions of the parameters. To prove the LTP model is uniquely identified is equivalent to prove that there exists a global unique solution $\theta^*$ to the system of nonlinear equations. Although the global uniqueness cannot be proved for the nonlinear equations with multiple variables, it is possible to prove that a  unique local solution exists.


```{theorem}
The parameters are locally identified only if the number of parameters is smaller than or equal to $N_{\Omega}-1$ and the Jacobian matrix of the moment conditions evaluated at the local optimal solution has full column rank.
```

```{proof}
If the number of parameters is larger than $N_{\Omega}-1$, there are more variables than equations. The system has no unique solution.

If Jacobian matrix evaluated at the optimal solution does not full column rank, it means that for at least one such $\theta^*_j$ that $\frac{\partial P(\Omega=\omega,\theta^*)}{\partial \theta_j^*} =0 \quad \forall \omega$. Therefore, $\theta_j$ has multiple roots and the solution is not unique.
```


### The Practical Implication

Theorem 3.2 puts an upper limit on the maximum number of parameters based on the number of sufficient statistics rather than the number of observations. The number of observations affects the precision of the estimated parameter but not the identification of the parameter. Assume a data set captures three responses for each learner. The analyst tries to set up an LTP model without learner engagement. If there is only one item, there are seven moment conditions. A LTP model with $M_X=2$ and $M_y=2$ (BKT) can be identified because it has four free parameters. A LTP model with $M_X=2$ and $M_y=3$ can be identified because it has six free parameters. A LTP model with $M_x=3$ and $M_y=2$ cannot be identified because it has eight free parameters. If there are two items, but the item sequence is fixed, the number of moment conditions is still seven. Only the BKT model can be identified because it has six free parameters in this case. In contrast, if the sequence order is not fixed for the two item, the number of moment conditions increase sharply to 63! This theorem is useful in practice because the analyst can design the sequence structure needed for identification. 

Another practical implication is that the learning curve is not the sufficient statistics. It should not be the basis of statistical inference. For example, Beck and Chang [-@beck2007identifiability] give three model specifications that generate the same learning curve but different distributions of join responses. Have they looked at the correct sufficient statistics, they would not have concluded that the Bayesian Knowledge Tracing model is not identified. For another example, Murray et al [-@murray2013revealing] are puzzled by an observation that each individual learning curve divided by practice sequence length is upward sloping but the aggregated learning curve is flat. Had they approached the problem from the perspective of this chapter, they would never look at the aggregate learning curve for the evidence of learning in the first place. 

Besides its fundamental importance to identification, the sufficient statistics of the LTP model may also serve as a useful tool for diagnostics. Because the LTP model offers a great latitude in model specifications, a comparison of the predicted sufficient statistics generated by the fitted parameters with the sample sufficient statistics may be revealing about the adequacy of the specification. The author has observed that the BKT model often under predicts the proportion of learners who get all or most of the items wrong. Such pattern suggests that increasing the number of states in the latent mastery would better capture the tail behavior. However, it could be hard to visualize the sufficient statistics in a complex dataset because of its large number of moments, a statistical test of model fitness needs to be developed to perform formal diagnostics.


## Identification of the Bayesian Knowledge Tracing Model

Although sufficient identification conditions cannot be obtained for the LTP model in general, they can be obtained for the Bayesian Knowledge Tracing (BKT) model by solving the system of nonlinear equations analytically.

Theorem 3.1 implies that the BKT model cannot be identified with a sequence of two responses because there are four parameters but only three moment conditions. When the sequence length is longer than or equal to three, there are more moment conditions than variables. Start by the simplest case of a sequence with length three.


```{theorem}
The Bayesian Knowledge Tracing model is identified on practice sequences with length three if $\pi\neq 1$, $0 \leq \ell<1$ and $c^{1,0} \neq c^{1,1}$.
```


```{proof}
Let $p_{ijk} = P(Y_1=i,Y_2=j,Y_3=k)$, $p_{i,j}=P(Y_1=i,Y_2=j)$, and $p_i=P(Y_1=i)$. Let $c_1=c^{1,1}$, $c_0=c^{1,0}$. Excluding $p_{0,0,0}$, the rest seven moment conditions are:

$$
\begin{aligned}
p_{111} &=(1-\pi)(1-\ell)c_0^3+(1-\pi)(1-\ell)\ell c_0^2c_1 + (1-\pi)\ell c_0c_1^2+\pi c_1^3 \\
p_{110} &=(1-\pi)(1-\ell)c_0^2(1-c_0)+(1-\pi)(1-\ell)\ell c_0^2(1-c_1) + (1-\pi)\ell c_0c_1(1-c_1)+\pi c_1^2(1-c_1)\\
p_{101} &=(1-\pi)(1-\ell)c_0^2(1-c_0)+(1-\pi)(1-\ell)\ell c_0(1-c_0)c_1 + (1-\pi)\ell c_0(1-c_1)c_1+\pi c_1^2(1-c_1)\\
p_{011} &=(1-\pi)(1-\ell)c_0^2(1-c_0)+(1-\pi)(1-\ell)\ell (1-c_0)c_0c_1 + (1-\pi)\ell (1-c_0)c_1^2+\pi c_1^2(1-c_1)\\
p_{100} &=(1-\pi)(1-\ell)c_0(1-c_0)^2+(1-\pi)(1-\ell)\ell c_0(1-c_0)(1-c_1) + (1-\pi)\ell c_0(1-c_1)^2+\pi c_1(1-c_1)^2\\
p_{010} &=(1-\pi)(1-\ell)c_0(1-c_0)^2+(1-\pi)(1-\ell)\ell (1-c_0)c_0(1-c_1) + (1-\pi)\ell (1-c_0)(1-c_1)c_1+\pi c_1(1-c_1)^2\\
p_{001} &=(1-\pi)(1-\ell)c_0(1-c_0)^2+(1-\pi)(1-\ell)\ell (1-c_0)^2c_1 + (1-\pi)\ell (1-c_0)(1-c_1)c_1+\pi c_1(1-c_1)^2
\end{aligned}
$$

From these base moments, derive the following moments by marginalizing over nuisance periods. For example $p_{11} = p_{111}+p_{110}$.

$$
\begin{aligned}
p_{11} &= (1-\pi)(1-\ell)c_0^2+(1-\pi)\ell c_0c_1+\pi c_1^2\\
p_{01} &= (1-\pi)(1-\ell)(1-c_0)c_0+(1-\pi)\ell (1-c_0)c_1+\pi (1-c_1)c_1\\
p_{10} &= (1-\pi)(1-\ell)c_0(1-c_0)+(1-\pi)\ell c_0(1-c_1)+\pi c_1(1-c_1)\\
p_1 &= (1-\pi)c_0+\pi c_1
\end{aligned}
$$

With some algebra, it is easy to show that if $\pi \neq 1$, $0 \leq \ell<1$ and $c_1 \neq c_0$, 
$$
\begin{aligned}
c_1 = \frac{p_{101}-p_{011}}{p_{01} - p_{10}}\\
c_0=\frac{p_{110}-p_{101}}{p_{110}-p_{101}+p_{001}-p_{010}}
\end{aligned}
$$

Plug $c_1$ and $c_0$ into $p_1$ to solve for $\pi$ 



$$
\pi = \frac{p_{10}+p_{01}-\frac{p_{110}-p_{101}}{p_{110}-p_{101}+p_{001}-p_{010}}}{\frac{p_{101}-p_{011}}{p_{01} - p_{10}} - \frac{p_{110}-p_{101}}{p_{110}-p_{101}+p_{001}-p_{010}}}
$$

Plug $c_1$, $c_0$ and $\pi$ into any of the equations above to solve for $\ell$. This proof chooses $p_{01}-p_{10}$.
$$
\ell = \frac{p_{01}-p_{10}}{(1-\pi)(c_1-c_0)} = \frac{p_{01}-p_{10}}{\frac{p_{101}-p_{011}}{p_{01} - p_{10}}-(p_{11}+p_{10})}
$$

Now that one solution to the system is found, it is necessary to prove that it is the only solution. $c_1$ and $c_0$ are both solutions to a linear equation with one unknown, therefore they are unique. $\pi$ is also the unique solution to a linear equation with one unknown when $c_1$ and $c_0$ are plugged in. When $c_1$, $c_0$ and $\pi$ are solved, $\ell$ is the unique solution to a linear equation in any equations. In sum, the solution is unique although the representation of the solution is not.


Now consider the special cases wheen the model is not identified.

If $\pi=1$ but $0<\ell<1$, $\ell$ and $c_0$ are not identified because they are never observed.

If $\ell=1$ but $0<\pi<1$, $\pi$ and $c_0$ are not identified because $p_1$ is a linear equation with two unknowns.

If $c_1=c_0$, $\pi$ and $\ell$ are not uniquely identified because the latent variable collapses to one state. 

```


With Theorem 3.3, it is possible to prove

```{theorem}
The Bayesian Knowledge Tracing model is identified if at least three periods of response are observed, $\pi\neq 1$, $0 \leq \ell<1$ and $c^{1,0} \neq c^{1,1}$.
```

```{proof}

The equivalent representation of Theorem 3.4 is that the BKT model is identified if the three-response sequence BKT model is identified.

Assume the BKT model based on sequences of length three is identified, but the model on sequences of length $T$ is not identified. Let m ($m\geq2$) be the size of the observation equivalent parameter sets. The parameters sets are denoted as $\Theta_1, \dots, \Theta_m$. Because that the parameter space is the same for the BKT model on sequences with length three and that with length $T$, $\Theta_1, \dots, \Theta_m$ also generates the observation for the BKT model on sequences with length three. However, it is uniquely identified, therefore $\Theta_1=\dots=\Theta_m=\Theta$, and the BKT model on sequences with length $T$ is identified. 

```



### Revisit Beck&Chang

A revisit of the example in Beck and Chang [-@beck2007identifiability] illustrates the argument in this section. The parameter sets are listed in table 3.1:

```{r,echo=FALSE,warning=FALSE,message=FALSE}

bc_param = data.frame(model=c('Knowledge','Guess','Reading Tutor'), pi=c(0.56,0.36,0.01),l=c(0.1,0.1,0.1),g=c(0.0,0.3,0.53),s=c(0.05,0.05,0.05) )

kable(bc_param,
  booktabs = TRUE,
  align='c',format='pandoc',
  caption = 'BKT Parameters in Beck&Chang (2007)'
)
```


Figure 3.1 plots the learning curves. The x-axis is the sequence position while the y-axis is the average success rate at that position($P(Y_t=1)$). It confirms Beck and Chang's observation that three parameter sets generate essentially the same learning curve.

```{r,echo=FALSE,warning=FALSE,message=FALSE, fig.cap = "The Learning Curves(T=3)",fig.align='center',out.height='8cm',out.width='8cm'}

update_mastery <- function(mastery, learn_rate){
  return (mastery + (1-mastery)*learn_rate)
} 

compute_success_rate <- function(slip, guess, mastery){
  return ( guess*(1-mastery) + (1-slip)*mastery )
}

generate_learning_curve <- function(slip, guess, init_mastery, learn_rate, Tl){
  p = init_mastery
  lc = data.frame(t= seq(1,Tl), ypct = as.numeric(0), xpct=as.numeric(0) )
  
    lc$ypct[1] = compute_success_rate(slip, guess, p)
    lc$xpct[1] = p
    
    for (t in seq(2,Tl)){
        p = update_mastery(p,learn_rate)
        lc$ypct[t] = compute_success_rate(slip, guess, p)     
        lc$xpct[t] = p
    }
    return(lc)
}

lc1 = generate_learning_curve(0.05, 0.0, 0.56, 0.1,T=3)
lc1$model='Knowledge'

lc2 = generate_learning_curve(0.05, 0.3, 0.36, 0.1,T=3)
lc2$model='Guess'

lc3 = generate_learning_curve(0.05, 0.53, 0.01, 0.1,T=3)
lc3$model='Reading Tutor'

lc = rbind(lc1,lc2,lc3)

library(ggplot2)
qplot(data=lc,x=t,y=ypct,col=model,geom='line',linetype=model)+ ylab('P(Yt=1)') +xlab('Number of Practice')

```

Figure 2 shows the sufficient statistics generated by three parameter sets. The x-axis is the joint distribution of the response  ($Y_1=i,Y_2=j,Y_3=k$ as $i,j,k$), the y-axis is the sample proportions ($P(Y_1=i,Y_2=j,Y_3=k)$). Figure 2 clearly shows that hree models generate distinct sufficient statistics and thus have distinct estimated parameters.

```{r,echo=FALSE,warning=FALSE,message=FALSE, fig.cap = "The Sufficient Statistics(T=3)",fig.align='center',out.height='10cm',out.width='10cm'}

generate_sufficient_statistics <- function(c_0,c_1,pi,ell){
    res = data.frame(ys=c('111','110','101','011','100','010','001'),pct=as.numeric(0))
    res$pct[1] =(1-pi)*(1-ell)*c_0^3        +(1-pi)*(1-ell)*ell*c_0^2*c_1           + (1-pi)*ell*c_0*c_1^2          +pi*c_1^3
    res$pct[2] =(1-pi)*(1-ell)*c_0^2*(1-c_0)+(1-pi)*(1-ell)*ell*c_0^2*(1-c_1)       + (1-pi)*ell*c_0*c_1*(1-c_1)    +pi*c_1^2*(1-c_1)
    res$pct[3] =(1-pi)*(1-ell)*c_0^2*(1-c_0)+(1-pi)*(1-ell)*ell*c_0*(1-c_0)*c_1     + (1-pi)*ell*c_0*(1-c_1)*c_1    +pi*c_1^2*(1-c_1)
    res$pct[4] =(1-pi)*(1-ell)*c_0^2*(1-c_0)+(1-pi)*(1-ell)*ell*(1-c_0)*c_0*c_1     + (1-pi)*ell*(1-c_0)*c_1^2      +pi*c_1^2*(1-c_1)
    res$pct[5] =(1-pi)*(1-ell)*c_0*(1-c_0)^2+(1-pi)*(1-ell)*ell*c_0*(1-c_0)*(1-c_1) + (1-pi)*ell*c_0*(1-c_1)^2      +pi*c_1*(1-c_1)^2
    res$pct[6] =(1-pi)*(1-ell)*c_0*(1-c_0)^2+(1-pi)*(1-ell)*ell*(1-c_0)*c_0*(1-c_1) + (1-pi)*ell*(1-c_0)*(1-c_1)*c_1+pi*c_1*(1-c_1)^2
    res$pct[7] =(1-pi)*(1-ell)*c_0*(1-c_0)^2+(1-pi)*(1-ell)*ell*(1-c_0)^2*c_1       + (1-pi)*ell*(1-c_0)*(1-c_1)*c_1+pi*c_1*(1-c_1)^2     
    return(res)
}

ss1 = generate_sufficient_statistics(0.0, 0.95, 0.56, 0.1)
ss1$model='Knowledge'

ss2 = generate_sufficient_statistics(0.3, 0.95, 0.36, 0.1)
ss2$model='Guess'

ss3 = generate_sufficient_statistics(0.53, 0.95, 0.01, 0.1)
ss3$model='Reading Tutor'

ss = rbind(ss1,ss2,ss3)

library(ggplot2)
ggplot(data=ss, aes(x=ys,y=pct, fill=model))+ geom_bar(stat = "identity",position="dodge") + xlab('Y1,Y2,Y3') + ylab('P(Y1,Y2,Y3)')
```


Table 3.2 shows the fitted pedagogical efficacy from simulation by the Monte Carlo Markov Chain algorithm introduced in the next chapter. The MCMC algorithm initiates four chains from random starting points so as to show that convergence does not depend on the initial guess. The tables report the mean and the 95% credible interval of the posterior parameter distribution.   

The "Knowledge" model and the "Guess" model converge to the true value when the sequence length is only three, which confirms the Theorem 3.4 and disproves Beck&Chang's claim that the BKT model cannot distinguish between the two. The "Read Tutor" model fails to converge until sequences of length five. Because the initial mastery probability is close to zero, the realized number of observations who have mastery at $t=1$ can be far from the expected value in one simulation due to the large variance. It results in slow bayesian learning when the degree of freedom is small because of the short sequence length.


```{r,echo=FALSE,warning=FALSE,message=FALSE}
# load the fitted parameters
seq_ids = c(3,4,5)
model_ids = c(0,1,2)
is_init = FALSE
for (sid in seq_ids){
  for (mid in model_ids){
    if ((sid!=3)&mid!=2){
      next
    }
    
    tmp_data = read.table(paste0(proj_dir, "/_data/01/",as.character(sid),'_',as.character(mid),'.txt'),
                          header=FALSE,sep=',',col.names=c('l','pi','c0','c1'))
    tmp_data$pi = 1-tmp_data$pi
    tmp_data$c1 = 1-tmp_data$c1
    tmp_data = tmp_data %>% gather(param,val)
    tmp_data$seq=sid
    tmp_data$model=mid
    if (!is_init){
      param_data = tmp_data
      is_init = TRUE
    }else{
      param_data = rbind(param_data, tmp_data)
    }
  }
}

param_data$model=factor(param_data$model, levels=c(0,1,2), labels=c('Knowledge','Guess','Read Tutor'))

# summarize 
param_stat = param_data %>% group_by(model,seq,param) %>% summarize(lower=quantile(val,0.05),mval=mean(val),upper=quantile(val,0.95))

# now print for learning parameters
kable(
  merge(data.frame(model=c('Knowledge','Guess','Read Tutor'),true_val=c(0.1,0.1,0.1)),
        param_stat %>% filter(param=='l') %>% select(-param)
        ) %>%  select(model,seq,true_val,mval,lower,upper),
  booktabs = TRUE,
  col.names=c('Model','Sequence length','True','Mean','95% CI(L)','95% CI(H)'),
  align='c',format='pandoc',
  caption = 'Estimated Efficacy'
)
```


Table 3.3-3.5 shows the estimated parameters for the initial mastery probability ($\pi$), the slip rate($g$) and the slip rate ($s$). The parameters of the "Knowledge" model and the "Guess" model are well identified with sequence of length three, but the "Read Tutor" model still does not converge to the true value with sequence of length five.

```{r,echo=FALSE,warning=FALSE,message=FALSE}
kable(
  merge(data.frame(model=c('Knowledge','Guess','Read Tutor'),true_val=c(0.56,0.36,0.01)),
        param_stat %>% filter(param=='pi') %>% select(-param)
        ) %>%  select(model,seq,true_val,mval,lower,upper),
  booktabs = TRUE,
  col.names=c('Model','Sequence length','True','Mean','95% CI(L)','95% CI(H)'),
  align='c',format='pandoc',
  caption = 'Estimated Initial Mastery'
)

kable(
  merge(data.frame(model=c('Knowledge','Guess','Read Tutor'),true_val=c(0.0,0.3,0.53)),
        param_stat %>% filter(param=='c0') %>% select(-param)
        ) %>%  select(model,seq,true_val,mval,lower,upper),
  booktabs = TRUE,
  col.names=c('Model','Sequence length','True','Mean','95% CI(L)','95% CI(H)'),
  align='c',format='pandoc',
  caption = 'Estimated Guess Rate'
)

kable(
  merge(data.frame(model=c('Knowledge','Guess','Read Tutor'),true_val=c(0.05,0.05,0.05)),
        param_stat %>% filter(param=='c1') %>% select(-param)
        ) %>%  select(model,seq,true_val,mval,lower,upper),
  booktabs = TRUE,
  col.names=c('Model','Sequence length','True','Mean','95% CI(L)','95% CI(H)'),
  align='c',format='pandoc',
  caption = 'Estimated Slip Rate'
)

```



## Auxiliary Identification Assumptions

### Item Exogeneity

Assume that the item sequence is chosen exogenously, which renders the item assignment independent of the learner type, the latent mastery, responses, effort choices, and stop decisions. $\mathbf{A} \perp\!\!\!\perp \mathbf{Y}, \mathbf{E}, \mathbf{H}, Z,\mathbf{X}$ 

### Label Switching

Label switching is a common problem in the mixture model. Consider a general mixture model with K components $(C_1,\dots,C_K)$, each associated with a parameter set $\Theta_{C_1},\dots,\Theta_{C_K}$. Because the labels of the components are arbitrary, permutation of the labels produces identical likelihood. 

Rank order condition is one solution to the label switching problem. The non-regressive state assumption is not sufficient to prevent label switching, even for the two-state case. When $M_X=2, M_Y=2$, it is conventional[@corbett1994knowledge] to assume that the correct rate is positively correlated with the mastery.

$$P(Y_t=1|X_t=0) < P(Y_t=1|X_t=1) \leftrightarrow c^{1,0} < c^{1,1}$$

When $M_Y=2$, the previous rank order conditions can be generalized as 
$$P(Y_t=1|X_t=m) < P(Y_t=1|X_t=n) \quad\forall m<n$$

When $M_Y>2$, there is no straightforward way to impose rank order conditions. Therefore, additional assumptions on the learning process are required to properly identify the model. For example, when $M_X=3, M_Y=3$,
the zone of proximal development implies the following assumptions: the lowest mastery never produces a fully correct response because the question is beyond their development stage. The highest mastery never produces a fully incorrect response because they can answer the question independently. $c_j^{2,0}=c_j^{0,2}=0$.

As for the learner type, assume that the type with higher value also has higher initial mastery. To wit:

$$P(X_1=1|Z=w)<P(X_1=1|Z=v) \quad \forall w<v$$

