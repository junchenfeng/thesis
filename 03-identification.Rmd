---
output:
  pdf_document: default
  html_document: default
---

```{r, echo=FALSE}
library(knitr)
library(dplyr)
library(tidyr)
proj_dir = getwd()
options(digits=3)
```

# Model Identification {#identification}

This chapter provides the necessary identification conditions for the general learning model and the sufficient and necessary identification conditions for the bayesian knowledge tracing model as a special case. Because the general learning model admits an arbitrary number of states in the latent mastery and the observed response, the necessary identification conditions serve as the upper limit on the model complexity.

Define identification as the following [@greene2003econometric]:

```{definition}
The parameter vector $\theta$ is identified, if for any other parameter vector $\theta^*$, for some data, $L(\theta^*|y)\neq L(\theta|y)$.
```


## The Literature Review

Because the general learning model is an application of the hidden Markov model(HMM), if the identification conditions for the hidden Markov model are established, the identification conditions of the general learning model can be derived from it. Ephraim and Merhav[-@ephraim2002hidden] provided a summary of identification research in the HMM literature, from which emerges two promising pathways. The first path of proof comes from Leroux[-@leroux1992maximum]'s observation that the likelihood of HMM can be viewed as a finite mixture of the product densities. He proves that the HMM is identified under loose conditions if the observed response draws from Poisson, Gaussian with fixed variance, Exponential, and negative Exponential distribution. The second path of proof comes from Ito et al[-@ito1992identifiability] takes an algebraic approach to study the equivalence of different discrete Markov processes. However, their approach requires the mapping of the latent variable and the observed variable to be a proper function, i.e. one latent state can only map to one observed response. Because the general learning model's observed response follows the multinomial distribution, Leroux's theorem does not apply. Because the general learning model's mapping between the latent mastery and the observed response is not a proper function, Ito et al's result does not apply. 

As for the Bayesian knowledge tracing model, the identification (or lack thereof) is also an important topic. Beck and Chang[-@beck2007identifiability] first raised the issue of model identification after correctly observing that multiple parameter sets with very different practical interpretation lead an identical learning curve. They subsequently incorrectly conclude that the BKT model is not identified. This conclusion is later echoed in Pardos and Hefferman[-@pardos2010navigating] and Van De Sande[-@van2013properties]. The "lack" of identifiability is one of reasons for the literature to move toward individualized model parameters[@d2008more;@pardos2010modeling]. However, the identification conditions of the individualization strategy are never formally proved. In fact, it is a counter-intuitive strategy because more, not less, parameters are introduced to solve the identifiablity problem with the same dataset and essentially the same model framework. Rai et al[-@rai2009using] introduce the dirichlet prior to confining the EM algorithm into certain parameter space. This is a reasonable strategy if the prior is correctly specified, which is not easy to guarantee. In short, the model identification is never properly addressed in the learning analytics literature.


The chapter attempts to provide a new perspective in the identification proof of the general learning model by using the moments of the joint distribution of the observed responses. The identification strategy is first used in Blackwell and Koopmans[-@blackwell1957identifiability]'s proof of identification for a special hidden Markov model. More importantly, it can borrow the identification intuition from the literature on Generalized Method of Moments[@hansen1982large].


## The Sufficient Statistics of the General Learning Model

This section reparametrizes the learning model as a multinomial distribution. The sample frequency of the joint variables are sufficient statistics for the multinomial distribution. By drawing the analogy, it is can be proven that the sample frequency of the joint responses are the sufficient statistics of the general learning model.


Because $\mathbf{Y},\mathbf{A}, \mathbf{E},\mathbf{H}$ are all discrete variables, the sample space of their joint distribution is countable. Define a mapping function $G(\mathbf{y},\mathbf{a},\mathbf{e},\mathbf{h})=\omega$ that projects each distinct combinations of $\mathbf{Y},\mathbf{A}, \mathbf{E},\mathbf{H}$ to a distinct $\omega$. Define a new random variable $\Omega$ that follows a multinomial distribution whose probability mass function is 

$$
P(\Omega=\omega) = P(\mathbf{Y}=\mathbf{y},\mathbf{A}=\mathbf{a}, \mathbf{E}=\mathbf{e},\mathbf{H}=\mathbf{h})
$$

```{example}
For the special case of BKT model, if only two responses are observed for the population. One can define the mapping function as

$$
G(Y_1,Y_2)=10Y_1+Y_2
$$
Consequently, $\Omega=\{0,1,10,11\}$ where $P(\Omega=1)=P(Y_1=0,Y_2=1)$.
```

Let $N_D$ be the cardinality of the sample space of $\Omega$. Define $n_\omega=\sum_{i=1}^N I(G(\mathbf{y}^i,\mathbf{a}^i,\mathbf{e}^i,\mathbf{h}^i)=\omega)$ where $i$ is the learner id, and $N$ is the number of learners.

```{theorem}
$\{ n_1,\dots\,n_{N_D}\}$ are sufficient statistics of the joint distribution $(\mathbf{Y},\mathbf{A}, \mathbf{E},\mathbf{H})$ 
```

```{proof}
Because IA 1, the total likelihood function is 

$$
L = \prod_{i=1}^NP(\mathbf{Y}=\mathbf{y}^i,\mathbf{A}=\mathbf{a}^i, \mathbf{E}=\mathbf{e}^i,\mathbf{H}=\mathbf{h}^i)
$$

Apply the mapping function, the total likelihood is equivalent to 

$$
\begin{aligned}
L &= \prod_{i=1}^NP(\Omega=G(\mathbf{y}^i,\mathbf{a}^i,\mathbf{e}^i,\mathbf{h}^i))\\
  &= \prod_{\omega=1}^{N_D} P(\Omega=\omega)^{\sum_{i=1}^N I(G(\mathbf{y}^i,\mathbf{a}^i,\mathbf{e}^i,\mathbf{h}^i)=\omega)}\\
\end{aligned}
$$

Since the likelihood function is expressed as a multinomial distribution, it is easy to see that the counts $\{n_1,\dots,n_{N_D}\}$ where $n_{\omega}=\sum_{i=1}^N I(G(\mathbf{y}^i,\mathbf{a}^i,\mathbf{e}^i,\mathbf{h}^i)=\omega)$ are sufficient statistics.
```




## Model Identification

In the reparametrized, the likelihood, $P(\Omega=\omega)$ is a function of the original model parameters. 

```{example}
Follow the example 2.1, 

$$
\begin{aligned}
P(\Omega=1) &= P(Y_1=0,Y_2=1)\\
&= (1-\pi)(1-\ell)(1-c^{1,0})c^{1,0}+(1-\pi)\ell (1-c^{1,0})c^{1,1} + \pi c^{1,1}(1-c^{1,1})
\end{aligned}
$$
```

Because all other first order moments of the responses are linear combinations of these moments, Theorem 2.1 provides $N_D-1$ moment conditions that are nonlinear functions of the parameters. To prove the general learning in uniquely identified is equivalent to prove that there exits a global unique solution $\theta^*$ to the system of nonlinear equations. The global uniqueness cannot be proved for the nonlinear equations with multiple variables. However, it is possible to prove that there exists a local unique solution. 


```{theorem}
The parameters are locally identified if the number of parameters is smaller than or equal to $N_D-1$ and the Jacobian matrix at $\theta^*$ has full column rank.
```

```{proof}
If the number of parameters is larger than $N_D-1$, there are more variables than equations. The system has no unique solution.

If the number of parameters is no larger than $N_D-1$, but the Jacobian matrix does not full column rank. It means that for at least one such $\theta^*_j$ that $\frac{\partial P(\Omega=\omega,\theta^*)}{\partial \theta_j^*} =0 \quad \forall \omega$. Therefore $\theta_j$ has multiple roots and the solution is not unique.
```

Theorem 2.2 is only a necessary condition for the model identification. To show this point, consider a mixture model.
**Add in**



### The Practical Implication

Theorem 2.2 puts an upper limit on the maximum number of parameters of the system based on the number of sufficient statistics rather than the number of observations. The number of observations affect the precision of the estimated parameter but not the uniqueness of the parameter. Assume the data captures three responses for each learner. The analyst tries to set up a response-only learning model. If there is only one item, there are 7 momement conditions. $M_X=2$,$M_y=2$ (BKT) can be identified because it has 4 free parameters. $M_X=2$,$M_y=3$ can be identified because it has 6 free parameters. $M_x=3$,$M_y=2$ cannot be identified because it has 8 free parameters. If there are two items, but the item sequence is fixed, the number of moment conditions is still 7. Only the $M_x=2$,$M_2$ model can be identified because it has 6 free parameters. In contrast, if there are two items, but the item sequence is not fixed, the number of moment conditions explodes to 63! This observation is useful in practice because now the analyst can design the sequence structure needed for identification. 

Another practical implication is that the learning curve is not the sufficient statistics and occupies no particular importance in the statistical inference. It may help the analyst to visualize the learning speed but it cannot serve as the basis for inference. For example, Beck and Chang[-@beck2007identifiability] gives three model specification that generates the same learning curve but different distribution of join responses. If they look at the correct sufficient statistics, they would not have concluded that the Bayesian Knowlede Tracing model is not identified. For another example, Murray et al[-@murray2013revealing] notice that although the aggregated learning curve is flat, each individual learning curve divided by practice sequence length is upward sloping. Had they approached the problem from the perspective of this chapter, they would never looked at the aggregate learning curve for the evidence of learning in the first place. 

Besides its usefulness to inference, the sufficient statistics are also useful for diagnostics. Because the assumptions that justify the general learning model is too strong in many circumstances, A comparison of the fitted sufficient statistics with the sample sufficient statistics may be revealing about the defect in the model. The author has observed that the BKT model often under predicts the proportion of learners that got all or most of the items wrong. Such pattern may suggest the necessity to increase the number of latent states to better capture the tail behavior.  



## Identification of the Bayesian Knowledge Tracing Model

Although sufficient identification condition cannot be obtained for the general learning model, it is possible to derive sufficient identification conditions for specific models by solving the system of nonlinear equations explicitly.

From Theorem 3.1, the Bayesian Knowledge Tracing(BKT) model cannot be identified with two observations because there are four parameters but only three moment conditions. When the sequence length is larger or equal to three, there are more moment conditions than variables. Start by the simplest case of the three responses sequence.


```{theorem}
The Bayesian Knowledge Tracing model with three-response sequence is identified if $\pi\neq 1$, $0 \geq \ell<1$ and $c^{1,0} \neq c^{1,1}$.
```


```{proof}
Let $p_{ijk} = P(Y_1=i,Y_2=j,Y_3=k)$, $p_{i,j}=P(Y_1=i,Y_2=j)$. Let $c_1=c^{1,2}$, $c_0=c^{1,1}$. Excluding $p_{0,0,0}$, the rest seven moment conditions are:

$$
\begin{aligned}
p_{111} &=(1-\pi)(1-\ell)c_0^3+(1-\pi)(1-\ell)\ell c_0^2c_1 + (1-\pi)\ell c_0c_1^2+\pi c_1^3 \\
p_{110} &=(1-\pi)(1-\ell)c_0^2(1-c_0)+(1-\pi)(1-\ell)\ell c_0^2(1-c_1) + (1-\pi)\ell c_0c_1(1-c_1)+\pi c_1^2(1-c_1)\\
p_{101} &=(1-\pi)(1-\ell)c_0^2(1-c_0)+(1-\pi)(1-\ell)\ell c_0(1-c_0)c_1 + (1-\pi)\ell c_0(1-c_1)c_1+\pi c_1^2(1-c_1)\\
p_{011} &=(1-\pi)(1-\ell)c_0^2(1-c_0)+(1-\pi)(1-\ell)\ell (1-c_0)c_0c_1 + (1-\pi)\ell (1-c_0)c_1^2+\pi c_1^2(1-c_1)\\
p_{100} &=(1-\pi)(1-\ell)c_0(1-c_0)^2+(1-\pi)(1-\ell)\ell c_0(1-c_0)(1-c_1) + (1-\pi)\ell c_0(1-c_1)^2+\pi c_1(1-c_1)^2\\
p_{010} &=(1-\pi)(1-\ell)c_0(1-c_0)^2+(1-\pi)(1-\ell)\ell (1-c_0)c_0(1-c_1) + (1-\pi)\ell (1-c_0)(1-c_1)c_1+\pi c_1(1-c_1)^2\\
p_{001} &=(1-\pi)(1-\ell)c_0(1-c_0)^2+(1-\pi)(1-\ell)\ell (1-c_0)^2c_1 + (1-\pi)\ell (1-c_0)(1-c_1)c_1+\pi c_1(1-c_1)^2
\end{aligned}
$$

From these base moments, derivative moments can be generated. For example $p_{11} = p_{111}+p_{110}$.

$$
\begin{aligned}
p_{11} &= (1-\pi)(1-\ell)c_0^2+(1-\pi)\ell c_0c_1+\pi c_1^2\\
p_{01} &= (1-\pi)(1-\ell)(1-c_0)c_0+(1-\pi)\ell (1-c_0)c_1+\pi (1-c_1)c_1\\
p_{10} &= (1-\pi)(1-\ell)c_0(1-c_0)+(1-\pi)\ell c_0(1-c_1)+\pi c_1(1-c_1)\\
p_1 &= (1-\pi)c_0+\pi c_1
\end{aligned}
$$

With some algebra, it is easy to show that if $\pi \neq 1$, $0\geq \ell<1$ and $c_1 \neq c_0$, 
$$
\begin{aligned}
c_1 = \frac{p_{101}-p_{011}}{p_{01} - p_{10}}\\
c_0=\frac{p_{110}-p_{101}}{p_{110}-p_{101}+p_{001}-p_{010}}
\end{aligned}
$$

Plug in $c_1$ and $c_0$ into $p_1$ to solve for $\pi$ 

$$
\pi = \frac{p_{10}+p_{01}-\frac{p_{110}-p_{101}}{p_{110}-p_{101}+p_{001}-p_{010}}}{\frac{p_{101}-p_{011}}{p_{01} - p_{10}} - \frac{p_{110}-p_{101}}{p_{110}-p_{101}+p_{001}-p_{010}}}
$$

Plug the previous variables into any of the equations to solve for $\ell$. This proof chooses $p_{01}-p_{10}$.
$$
\ell = \frac{p_{01}-p_{10}}{(1-\pi)(c_1-c_0)} = \frac{p_{01}-p_{10}}{\frac{p_{101}-p_{011}}{p_{01} - p_{10}}-(p_{11}+p_{10})}
$$

Now that one solution to the system is obtained, it is necessary to prove that it is the only solution. $c_1$ and $c_0$ are both solutions to a linear equation with one unknown, therefore they are unique. Therefore, $\pi$ is also the unique solution to a linear equation with unknown when $c_1$ and $c_0$ are plugged in. After $c_1$, $c_0$ and $\pi$ are solved, $\ell$ is the unique solution to a linear equation with one unknown when plugging in any equation. In sum, the solution is unique although the representation of the solution is not unique.

Now consider the special cases that the model is not identified:
- If $\pi=1$,$0<\ell<1$, $\ell$ and $c_0$ are not identified because they are never observed.
- If $\ell=1$,$0<\pi<1$, $\pi$ and $c_0$ are not identified because $p_1$ is a linear equation with two unknowns.
- If $c_1=c_0$, $\pi$ and $\ell$ are not uniquely identified because the latent variable collapses to one state. 
```

With theorem 3.2, it is possible to prove the main theorem of the chapter

```{theorem}
The Bayesian Knowledge Tracing model is identified if at least three periods of response are observed, $\pi\neq 1$, $0 \geq \ell<1$ and $c^{1,0} \neq c^{1,1}$.
```

```{proof}

The equivalent representation of theorem 3.3 is that the BKT model is identified if and only if the three-response sequence BKT model is identified.

(1) Necessary condition. If the model for sequence $T$ where $T\geq 3$ is identified, marginalize over $Y_t$ where $t>3$, it implies that the model for sequence 3 is also identified.

(2) Sufficient condition.  Assume the model for sequence 3 is identified, but the model for sequence $T$ is not identified. Let the size of set of parameters that are observation equivalent is m ($m\geq2$): $\Theta_1, \dots, \Theta_m$. Notice that the parameter space is the same for sequence 3 and sequence $T$, $\Theta_1, \dots, \Theta_m$ also generates the observation for sequence 3. However, the model for sequence 3 is uniquely identified, therefore $\Theta_1=\dots=\Theta_m=\Theta$. 

```



### Revisit Beck&Chang

To illustrate the point, let's review the example in Beck and Chang[-@beck2007identifiability]. The parameter sets are the following:

| model    | $\pi$ | $\ell$ | $c_0$ | $c_1$ |
|----------|-------|--------|-------|-------|
| Knowledge| 0.56  |   0.1  | 0.0   | 0.95  |
| Guess    | 0.36  |   0.1  | 0.3   | 0.95  |
| Reading Tutor| 0.01| 0.1  | 0.53  | 0.95  |

Figure 1 plots the learning curves ($P(Y_t=1)$), which confirms Beck and Chang's observation that all three models generate essentially the same learning curve. However, three models generate distinct sufficient statistics. The sufficient statistics for sequence length 3 are shown in the Figure 2.

```{r,echo=FALSE,warning=FALSE,message=FALSE, fig.cap = "The Learning Curves(T=3)",fig.align='center',out.height='8cm',out.width='8cm'}

update_mastery <- function(mastery, learn_rate){
  return (mastery + (1-mastery)*learn_rate)
} 

compute_success_rate <- function(slip, guess, mastery){
  return ( guess*(1-mastery) + (1-slip)*mastery )
}

generate_learning_curve <- function(slip, guess, init_mastery, learn_rate, Tl){
  p = init_mastery
  lc = data.frame(t= seq(1,Tl), ypct = as.numeric(0), xpct=as.numeric(0) )
  
    lc$ypct[1] = compute_success_rate(slip, guess, p)
    lc$xpct[1] = p
    
    for (t in seq(2,Tl)){
        p = update_mastery(p,learn_rate)
        lc$ypct[t] = compute_success_rate(slip, guess, p)     
        lc$xpct[t] = p
    }
    return(lc)
}

lc1 = generate_learning_curve(0.05, 0.0, 0.56, 0.1,T=3)
lc1$model='Knowledge'

lc2 = generate_learning_curve(0.05, 0.3, 0.36, 0.1,T=3)
lc2$model='Guess'

lc3 = generate_learning_curve(0.05, 0.53, 0.01, 0.1,T=3)
lc3$model='Reading Tutor'

lc = rbind(lc1,lc2,lc3)

library(ggplot2)
qplot(data=lc,x=t,y=ypct,col=model,geom='line',linetype=model)+ ylab('P(Yt=1)') 

```




```{r,echo=FALSE,warning=FALSE,message=FALSE, fig.cap = "The Sufficient Statistics(T=3)",fig.align='center',out.height='10cm',out.width='10cm'}

generate_sufficient_statistics <- function(c_0,c_1,pi,ell){
    res = data.frame(ys=c('111','110','101','011','100','010','001'),pct=as.numeric(0))
    res$pct[1] =(1-pi)*(1-ell)*c_0^3        +(1-pi)*(1-ell)*ell*c_0^2*c_1           + (1-pi)*ell*c_0*c_1^2          +pi*c_1^3
    res$pct[2] =(1-pi)*(1-ell)*c_0^2*(1-c_0)+(1-pi)*(1-ell)*ell*c_0^2*(1-c_1)       + (1-pi)*ell*c_0*c_1*(1-c_1)    +pi*c_1^2*(1-c_1)
    res$pct[3] =(1-pi)*(1-ell)*c_0^2*(1-c_0)+(1-pi)*(1-ell)*ell*c_0*(1-c_0)*c_1     + (1-pi)*ell*c_0*(1-c_1)*c_1    +pi*c_1^2*(1-c_1)
    res$pct[4] =(1-pi)*(1-ell)*c_0^2*(1-c_0)+(1-pi)*(1-ell)*ell*(1-c_0)*c_0*c_1     + (1-pi)*ell*(1-c_0)*c_1^2      +pi*c_1^2*(1-c_1)
    res$pct[5] =(1-pi)*(1-ell)*c_0*(1-c_0)^2+(1-pi)*(1-ell)*ell*c_0*(1-c_0)*(1-c_1) + (1-pi)*ell*c_0*(1-c_1)^2      +pi*c_1*(1-c_1)^2
    res$pct[6] =(1-pi)*(1-ell)*c_0*(1-c_0)^2+(1-pi)*(1-ell)*ell*(1-c_0)*c_0*(1-c_1) + (1-pi)*ell*(1-c_0)*(1-c_1)*c_1+pi*c_1*(1-c_1)^2
    res$pct[7] =(1-pi)*(1-ell)*c_0*(1-c_0)^2+(1-pi)*(1-ell)*ell*(1-c_0)^2*c_1       + (1-pi)*ell*(1-c_0)*(1-c_1)*c_1+pi*c_1*(1-c_1)^2     
    return(res)
}

ss1 = generate_sufficient_statistics(0.0, 0.95, 0.56, 0.1)
ss1$model='Knowledge'

ss2 = generate_sufficient_statistics(0.3, 0.95, 0.36, 0.1)
ss2$model='Guess'

ss3 = generate_sufficient_statistics(0.63, 0.95, 0.01, 0.1)
ss3$model='Reading Tutor'

ss = rbind(ss1,ss2,ss3)

library(ggplot2)
ggplot(data=ss, aes(x=ys,y=pct, fill=model))+ geom_bar(stat = "identity",position="dodge") + xlab('Y1,Y2,Y3')
```


The following table shows the fitted parameter from simulated data by the Monte Carlo Markov Chain algorithm introduced in the next chapter. The MCMC algorithm initates 4 chains from random start points. The tables report the mean and the 95% credible interval from the posterior parameter distribution.   

The "Knowledge" model and the "Guess" model converges to the true value when the sequence length is only 3, which confirms the theorm 3.3 and disprove Beck&Chang's claim that the BKT model cannot distinguish the two. The "Read Tutor" model fails to converge until sequence length 5. This may be the result of the probablity of initial mastery is close to the boundary conditions. 


```{r,echo=FALSE,warning=FALSE,message=FALSE}
# load the fitted parameters
seq_ids = c(3,4,5)
model_ids = c(0,1,2)
is_init = FALSE
for (sid in seq_ids){
  for (mid in model_ids){
    if ((sid!=3)&mid!=2){
      next
    }
    
    tmp_data = read.table(paste0(proj_dir, "/_data/01/",as.character(sid),'_',as.character(mid),'.txt'),
                          header=FALSE,sep=',',col.names=c('l','pi','c0','c1'))
    tmp_data$pi = 1-tmp_data$pi
    tmp_data$c1 = 1-tmp_data$c1
    tmp_data = tmp_data %>% gather(param,val)
    tmp_data$seq=sid
    tmp_data$model=mid
    if (!is_init){
      param_data = tmp_data
      is_init = TRUE
    }else{
      param_data = rbind(param_data, tmp_data)
    }
  }
}

param_data$model=factor(param_data$model, levels=c(0,1,2), labels=c('Knowledge','Guess','Read Tutor'))

# summarize 
param_stat = param_data %>% group_by(model,seq,param) %>% summarize(lower=quantile(val,0.05),mval=mean(val),upper=quantile(val,0.95))

# now print for learning parameters
kable(
  merge(data.frame(model=c('Knowledge','Guess','Read Tutor'),true_val=c(0.1,0.1,0.1)),
        param_stat %>% filter(param=='l') %>% select(-param)
        ) %>%  select(model,seq,true_val,mval,lower,upper),
  booktabs = TRUE,
  col.names=c('Model','Sequence length','True','Mean','95% CI(L)','95% CI(H)'),
  align='c',
  caption = 'Estimated Efficacy'
)

kable(
  merge(data.frame(model=c('Knowledge','Guess','Read Tutor'),true_val=c(0.56,0.36,0.01)),
        param_stat %>% filter(param=='pi') %>% select(-param)
        ) %>%  select(model,seq,true_val,mval,lower,upper),
  booktabs = TRUE,
  col.names=c('Model','Sequence length','True','Mean','95% CI(L)','95% CI(H)'),
  align='c',
  caption = 'Estimated Initial Mastery'
)

kable(
  merge(data.frame(model=c('Knowledge','Guess','Read Tutor'),true_val=c(0.0,0.3,0.53)),
        param_stat %>% filter(param=='c0') %>% select(-param)
        ) %>%  select(model,seq,true_val,mval,lower,upper),
  booktabs = TRUE,
  col.names=c('Model','Sequence length','True','Mean','95% CI(L)','95% CI(H)'),
  align='c',
  caption = 'Estimated Guess Rate'
)

kable(
  merge(data.frame(model=c('Knowledge','Guess','Read Tutor'),true_val=c(0.05,0.05,0.05)),
        param_stat %>% filter(param=='c1') %>% select(-param)
        ) %>%  select(model,seq,true_val,mval,lower,upper),
  booktabs = TRUE,
  col.names=c('Model','Sequence length','True','Mean','95% CI(L)','95% CI(H)'),
  align='c',
  caption = 'Estimated Slip Rate'
)

```



## Auxularity Identification Assumptions

### User Homogeneity 

Chapter 2 describes a learning model for an individual. Because the latent state never regresses, practice sequence from any individual may not exhaust all states and state transitions. Therefore, the identification must be performed on repeated observations[@rabiner1989tutorial], or on the population in this context. The identification assumption that justifies applying a model of individual learner to the population data is. The learner's initial latent state mastery is I.I.D draw from the multinomial distribution with probability mass function $P(X_1=k)=\pi_k$.

### Item Exogeneity

Assumes that the item sequence is chosen exogenously. Therefore, the item assignment is independent of the latent mastery, the responses, the effort choice, and the stop choice. $\mathbf{A} \perp\!\!\!\perp \mathbf{Y}, \mathbf{E}, \mathbf{H}, \mathbf{X}$ 

### Label Switching

Label switching is a common problem in the mixture model. Consider a general mixture model with K components $(C_1,\dots,C_K)$, each associated with a parameter set $\Theta_{C_1},\dots,\Theta_{C_K}$. Because the labels of the components are arbitrary, permutation of the labels produces identical likelihood. 

Rank order condition is one solution to the label switching problem. The non-regressive state assumption is not sufficient to prevent label switching, even for the two-state case. When $M_X=2, M_Y=2$, it is conventional[@corbett1994knowledge] to assume that the correct rate is positively correlated with the mastery.

$$P(Y_t=1|X_t=0) < P(Y_t=1|X_t=1) \leftrightarrow c^{1,0} < c^{1,1}$$

When $M_Y=2$, the previous rank order conditions can be generalized as 
$$P(Y_t=1|X_t=m) < P(Y_t=1|X_t=n) \quad\forall m<n$$

When $M_Y>2$, there is no straightforward way to impose rank order conditions. Therefore, additional assumption on the learning process is required to properly identify the model. For example, when $M_X=3, _Y=3$,
the zone of development theory (see 1.4.4) implies the following assumptions: the lowest mastery never produces fully correct response because the question is beyond their development stage. The highest mastery never produces fully incorrect response because they can answer the question independently. $c_j^{2,0}=c_j^{0,2}=0$.
