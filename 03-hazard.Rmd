---
output:
  pdf_document: default
  html_document: default
---

# Selection Bias of the Exit Decision {#exit}

```{r env, echo=FALSE, warning=FALSE, message=FALSE}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(knitr)
proj_dir = getwd()
options(digits=2)
```
```{r func, echo=FALSE, warning=FALSE, message=FALSE}
imputate_hazard_rate <- function(test_data, Tmax){
  alldata = data.frame(t=seq(1,Tmax), hr=as.numeric(0), pc = as.numeric(0), pw = as.numeric(0),Nc=as.numeric(0),Nw=as.numeric(0))
  for (t in seq(1,Tmax)){
    base_num = sum(test_data$t==t)
    exit_num = sum(test_data$t==t & test_data$idx==1)
    base_yes_num = sum(test_data$t==t & test_data$atag==1)
    base_no_num = sum(test_data$t==t & test_data$atag==0)
    exit_yes_num = sum(test_data$t==t & test_data$atag==1 & test_data$idx==1)
    exit_no_num =  sum(test_data$t==t & test_data$atag==0 & test_data$idx==1)
    alldata[t,] = c(t, exit_num/base_num, exit_yes_num/base_yes_num, exit_no_num/base_no_num, base_yes_num, base_no_num)
  }
  alldata =  alldata %>% mutate(sdc=sqrt(pc*(1-pc)/Nc),sdw=sqrt(pw*(1-pw)/Nw))
  
  hr_point = alldata %>% select(t,pc,pw) %>% rename(correct=pc,incorrect=pw) %>% gather(res,h,-t)
  hr_sd = alldata %>% select(t,sdc,sdw) %>% rename(correct=sdc,incorrect=sdw) %>% gather(res,sd_h,-t)
  harzard_rate_data = merge(hr_point,hr_sd,by=c('t','res'))
  return(harzard_rate_data)
}
```



## Motivation
To motivate the discussion of this chapter, consider the following problem. A teacher wants to assign homework on divisions. She can assign one of the two sets of pratices that contain the same number of (interchangeable) items, say $T$. What evidence shall she examine in order to choose the better assignment?

If the teacher has access to te average correct rate at each practice opportunity(the unconditional learning curve), a naive, and intuitive, policy choose the set of quiz that offers the highest observed learning gain, which is defined as the difference in correct rate between the last practice and the first practice. If the observation matrix is the same for both problem set, the classical learning analytics model endorses such policy because the change in the difference of correct rate is a monotone function of the change in the latent mastery.

If the students are obliged to finish all the items with maximum effort, such naive policy may be optimal indeed. However, if the students have the option to stop early, the unconditional difference is not enough to make the decision. Suppose the true pedagogical efficacy for both practice sets are zero, and practice set A induces every student to finish the last item while practice set B only induces the mastered student to finish item (unmastered students just drop out). The unconditional difference in correct rate reports practice set B is superior to practice set A. The higher learning gain faithfully reports the change in the composition of mastery among the learners, but fails to distinguish the change from learning and the change from (dynamic) sample selection.


This chapter analyzes the condition under which the stop decision biases the estimated pedagogical efficacy and how to correct it by the general model. This chapter is organized as the following. Section 3.2 characterizes the selection bias caused by exit decision. Section 3.3 describes how the general model corrects the bias and verify the correction with simulation dataset. Section 3.4 apply the bias correction in a real dataset.

Throughout the chapter, assume there is no effort choice($E_{t} = 1 \quad \forall t$) and there is only one item($J=1$).


## Exit Decision and Dynamic Selection Bias

It is often the case that the observed hazard rate differs conditioning on the responses. However, the industry's common practice is to ignore it. It leads to two critical questions: 

1. When is it OK to ignore the stop decision?
2. If the hazard model is needed, what is the functional form and dependence structure?

The answer to these two questions are:

1. If the stop deicison does not directly depend on the latent state, the item parameters are consistently estimated without the hazard model, if the model is identified. In fact, when the true data generating process is response dependent but the state dependent hazard model is added, the estimated pedagogical efficacy is biased. Vice versa. When the true data generating process has a state dependent hazard model, the responses only model produces a biased pedagogical efficacy. 

2. The nonparametric model (cell estimator) is consistent but the proportial hazard model is efficient if the hazard rate curve is continuous and monotone.

### Selection bias and Dependence Structure

### State dependent hazard model 

In the responses only model, all the differences in the composition of the latent mastery are attributed to the pedagogical efficacy(state transition). However, if the hazard rate differs among states, the previous statement is no longer true because differential attrition also contributes to the change in the latent mastery composition. For example, if the latent mastery is negatively correlated with the hazard rate, the differential attrition produces a larger shift toward the higher mastery than the response-only model, thus the response-only model overestimates the pedagogical efficacy. This is the intuition of the dynamic selection bias.

More formally, consider the conditional state density after observing $\mathbf{Y}_{1,T}$. To observe the sequence, the learner must have survived at $t-1$. It can be proved (in appendix 1) that:

**Lemma 1**: Under state dependent hazard model, $P(X_t=m|\mathbf{Y},H_{t-1}=0)=P(X_t=m|\mathbf{Y})$ if and only if $P(H_{t-1}=0|X_{t-1})=P(H_{t-1}=0)$.

With similar logic, one can argue that 

**Lemma 2**: Under state dependent hazard model, $P(X_t=m,X_{t-1}=n|\mathbf{Y},H_{t-1}=0)=P(X_t=m,X_{t-1}=n|\mathbf{Y})$ if and only if $P(H_{t-1}=0|X_{t-1})=P(H_{t-1}=0)$.

With the help of lemma 1 and lemma 2, it is possible to prove that(in appendix 2):

**Theorem 1**: Under state dependent hazard model, the pedagogical efficacy is not consistently estimated by the response only model.

In the special case where $M_x=2$, it is easy to prove the following:

**Collary 1**: If $h_t^{X=1}<h_t^{X=0}, \hat{\ell}_{ResponseOnly}>\ell$. 



### Response Dependent Hazard model

The most counter-intuitive result in this chapter is that the response-only model produces consistent estimator of the pedagogical efficacy.The key insight is that the pedagogical efficacy is estimated conditional on the joint responses by the classical model, therefore the stop decision, which is a function of the responses, provides no extra information and is canceled when calculating the conditional density.

Following the strategy in the previous subsection, first prove that conditional on $\mathbf{Y}$, the marginal and the joint state density is independent of the stop decision (in Appendix 3).

**Lemma 3**: Under response dependent hazard model, $P(X_t=m|\mathbf{Y},H_{t-1}=0)=P(X_t=m|\mathbf{Y})$

**Lemma 4**: Under response dependent hazard model, $P(X_t=m,X_{t-1}=n|\mathbf{Y},H_{t-1}=0)=P(X_t=m,X_{t-1}=n|\mathbf{Y})$


Thus it is possible to prove that
**Theorem 2**: Under response dependent hazard model, the pedagogical efficacy is consistently estimated by the response only model.

Although it is hard to prove rigorously, as supported by simulation evidence,  the following proposition is intuitively true.

**Proposition 1**: If $h_t^{Y=1}<h_t^{Y=0}, \hat{\ell}_{State Hazard} <\ell$. 

Proposition 1 stands because $h_t^{Y=1}<h_t^{Y=0}$ leads to $\hat{h}_t^{X=1}<\hat{h}_t^{X=0}$ in the state dependent hazard model. By the reverse of collary 1, $\hat{\ell}_{State Hazard} <\hat{\ell}_{ResponseOnly}=\ell$.  Proposition 1 highlights the importance, and the difficulty, to understand the dynamic selection bias conditional on the responses.


## Simulation

This section illustrate the previous section by simulations. The section first demonstrates the response dependent hazard model by a "2 strike rule". The section then demonstrates the selection bias in the state dependent hazard model.

The parameters for the learning process is
$$
\begin{aligned}
M_x&=2\\
M_y&=2\\
P(X_1=1) &= 0.4\\
P(X_t=1|X_{t-1}=0) &= 0.3\\
P(Y_t=1|X_t=1) &= 0.9\\
P(Y_t=1|X_t=0) &= 0.2\\
\end{aligned}
$$


### 2-strike Rule 

Assume that the learners are forced to stop if they accumulates two errors. Thus the hazard rate only depends on the responses. If the response is right, the hazard rate is 0. If the response is wrong, the hazard rate curve is as following

```{r,echo=FALSE, warning=FALSE, message=FALSE}
data_dir = paste0(proj_dir,'/_data/02/sim/')
strike_data = read.table(paste0(data_dir, 'xstrike_data.txt'),sep=',',col.names=c('i','t','S','H','y'))
strike_data = strike_data %>% group_by(i) %>% mutate(S0=lag(S))
transit_cnt = strike_data %>% group_by(S0,S) %>% summarize(n=n())
strike_ell = transit_cnt$n[2]/(transit_cnt$n[1]+transit_cnt$n[2])
hazard_rates = strike_data %>% group_by(y,t) %>% summarize(h=mean(H)) %>% filter(y==0)

kable(hazard_rates %>%ungroup(y) %>% mutate(sequence=t+1) %>% rename(hazard = h) %>% select(sequence,hazard) )

```

Because the discontinuity and non-monotonicity in the hazard rates, the hazard model is estimated with the non-parametric estimator. The following figure shows the posterior distribution of the pedagogical efficacy obtained from the the response-only model, the response dependent hazard model, and the state dependent hazard model. The response-only model and response-dependent hazard model have very similar posterior dsitribution but the state-dependent model does not have a uni-modal posterior distribution.


```{r,echo=FALSE, warning=FALSE, message=FALSE}
strike_no_param = read.table(paste0(data_dir,'strike_no.txt'), header = F, sep=',')
strike_y_param = read.table(paste0(data_dir,'strike_yh.txt'), header = F, sep=',')
strike_x_param = read.table(paste0(data_dir,'strike_xh.txt'), header = F, sep=',')

names(strike_no_param)[1] = 'l'
names(strike_y_param)[1] = 'l'
names(strike_x_param)[1] = 'l'

strike_no_param$model='Response Only'
strike_y_param$model='Response Dependent'
strike_x_param$model='State Dependent'

strike_lrs = rbind(strike_no_param%>%select(model,l), strike_y_param%>%select(model,l), strike_x_param%>%select(model,l))

strike_map =  strike_lrs %>% group_by(model) %>% summarize(l=mean(l))
strike_map = rbind(data.frame(model='True', l=strike_ell), strike_map)
kable(strike_map)
```


### State dependent Attrition 
As for the state dependent attrition, The hazard rate of the mastered learner is 0.6 for any period while that of the unmastered learner is 0.4. 


Because the hazard rate is continuous and monotone, the hazard model is estimated with the proportional hazard model. The following figure shows the posterior distribution of the pedagogical efficacy obtained from the the response-only model, the response dependent hazard model, and the state dependent hazard model. The posterior distribution of the state dependent hazard model produces a MAP estimator close to the true parameter. The posterior distribution of the response-only and the response-dependent hazard model produces MAP estimator that overestimates the true parameter.

```{r,echo=FALSE, warning=FALSE, message=FALSE}
state_data = read.table(paste0(data_dir, 'x_data.txt'),sep=',',col.names=c('i','t','S','H','y'))
state_data = state_data %>% group_by(i) %>% mutate(S0=lag(S))
transit_cnt = state_data %>% group_by(S0,S) %>% summarize(n=n())
state_ell = transit_cnt$n[2]/(transit_cnt$n[1]+transit_cnt$n[2])

state_no_param = read.table(paste0(data_dir,'x_no.txt'), header = F, sep=',')
state_y_param = read.table(paste0(data_dir,'x_yh.txt'), header = F, sep=',')
state_x_param = read.table(paste0(data_dir,'x_xh.txt'), header = F, sep=',')

names(state_no_param)[1] = 'l'
names(state_y_param)[1] = 'l'
names(state_x_param)[1] = 'l'

state_no_param$model='Response Only'
state_y_param$model='Response Dependent'
state_x_param$model='State Dependent'

state_lrs = rbind(state_no_param%>%select(model,l), state_y_param%>%select(model,l), state_x_param%>%select(model,l))

state_map =  state_lrs %>% group_by(model) %>% summarize(l=mean(l))
state_map = rbind(data.frame(model='True', l=state_ell), state_map)

```


## Case Study with a Real Dataset

### The data collection

The data are collected from December 2015 and January 2016 in China. The target learners are mainly first-grade and second-grade schoolchildren. The demographics of the learner population are unknown beyond the grade distribution. The practices are supplemental learning materials that the students can practice on their own initiative. There is no material punishment for low performance or an early exit.


The practices are embeddd in a gamified learning environment, framed as a turn-based role playing game. Each turn, the learner answers a quiz. If he responds correctly, he is rewarded with a small amount of in-game currency; If else, he takes a damage. The game continues until the Health Point of the player drops to zero, all enemies are defeated, or the learner chooses to quit. The learner can withstand 2 or 3 errors before he is forced to quit. The learner needs to correctly answer about 6-9 questions before all enemies in the level are defeated. A screenshot of the interface is shown in figure 3.3

<center>![Figure 3.3: The Gamified Learning Interface](fig/app.png)</center>


Each practice sequence trains ONE knowledge point but with different question forms. For example, if the knowledge point is "Two digit minus one digit", the item can be "13-9" or "37-6". The items treated as if they have identical item parameters. For each knowledge point, there are a sizable but finite set of items in the question bank. If the learner practices long enough to exhaust them all, he encounters questions from the past. 

This chapter chooses two representative knowledge points out of more than 200 candidates:  two digits multiplication(grade 2) and vertical division(grade 3). Appendix 2 details the data cleaning process.  The two items represent "slow learning" and "fast learning" judged by the unconditional learning curve, which is plotted in Figure 3.4.

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.cap = "Empirical Learning Curves of Different Knowledge Points", fig.align='center',out.height='8cm',out.width='8cm'}

kpids =  c('87','138')
kp_names = c('Two Digit Multiplication','Vertical Division')
n = length(kpids)
for (i in seq(n)){
    file_path = paste0(proj_dir,'/_data/02/spell_data_',kpids[i],'.csv')
    tmp_data = read.csv(file_path, col.names=c('spell_id','t','atag','idx'),header=F)  
    tmp_data$kpid = kpids[i]
    if (i==1){
        spell_data = tmp_data
    }else{
        spell_data = rbind(spell_data, tmp_data)
    }
}

spell_data$knowlege_point = factor(spell_data$kpid, levels=kpids, labels=kp_names)

lc_plot = spell_data %>% group_by(knowlege_point, t) %>% summarize(pct=mean(atag)) %>% filter(t<=5)

qplot(data=lc_plot , x=t, y=pct, geom='line', col=knowlege_point, linetype=knowlege_point) + ggtitle('Observed Learning Curve') + ylab('Success Rate') + xlab('Number of Practice Opportunity')

```


The hazard rate curve is plotted in Figure 3.5. **Add an explanation to the graph**. Both knowledge points exhibit differential attrition rates based on the concurrent response($P(H_t=1|H_{t-1}=0,Y_t=k)$). In both cases, the hazard rate is monotone, incorrect responses leads to higher attrition. However, there are some visible dicontinuity. Hazard rates for incorrect response increases faster in the second and third practices, which may be the result of the X-strike rule.

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.cap = "Empirical Hazard Rates of Different Knowledge Points", fig.align='center',out.height='8cm',out.width='8cm'}
#Check the hazard rate
#There is significant difference for item 138. Not so much for other items
maxT = 4
kp_spell_data = spell_data %>% filter(kpid==87)
hr_data = imputate_hazard_rate(kp_spell_data, maxT)
hr_data$res = factor(hr_data$res)
hr_data =  hr_data %>% mutate(hmax=h+1.97*sd_h,hmin=h-1.97*sd_h)
# check the fitted hazard rate
h1=qplot(data=hr_data, x=t,y=h,geom='line', col=res) + 
  geom_errorbar( mapping=aes(x=t, ymin=hmin, ymax=hmax,color=res),width=0.1)+
  ylab('Hazard Rate')+ 
  theme(axis.text.x=element_blank(),
        axis.title.x=element_blank(),
        axis.ticks.x=element_blank())+ 
  ggtitle('Two Didit Multiplication')



kp_spell_data = spell_data %>% filter(kpid==138)
hr_data = imputate_hazard_rate(kp_spell_data, maxT)
hr_data$res = factor(hr_data$res)
hr_data =  hr_data %>% mutate(hmax=h+1.97*sd_h,hmin=h-1.97*sd_h)

h2=qplot(data=hr_data, x=t,y=h,geom='line', col=res) + 
  geom_errorbar( mapping=aes(x=t, ymin=hmin, ymax=hmax,color=res),width=0.1)+
  ylab('Hazard Rate')+ 
  xlab('Number of practice') + ggtitle('Vertical Division')


grid.arrange(h1, h2, ncol=1)

```

Not all the stop decisions are driven by the X strike rule. Otherwise, the hazard rate for the first period and that conditions on concurrent correct response shall be zero. However, not all the stop decision are driven by the state depenent mechanism either because the presence of the X-strike rule. Therefore, it is reasonable to assume that neither the response-dependent hazard model or the state depenent hazard model produces the consistent result. That said, from collary 1 and propostion 1, they constitute a bound on the true parameter.  Consequently,both nonparametric and parametric hazard model are estimated for the dataset,although non-parametric model may be a better choice.


```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.cap = "Fitted Conditional Hazard Curve:Proportional Model", fig.align='center',out.height='8cm',out.width='8cm'}
include_graphics('fig/hr_prop.png')
```

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.cap = "Fitted Conditional Hazard Curve:Nonparametric Model", fig.align='center',out.height='8cm',out.width='8cm'}
include_graphics('fig/hr_nonparametric.png')
```


### Selection bias 

```{r, echo=FALSE, warning=FALSE, message=FALSE}
for (i in seq(2)){
  file_path = paste0(proj_dir,'/_data/02/res/',kpids[i],'/yh.txt')
  y_param_data = read.table(file_path, col.names=c('l','pi','c0','c1','lambda0','beta0','lambda1','beta1'), header=F,sep=',')
  y_param_data = y_param_data %>% select(l) %>% mutate(type='Response Dependent', model='proportional')
  
  file_path = paste0(proj_dir,'/_data/02/res/',kpids[i],'/xh.txt')
  x_param_data = read.table(file_path, col.names=c('l','pi','c0','c1','lambda0','beta0','lambda1','beta1'), header=F,sep=',') 
  x_param_data = x_param_data %>% select(l) %>% mutate(type='State Dependent', model='proportional')
  
  file_path = paste0(proj_dir,'/_data/02/res/',kpids[i],'/yh_np.txt')
  y_param_data_np = read.table(file_path, col.names=c('l','pi','c0','c1','h01','h02','h03','h04','h11','h12','h13','h14'), header=F,sep=',')
  y_param_data_np = y_param_data_np %>% select(l) %>% mutate(type='Response Dependent', model='nonparametric')
  
  file_path = paste0(proj_dir,'/_data/02/res/',kpids[i],'/xh_np.txt')
  x_param_data_np = read.table(file_path, col.names=c('l','pi','c0','c1','h01','h02','h03','h04','h11','h12','h13','h14'), header=F,sep=',') 
  x_param_data_np = x_param_data_np %>% select(l) %>% mutate(type='State Dependent', model='nonparametric')
  
  tmp = rbind(y_param_data, x_param_data, y_param_data_np, x_param_data_np)
  tmp$kp = kp_names[i]
  
  if (i==1){
    param_data = tmp
  }else{
    param_data = rbind(param_data, tmp)
  }
}

param_stat = param_data %>% group_by(kp,model,type) %>% summarize(l=mean(l))
kable(param_stat)

```

The fitted learning curves are 

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.cap = "Fitted Observed Learning Curve:Proportional Model", fig.align='center',out.height='8cm',out.width='8cm'}
include_graphics('fig/lc_prop.png')
```

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.cap = "Fitted Observed Learning Curve:Nonparametric Model", fig.align='center',out.height='8cm',out.width='8cm'}
include_graphics('fig/lc_nonparametric.png')
```

## (APPENDIX) Appendix {-} 

###  Proof of Lemma 1
$$
\begin{aligned}
P(X_t=m|\mathbf{Y},H_{t-1}=0) &= \frac{\sum_{k=1}^{m}P(X_t=m|X_{t-1}=k)P(H_{t-1}=0|X_{t-1}=k)P(X_{t-1}=k|\mathbf{Y})}{\sum_{n=1}^{M_x}[\sum_{l=1}^{n}P(X_t=n|X_{t-1}=l)P(H_{t-1}=0|X_{t-1}=l)P(X_{t-1}=k|\mathbf{Y})]}\\
P(X_t=m|\mathbf{Y}) &= \frac{\sum_{k=1}^{m}P(X_t=m|X_{t-1}=k)P(X_{t-1}=k|\mathbf{Y})}{\sum_{n=1}^{M_x}[\sum_{l=1}^{n}P(X_t=n|X_{t-1}=l)P(X_{t-1}=l|\mathbf{Y})]}\\
\end{aligned}
$$
Let $P(X_t=m|X_{t-1}=n)$ be $\ell^{nm}$,$P(X_{t-1}=k|\mathbf{Y})=\pi_k$, $P(H_{t-1}=0|X_{t-1}=k)=h^k$. 

$$
P(X_t=m|\mathbf{Y},H_{t-1}=0) = P(X_t=m|\mathbf{Y}) \rightarrow \sum_{n=1}^{M_x}\sum_{l=1}^{n}\sum_{k=1}^{m} \ell^{km}\ell^{ln}\pi_k\pi_l(h_l-h_k) = 0
$$

If $h_l=h_k \quad \forall{\ell,k}$, it is obvious that the equality stands. If the equality stands, but for $h_l\neq h_k\quad \text{for some }l,k$, then it must be true that $\ell^{km}\ell^{ln} = \ell^{lm}\ell^{kn}$. Since no conditions are imposed on the pedagogical efficacy, it must not be the case.


### 2. Proof of Theorem 1

To prove inconsitency, it is sufficient to prove that the true parameters are not the stationary point. Given the true parameter $\Theta$, the EM algorithm essentially esimate $\ell^{nm}=P(X_t=m|X_t=n)$ by 

$$
\begin{aligned}
\hat{\ell}^{mn} &= \frac{\sum_{t=2}^T\sum_{i=1}^NI(X^i_t=m,X^i_{t-1}=n|\Theta,\mathbf{Y}^i_{1,t},H_{t-1}=0)}{\sum_{t=2}^T\sum_{i=1}^NI(X^i_{t-1}=n|\Theta,\mathbf{Y}^i_{1,t},H_{t-1}=0)}\\
&= \frac{\sum_{t=2}^{T}\frac{\sum_{i=1}^NI(X^i_t=m,X^i_{t-1}=n|\Theta,\mathbf{Y}^i_{1,t},H_{t-1}=0)}{N}+}{\sum_{t=2}^{T}\frac{\sum_{i=1}^NI(X^i_{t-1}=n|\Theta,\mathbf{Y}^i_{1,t},H_{t-1}=0)}{N}}
\end{aligned}
$$


By law of large number, 

$$
\begin{aligned}
\lim_{N\rightarrow \infty}\frac{\sum_{i=1}^NI(X^i_t=m,X^i_{t-1}=n|\Theta,\mathbf{Y}^i_{1,t},H_{t-1}=0)}{N}&\rightarrow P(X_t=m,X_{t-1}=n|\mathbf{Y},H_{t-1}=0)\\
\lim_{N\rightarrow \infty} \frac{\sum_{i=1}^NI(X^i_{t-1}=n|\Theta,\mathbf{Y}^i_{1,t},H_{t-1}=0)}{N} &\rightarrow P(X_{t-1}=n|\mathbf{Y},H_{t-1}=0)
\end{aligned}
$$
$$
\begin{aligned}
\lim_{N\rightarrow\infty}\hat{\ell}^{mn} &\rightarrow \frac{\sum_{t=2}^TP(X_t=m,X_{t-1}=n|\mathbf{Y},H_{t-1}=0)}{\sum_{t=2}^TP(X_{t-1}=n|\mathbf{Y},H_{t-1}=0)}\\
&\neq \frac{\sum_{t=2}^TP(X_t=m,X_{t-1}=n|\mathbf{Y})}{\sum_{t=2}^TP(X_{t-1}=n|\mathbf{Y})} = \ell^{mn}
\end{aligned}
$$

Therefore, the true parameter $\Theta$ is not a stationary point, the EM algorithm does not converge to the true parameter.

The MAP estimator by MCMC algorithm is similar to the EM algorithm when the model is identified by the EM algorithm. When sample is large enough, the influence of prior approaches 0.

## Proof of Lemma 3 and Lemma 4

Lemma 3
$$
\begin{aligned}
P(X_t=k|\mathbf{Y},H_{t-1}) &=\frac{P(X_t=k,\mathbf{Y},H_{t-1}=0)}{P(\mathbf{Y},H_{t-1})} \\
&= \frac{P(H_{t-1}|\mathbf{Y})P(\mathbf{Y}|X_t=k)P(X_t=k)}{\sum_{m=0}^1P(H_{t-1}|\mathbf{Y})P(\mathbf{Y}|X_t=m)P(X_t=m)} \\
&= \frac{P(\mathbf{Y}|X_t=k)P(X_t=k)}{\sum_{m=0}^1P(\mathbf{Y}|X_t=m)P(X_t=m)} \\
&= P(X_t=k|\mathbf{Y})
\end{aligned}
$$


### Theorem 2

$$
\begin{aligned}
E(\hat{\ell}^{m,n}) &= \frac{\sum_{t=1}^TP(X_t=n,X_{t-1}=m,H_{t-1}|\mathbf{Y})}{\sum_{t=1}^TP(X_{t-1}=m,H_{t-2}|\mathbf{Y})}\\
&=\frac{\sum_{t=1}^TP(X_t=n,X_{t-1}=m|\mathbf{Y})}{\sum_{t=1}^TP(X_{t-1}=m|\mathbf{Y})} \\
&=\ell^{m,n}
\end{aligned}
$$


### Simulation Distribution

```{r,echo=FALSE, warning=FALSE, message=FALSE, fig.cap = "Posterior Distribution of Pedagogical Efficacy:Simulation"}

strike_lrs$type = 'Response Dependent'
state_lrs$type = 'State Dependent'

h1= qplot(data=strike_lrs, x=l, geom='density',col=model, linetype=model) + geom_vline(xintercept = strike_ell) + ggtitle('2 Strike')+theme(legend.position="top")
h2= qplot(data=state_lrs, x=l, geom='density',col=model, linetype=model) + geom_vline(xintercept = state_ell) + ggtitle('State Differential Hazard')+theme(legend.position="top")
grid.arrange(h1,h2,ncol=2)


```

### Data Cleaning Process
The study collected more than 68 million exercise logs. 

First retain serious learners, defined as those have more than 50 log entries, accounting for 20% of the total learners. However, serious learners generated 42 million exercises log or 62% of the total logs.

The recommendation algorithm is designed as much that the knowledge points alternate between different practice sequences so that the learner is not bored. Therefore, there are two sequence ranks one can compute. The global sequence rank ignores the interval and continues the practice rank count over the whole sample period. The local sequence rank only counts the practice rank within the sequence. Neither counting method is perfect. The global sequence rank completely ignores the dropout; while the local sequence rank violates the assumption of homogeneous initial mastery. For the purpose of this chapter, learner heterogeneity is a lesser evil so the local sequence rank is chosen.

The three knowledge points all have more than 200,000 practice logs. Other than the vertical division, 95% of the practice sequences have a life span smaller than 5 periods. For vertical division, 85% of the sequences ends before the sixth practice. 

Take a 2% random sample from the log repository of each knowledge points for parameter estimation and 1% random sample for the out sample forecast. The vertical division has a smaller sample size thus the percentages for in-sample and out-sample are 4% and 2% respectively. 


### Distribution of the Estimated Parameters
```{r,echo=FALSE, warning=FALSE, message=FALSE, fig.cap = "Posterior Distribution of Pedagogical Efficacy:Real Dataset"}
qplot(data=param_data, x=l, facets = kp~model, col=type,geom='density')+theme(legend.position="top")
```
