<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Essays on Learning Through Practice</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Junchen Feng’s dissertation.">
  <meta name="generator" content="bookdown <!--bookdown:version--> and GitBook 2.6.7">

  <meta property="og:title" content="Essays on Learning Through Practice" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Junchen Feng’s dissertation." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Essays on Learning Through Practice" />
  
  <meta name="twitter:description" content="Junchen Feng’s dissertation." />
  

<meta name="author" content="Junchen Feng">

  
<meta name="date" content="2017-02-08">

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<!--bookdown:link_prev-->
<!--bookdown:link_next-->

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>



<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


<!--bookdown:title:start-->
<div id="header">
<h1 class="title">Essays on Learning Through Practice</h1>
<h4 class="author"><em>Junchen Feng</em></h4>
<h4 class="date"><em>2017-02-08</em></h4>
</div>
<!--bookdown:title:end-->

<!--bookdown:toc:start-->
  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">
<!--bookdown:toc2:start-->
<ul>
<li><a href="#abstract">Abstract</a></li>
<li><a href="#introduction"><span class="toc-section-number">1</span> Introduction</a></li>
<li><a href="#model"><span class="toc-section-number">2</span> Learning Through Practice</a><ul>
<li><a href="#practice-efficacy-and-learning-process"><span class="toc-section-number">2.1</span> Practice Efficacy and Learning Process</a><ul>
<li><a href="#the-general-definition-of-efficacy"><span class="toc-section-number">2.1.1</span> The General Definition of Efficacy</a></li>
<li><a href="#the-working-definition-of-efficacy"><span class="toc-section-number">2.1.2</span> The Working Definition of Efficacy</a></li>
<li><a href="#the-learning-process"><span class="toc-section-number">2.1.3</span> The Learning Process</a></li>
<li><a href="#the-learning-process-with-learner-engagement"><span class="toc-section-number">2.1.4</span> The Learning Process with Learner Engagement</a></li>
</ul></li>
<li><a href="#the-statistical-inference-model"><span class="toc-section-number">2.2</span> The Statistical Inference Model</a><ul>
<li><a href="#an-overview-of-hidden-markov-model"><span class="toc-section-number">2.2.1</span> An Overview of Hidden Markov Model</a></li>
<li><a href="#hidden-markov-model-as-a-learning-model"><span class="toc-section-number">2.2.2</span> Hidden Markov Model As A Learning Model</a></li>
<li><a href="#the-bayesian-knowledge-tracing-model"><span class="toc-section-number">2.2.3</span> The Bayesian Knowledge Tracing Model</a></li>
<li><a href="#learner-heterogeneity"><span class="toc-section-number">2.2.4</span> Learner Heterogeneity</a></li>
<li><a href="#learner-engagement"><span class="toc-section-number">2.2.5</span> Learner Engagement</a></li>
<li><a href="#an-example-of-input-data"><span class="toc-section-number">2.2.6</span> An Example of Input Data</a></li>
</ul></li>
<li><a href="#express-learning-theories-with-the-ltp-model"><span class="toc-section-number">2.3</span> Express Learning Theories with the LTP model</a><ul>
<li><a href="#zone-of-proximal-development"><span class="toc-section-number">2.3.1</span> Zone of Proximal Development</a></li>
<li><a href="#reinforcement-learning"><span class="toc-section-number">2.3.2</span> Reinforcement Learning</a></li>
</ul></li>
<li><a href="#adaptive-practice-recommendation"><span class="toc-section-number">2.4</span> Adaptive Practice Recommendation</a></li>
</ul></li>
<li><a href="#identification"><span class="toc-section-number">3</span> Model Identification</a></li>
<li><a href="#estimation"><span class="toc-section-number">4</span> Model Estimation</a></li>
</ul>
<!--bookdown:toc2:end-->
      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Essays on Learning Through Practice</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<!--bookdown:toc:end-->
<!--bookdown:body:start-->
<div id="abstract" class="section level1 unnumbered">
<h1>Abstract</h1>
<!--chapter:end:index.Rmd-->
</div>
<div id="introduction" class="section level1">
<h1><span class="header-section-number">1</span> Introduction</h1>
<!--chapter:end:01-introduction.Rmd-->
</div>
<div id="model" class="section level1">
<h1><span class="header-section-number">2</span> Learning Through Practice</h1>
<p>The ultimate goal of the Learning through Practice (LTP) model is a selection and evolution strategy of practice problems based on practice efficacy to enhance a learner’s mastery . To achieve this goal, this chapter provides a learning model to define practice efficacy and a statistical model to infer it from observed data.</p>
<p>The focal parameter of the chapter is practice efficacy (<span class="math inline">\(\ell\)</span>). Practice efficacy is defined as the probability that a learner reaches a higher level of mastery after attempting a practice item, or a practice problem, once. <span class="math inline">\(\ell\)</span> ranges between 0 and 1. When <span class="math inline">\(\ell=1\)</span>, the practice efficacy is perfect, a learner achieves higher mastery for sure after one attempt. When <span class="math inline">\(\ell=0\)</span>, the practice efficacy is null, a learner never achieves higher mastery no matter how much attempts she makes. Practice items differ in their efficacy, which is the rationale for engineering item assignments (<span class="math inline">\(A\)</span>) of a practice sequence.</p>
<p>The Bayesian Knowledge Tracing model (BKT) model assumes the simplest structure for practice efficacy. It postulates that the mastery only has two states and learners are homogenous in their response to items. The BKT model masks key learner heterogeneities that are important to the understanding of the learning process and the engineering of an adaptive learning strategy. This chapter assumes that there are more than two levels of mastery and learning rates differ depending on the mastery level, which is called state heterogeneity. This chapter also assumes that learners differ in their learning rate conditional on the same mastery level, which is called type heterogeneity. Formally, the LTP model postulates a learner’s mastery level is an ordinal variable <span class="math inline">\(X\)</span>, learner’s type is a nominal variable <span class="math inline">\(Z\)</span>, occasion within the practice sequence is a positive integer <span class="math inline">\(t\)</span> and the item id is a positive integer <span class="math inline">\(j\)</span>. The practice efficacy expressed the probability of a learner of type <span class="math inline">\(z\)</span> ascends to mastery level <span class="math inline">\(n\)</span> from the base level <span class="math inline">\(m\)</span> after exposing to item <span class="math inline">\(j\)</span> as <span class="math inline">\(\ell_j^{z;m,n}=P(X_t=n|X_{t-1}=n,Z=z,A_t=j)\)</span>.</p>
<p> The learning process can be defined in relation to the pedagogical efficacy and item assignments. In this thesis, it is characterized by the evolution of density of mastery levels over time, rather than the evolution of mastery levels per se. As long as practice items differ in their efficacies, a learning process cannot be defined without the knowledge of item assignment, because different orderings of items lead to different orderings of transition probabilities, and thus different expected density, even given the same starting density. Formally, let the probability of a learner with type <span class="math inline">\(z\)</span> has mastery level <span class="math inline">\(k\)</span> at the first practice occasion be <span class="math inline">\(\pi_1^{z;k} = P(X_1=k|Z=z)\)</span>. This is the prior belief of the learner mastery profile. Given the item assignments (<span class="math inline">\(\mathbf{A}=\{A_1,\dots,A_T\}\)</span>), the learning process is defined by the following recursive expression</p>
<p><span class="math display">\[
\pi_{t,\mathbf{A}}^{z;n} = \sum_{m=0}^{n} \pi_{t,\mathbf{A}-1}^{z;m} \ell_{A_t=j}^{z;m,n} \quad \forall t&gt;1
\]</span></p>
<p>Had the mastery (<span class="math inline">\(X\)</span>) and the learner type (<span class="math inline">\(Z\)</span>) been observed, both initial density (<span class="math inline">\(\pi^{z;k}_1\)</span>) and the practice efficacy (<span class="math inline">\(\ell^{z;m,n}_j\)</span>) could be estimated from data. Unfortunately, neither of them is observed. The A statistical model is therefore needed to reveal it. Hidden Markov Model (HMM) is a classical framework to describe such dynamic latent variable model. The HMM model has two components: the hidden layer and the observed layer. The hidden layer describes the evolution of latent states in the form of a Markov Chain. The observed layer describes the generation of observed data based on latent states.</p>
<p>The hidden layer consists of two elements, the initial density of latent states and the state transition matrix. In the context of learning process, the initial density of the hidden layer is the prior belief of the density of a learner’s inintial mastery. The lower right diagonal of the state transition matrix is 0 because of the no-regression assumption. The upper right diagonal of the state transition matrix is filled with corresponding practice efficacy.</p>
<p>The observed layer only involves the observed response (<span class="math inline">\(Y\)</span>) in the classical BKT model. This chapter expands the observed data to include learner engagement, which includes the stop decision (<span class="math inline">\(H\)</span>) and the effort decision (<span class="math inline">\(E\)</span>). The generation of the discrete observed data (<span class="math inline">\(O = \{Y,E,H\}\)</span>) is specified as a multinomial distribution whose probablity mass function depends on the latent mastery level <span class="math inline">\(P(O|X=k)\)</span>. Importantly, when effort choice is involved, this thesis makes a critical assumption that “no pain no gain” which aruges that a learner’s mastery cannot be elevated unless she tries. Therefore, it also affects the evolution of the latent state. Let <span class="math inline">\(E_t\)</span> denote a learner makes a valid effort at occasion <span class="math inline">\(t\)</span>, given the item assignments (<span class="math inline">\(A\)</span>), the learning process is hence</p>
<p><span class="math display">\[
\pi_{t,\mathbf{A}}^{z;n} = \pi_{t-1,\mathbf{A}}^{z;n} + E_t\sum_{m=0}^{n-1} \ell^{z;m,k}_j\pi_{t-1,\mathbf{A}}^{z;m} \quad \forall t&gt;1
\]</span> </p>
<div id="practice-efficacy-and-learning-process" class="section level2">
<h2><span class="header-section-number">2.1</span> Practice Efficacy and Learning Process</h2>
<p>The core of the Learning Through Practice model (LTP) is practice efficacy. It characterizes the how practice items boost a learner’s mastery. This section starts with a general definition built on an ordinal representation of mastery, then outlines the assumptions leading to a simpilifed working definition used in this thesis.</p>
<div id="the-general-definition-of-efficacy" class="section level3">
<h3><span class="header-section-number">2.1.1</span> The General Definition of Efficacy</h3>
<p>Let the mastery (<span class="math inline">\(X_t\)</span>) at practice occasion <span class="math inline">\(t\)</span> be represented as a unidimensional ordered discrete variable with <span class="math inline">\(M_x\)</span> number of states. <span class="math inline">\(M_x\)</span> is a positive integer. In this thesis, it can take value either 2 or 3. <span class="math inline">\(t \in \{1,2,\dots,T\}\)</span>, where <span class="math inline">\(T\)</span> is the max length of the practice sequence. The learning is not timed by clock time, but by the number of practice problems done, or the practice occasion. In the following analysis, “time” and “practice occasion” are used interchangeably.</p>
<p>The unidimensionality specification avoids the complexity of representing the response as a function of multiple inputs. If the latent mastery is multi-dimensional, the likelihood function of the observed response depends on the question being a single solution with sequential reasoning, multiple solutions with single step reasoning, or multiple steps with sequential reasoning. In addition, each reasoning can house one or more components of the mastery. The unidimensionality assumption also avoids the explosion of pedagogical efficacy parameters. If the latent mastery is multi-dimensional, the state transition matrix of one dimension is unlikely to be independent of all other dimensions. The number of the parameters of the transition matrix explodes exponentially as the dimension of the latent mastery grows.</p>
<p>In an ordinal mastery representation, learning can be defined as ascending from a lower level to a higher level. Practice efficacy is thus defined as the probability of such ascension. In addition to the learner type and mastery levels, a general definition of efficacy requires encoding the learning context, which includes item assignments, responses, and practice occasion. Let <span class="math inline">\(j\)</span> practice item id, <span class="math inline">\(j \in \{1,2,\dots,J\}\)</span>, where <span class="math inline">\(J\)</span> is the total number of items in the question bank. Let <span class="math inline">\(Y_t\)</span> denote the observed response at practice occasion <span class="math inline">\(t\)</span>, which can take value 0 or 1. Let <span class="math inline">\(A_t\)</span> denote the item assignment at practice occasion <span class="math inline">\(t\)</span>. <span class="math inline">\(A_t = j\)</span> means that a learner encounters item <span class="math inline">\(j\)</span> at practice occasion <span class="math inline">\(t\)</span>. Formally, the general efficacy can be written as</p>
<p><span class="math display">\[
\ell^{z;m,n}_{A_1,\dots,A_t;Y_1,\dots,Y_t;t} \equiv P(X_t=n|X_{t-1}=m; Z=z; A_1,\dots, A_t; Y_1,\dots,Y_t;t)
\]</span></p>
<p>An important assumption of the learning process is that a learner never regresses on mastery, or she never forgets what she has learned.</p>
<p><strong>Assumption 1:</strong></p>
<p><span class="math display">\[
\ell^{z;m,n}_{A_1,\dots,A_t;Y_1,\dots,Y_t;t} = 0 \quad \forall m &gt; n
\]</span></p>
</div>
<div id="the-working-definition-of-efficacy" class="section level3">
<h3><span class="header-section-number">2.1.2</span> The Working Definition of Efficacy</h3>
<p>The highly contextualized general definition of practice efficacy results in too many parameters to be estimated. For the sake of feasible inference, a working definition of practice efficacy needs to be decontextualised. In this thesis, the working definition is</p>
<p><span class="math display">\[
\ell^{z;m,n}_{j} \equiv P(X_t=n|X_{t-1}=m; Z=z; A_t=j)
\]</span></p>
<p>Compared to the general definition, the working definition of the practice efficacy removes the dependencies on observed response (<span class="math inline">\(Y_1,\dots,Y_t\)</span>), previous item assignments (<span class="math inline">\(A_1,\dots,A_{t-1}\)</span>), and practice occasion (<span class="math inline">\(t\)</span>). This subsection discusses the practical implications of these simplification.</p>
<p>For a particular practice sequence with length <span class="math inline">\(T\)</span>, denote the joint responses as <span class="math inline">\(\mathbf{Y}_{1,T} = (Y_1,\dots,Y_T)\)</span>. Similarly, <span class="math inline">\(\mathbf{X}_{1,T}\)</span> is the joint latent masteries, and <span class="math inline">\(\mathbf{A}_{1,T}\)</span> the joint item compositions. Let <span class="math inline">\(\mathbf{y}_{1,T}=(y_1,\dots, y_T)\)</span> be the realized response sequence and <span class="math inline">\(P\)</span> denotes the probability mass function where <span class="math inline">\(P(\mathbf{Y}_{1,T}=\mathbf{y}_{1,T}) = P(Y_1=y_1,\dots, Y_T=y_T)\)</span>. Similary, <span class="math inline">\(\mathbf{x}_{1,T}\)</span> is the realized joint latent masteries, and <span class="math inline">\(\mathbf{a}_{1,T}\)</span> the realized joint item compositions. For simplicity, when referring to the whole practice sequence, the underscript <span class="math inline">\(1,T\)</span> is dropped throughout the thesis.</p>
<p><strong>Assumption 2</strong>: Pedagogical efficacy does not depend on responses conditional on the previous latent mastery.</p>
<p><span class="math display">\[
\begin{aligned}
&amp;P(X_t=n|X_{t-1}=m; Z=z; \mathbf{A}_{1,t-1}, A_t=j; \mathbf{Y}_{1,t-1}, Y_t,;t) \\
&amp;= P(X_t=n|X_{t-1}=m;Z=z;\mathbf{A}_{1,t-1},A_t=j;t) \quad \forall \mathbf{Y}_{1,t-1}, Y_t
\end{aligned}
\]</span></p>
<p>It may strike some readers as odd to assume learner learns at the same rate whether or not she solves the problem or not. This is the exact critique of the performance factor analysis model (PFA) <span class="citation">(Pavlik Jr, Cen, and Koedinger <a href="#ref-pavlik2009performance">2009</a>)</span>. However, what stylized fact does the Assumption 2 fails to account for? The proponents of the PFA may argue that Assumption 2 does not generate a positive correlation between successes. This critique is not entirely correct because the successes are positive correlated unconditional on the latent mastery. More previous success implies higher mastery and consequently higher success rate in the future practice. Assumption 2 claims independence only after conditioning on the latent mastery. The proponents of the PFA may be right to argue that Assumption 2 does not generate high enough positive correlation with an LTP model of binary latent mastery. However, a larger magnitude of the positive correlation may be achieved by allowing the latent mastery to have more states and a positive correlation between efficacy and the state of latent mastery. In short, the Assumption 2 greatly reduces the complexity of parameter learning without significantly impairs the model’s explanatory power on a learning dataset.</p>
<p><strong>Assumption 3</strong>: There is no complementarity or substitution effect in the item composition. <span class="math display">\[
\begin{aligned}
&amp;P(X_t=n|X_{t-1}=m;Z=z;\mathbf{A}_{1,t-1},A_t=j;t)\\
&amp;=P(X_t=n|X_{t-1}=m;Z=z;A_t=j;t) \quad \forall \mathbf{A}_{1,t-1}
 \end{aligned}
\]</span></p>
<p>Assumption 3 rules out scaffolding by preparing a learner with a string of easy problems to solve the final difficult problem. It also rules out a decreasing pedagogical efficacy in the case of naive repetition. If items in the sequence are nearly identical, the learner learns from subsequent practice opportunities with equal probability even if she does not learn from the first attempt.</p>
<p><strong>Assumption 4</strong>: The pedagogical efficacy is independent of the sequence position.</p>
<p><span class="math display">\[
\begin{aligned}
&amp;P(X_t=n|X_{t-1}=m;Z=z;A_t=j;t)\\
&amp;=P(X_t=n|X_{t-1}=m;Z=z;A_t=j) \quad \forall t
\end{aligned}
\]</span></p>
<p>Assumption 4 claims that a learner has the same probability of learning whether the item is the first one in the sequence or the last one. It essentially assumes that the learner is a paragon of grit and positive psychology. She is never frustrated by failures, never bored by repetition, and can focus as long as it takes.</p>
<p></p>
</div>
<div id="the-learning-process" class="section level3">
<h3><span class="header-section-number">2.1.3</span> The Learning Process</h3>
<p>For any particular learner, the learning process is characterized by the evolution of mastery levels, <span class="math inline">\(\{X_1,X_2,\dots,X_T\}\)</span>. When two consecutive levels differ (<span class="math inline">\(X_t \neq X_{t+1}\)</span>), learning happens. The thesis does not take approach to characterize the learning process. For one thing, learning is a probablistic event, therefore any particular realization of learning (or lack thereof) is a noisy signal of practice efficacy. For another thing, an analyst may be interested in mastery at the population levels for each type of learners. Instead, this thesis characterizes the learning process as change of density of mastery levels over time. Let the probability of the “typical” learner of type <span class="math inline">\(z\)</span> has mastery level <span class="math inline">\(k\)</span> at practice occasion <span class="math inline">\(t\)</span> be <span class="math inline">\(\pi_t^{z,k} = P(X_t=k|Z=z)\)</span>. Let the density of mastery for learner type <span class="math inline">\(z\)</span> at occasion <span class="math inline">\(t\)</span> be noted as <span class="math inline">\(\mathbf{\Pi}^z_t = \{\pi_t^{z,0},\dots,\pi_t^{z,M_X-1}\}\)</span> where <span class="math inline">\(\sum_{k=0}^{M_x-1}\pi_t^{z,k}=1\)</span> and <span class="math inline">\(0\leq\pi_t^{z,k}\leq1\)</span>. The generic characterization of the learning process is <span class="math inline">\(\mathbf{\Pi}^z_1,\dots,\mathbf{\Pi}^z_T\)</span>.</p>
<p>The generic characterization of the learning process masks the important role of item assignments. If practice efficacy is known, the expected learning process is deterministic in the sense that it can be expressed explicitly by the prior belief of the density of the initial mastery (<span class="math inline">\(\mathbf{\Pi}^z_1\)</span>), practice efficacies (<span class="math inline">\(\ell_j^{z;m,n}\)</span>) and item assignments (<span class="math inline">\(\mathbf{A}=\{A_1,\dots,A_t\}\)</span>). In another words, given practice efficacies, it may be possible to formulate an item assignment <span class="math inline">\(\mathbf{A^*}\)</span> that is optimal to a particular learner’s estimated mastery profile <span class="math inline">\(\widetilde{\mathbf{\Pi}}^z_1\)</span>, which is the goal of the thesis.</p>
<p>Consider the learning process of a learner encountering item <span class="math inline">\(j\)</span> at practice occasion <span class="math inline">\(t\)</span>. If her starting level of mastery is known for sure, say <span class="math inline">\(m\)</span>, her new expected mastery profile can be expressed as</p>
<p><span class="math display">\[
\begin{aligned}
\Pi^{z}_{t,j}&amp;=P(X_t|X_{t-1}=n,A_t=j,Z=z)\\
&amp;= \Bigg\{ \begin{array}{cc}
0 &amp; \text{if }X_t&lt;m \\
\ell^{z;m,X_t}_j &amp; \text{if }X_t\geq m
\end{array}
\end{aligned}
\]</span></p>
<p>Instead, her exact mastery of level is unknown, but the density of her mastery is known. The probability that she reaches a higher mastery, say <span class="math inline">\(n\)</span>, can be calculated by computing the joint likelihood of mastery at two states then integrating out the previous mastery. The tricky part is that the starting density depends on the previous item assignments <span class="math inline">\(A_1,\dots,A_{t-1}\)</span>, except for the first practice occasion where the prior belief is employed. To save the notation from an abuse of symbols, this thesis simply note the item assignment scheme as <span class="math inline">\(\mathbf{A}\)</span>. The new density is therefore</p>
<p><span class="math display">\[
\begin{aligned}
\pi^{z,n}_{1,\mathbf{A}}&amp;=\pi^{z,n}_1\\
\pi^{z,n}_{t,\mathbf{A}}&amp;=\sum_{X_{t-1}}P(X_t=n,X_{t-1}|\mathbf{A}) \quad (\forall t&gt;1)\\
&amp;=\sum_{X_{t-1}}P(X_t=n|X_{t-1},A_t=j)P(X_{t-1}|\mathbf{A})\\
&amp;=\sum_{k=0}^{n}\ell^{z;m,k}_j\pi^{z,k}_{t-1,\mathbf{A}} + \sum_{k=n+1}^{M_x-1}0\pi^{z,k}_{t-1,\mathbf{A}} \quad \text{(1.1)}\\
\end{aligned}
\]</span> The last term (<span class="math inline">\(\sum_{k=n+1}^{M_x-1}0\pi^{z,k}_{t-1,\mathbf{A}}\)</span>) of equation (1.1) is implied by the no forgetting assumption.</p>
</div>
<div id="the-learning-process-with-learner-engagement" class="section level3">
<h3><span class="header-section-number">2.1.4</span> The Learning Process with Learner Engagement</h3>
<p>One important aspect of the leaner engagement is the effort decision (<span class="math inline">\(E_t\)</span>), which is a binary variable. Motivated by the intuition of “no pain no gain”, this thesis makes a strong that learning does not happen unless a learner actually tries</p>
<p><strong>Assumption 5</strong>: <span class="math display">\[
\pi^{z,n}_{t,\mathbf{A}} = \pi^{z,n}_{t-1,\mathbf{A}}\quad\text{if } E_t=0
\]</span> With the addition of the effort decision, the full learning process can be described as</p>
<p><span class="math display">\[
\begin{aligned}
\pi^{z,n}_{t,\mathbf{A}}
&amp;= \Bigg\{ \begin{array}{cc}
\pi^{z,n}_1 &amp; \text{if }t=1 \\
\pi_{t-1,\mathbf{A}}^{z;n} + E_t\sum_{m=0}^{n-1} \ell^{z;m,k}_j\pi_{t-1,\mathbf{A}}^{z;m} &amp; \text{if }t&gt;1
\end{array}
\end{aligned}
\]</span></p>
</div>
</div>
<div id="the-statistical-inference-model" class="section level2">
<h2><span class="header-section-number">2.2</span> The Statistical Inference Model</h2>
<p>Because the mastery of a learner is not directly observed but has to be inferred from observed data, this section develops a statistical model to make the inference. Hidden Markov model (HMM) is a classical framework that describes a dynamic latent process. The HMM has a hidden layer and an observed layer. The hidden Layer describes the evolution of the latent state while the observed layer describes the generation of observed data. The Bayesian Knowledge Tracing model is a special case of the HMM model. This thesis extends the the BKT model in two ways: the introduction of learner heterogeneity and the inclusion of learner engagement. The extended model is named as Learning Through Practice model (LTP) to differentiate it from the BKT model.</p>
<div id="an-overview-of-hidden-markov-model" class="section level3">
<h3><span class="header-section-number">2.2.1</span> An Overview of Hidden Markov Model</h3>
<p>Let the latent state be <span class="math inline">\(S\)</span>, the observed data be <span class="math inline">\(O\)</span>. Let the parameters of the hidden layer be <span class="math inline">\(\Theta_S\)</span>, the parameters of the observed layer be <span class="math inline">\(\Theta_O\)</span>. Let all the parameters be <span class="math inline">\(\Theta=\{\Theta_S,\Theta_O\}\)</span>. The key challenge HMM tries to solve it the following. The goal is to infer <span class="math inline">\(P(\Theta_S,\Theta_O|O)\)</span>, but <span class="math inline">\(S\)</span> is not observed. The key insight HMM is that <span class="math inline">\(S\)</span> can first be augmented to produce full likelihood then be integrated out to get the posterior parameter distribution.</p>
<p>Let <span class="math inline">\(R_{\Theta}\)</span> denote the parameter space of <span class="math inline">\(\Theta\)</span>, <span class="math inline">\(F(\Theta)\)</span> denote the prior distribution of parameters. The HMM argues that posterior distribution of parameter<span class="math inline">\(P(\Theta|O)\)</span> can be obtained by</p>
<p><span class="math display">\[
\begin{aligned}
P(\Theta|O) &amp;= \frac{P(O,\Theta)}{\int_{R_{\Theta}}P(O,\Theta)dF(\Theta)}\\
P(O,\Theta) &amp;=\sum_SP(O,S,\Theta)
\end{aligned}
\]</span></p>
<p>Whereas the latent variable is sampled from the posterior distribution</p>
<p><span class="math display">\[
P(S|O,\Theta) = \frac{P(O,S,\Theta)}{P(O,\Theta)}
\]</span></p>
<p>Obviously, the agumented likelihood (<span class="math inline">\(P(O,S,\Theta)\)</span>) plays a key role in the HMM scheme. It is the combined result of the hidden layer and the observed layer. The hidden layer is characterized by the joint likelihood <span class="math inline">\(P(S,\Theta_S)\)</span>. It is usually factored as</p>
<p><span class="math display">\[
P(S,\Theta_S) = P(S|\Theta_S)P(\Theta_S)
\]</span></p>
<p>where <span class="math inline">\(P(S|\Theta_S)\)</span> is informed by the theory about the hidden states while <span class="math inline">\(P(\Theta_S)\)</span> is the prior. The conditional likelihood <span class="math inline">\(P(O|S,\Theta_O)\)</span> characterizes the generation of the observed layer, where the dependence of <span class="math inline">\(S\)</span> is made explicit. The full likelihood is thus</p>
<p><span class="math display">\[
P(O,S,\Theta) = P(O|S,\Theta_O)P(S|\Theta_S)P(\Theta)
\]</span></p>
<p>Here is an brief summary of the inference rountine, which is described in much greater detail in the Chapter 3:</p>
<ol style="list-style-type: decimal">
<li><p>From prior parameter distribution sample <span class="math inline">\(\hat{\Theta}\)</span></p></li>
<li><p>Intitialize the latent state <span class="math inline">\(\hat{S}\)</span> by <span class="math inline">\(P(S|\Theta_S)\)</span></p></li>
<li><p>Sample new parameters <span class="math inline">\(\hat{\Theta}&#39;\)</span> by <span class="math inline">\(P(\Theta|O)\)</span></p></li>
<li><p>Sample new latent states <span class="math inline">\(\hat{S}&#39;\)</span> by <span class="math inline">\(P(S|O,\Theta)\)</span></p></li>
<li><p>Repeat step (3)-(4) until converges</p></li>
</ol>
</div>
<div id="hidden-markov-model-as-a-learning-model" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Hidden Markov Model As A Learning Model</h3>
<p></p>
<p>It should be noted that although the mastery is an ordinal variable, the statistical model for the hidden layer is a latent class model, rather than an ordered multinomial logit model. The ordered logit uses the ranking of the cutoff points of the continuous latent variable to impose the order while here the no-regression assumption (Assumption 1) imposes the order. Readers familiar with the literature of Item Response Theory (IRT) may also wonder why the latent mastery is not continuous. The different choices of operational definition are reflected in the shape of item characteristic curve (ICC). The ICC of the LTP model is a step function, whereas the ICC of the IRT model is a smooth sigmoid function. A priori, it is difficult to assert which ICC describes the true data generating process better. There are two scenarios in which the step function fits better. In the first scenario, the true mastery is a discrete variable and the true ICC is a step function. In the second scenario, the true mastery is a continuous variable but the true ICC is a mixture of sigmoid functions. In this case, a single sigmoid function may not fit as well as a flexible step function. As the number of discrete states increases, the step ICC will eventually outperform the single sigmoid ICC. Therefore, assuming the latent mastery as a discrete variable is not very restrictive as long as the number of states is allowed to vary.</p>
</div>
<div id="the-bayesian-knowledge-tracing-model" class="section level3">
<h3><span class="header-section-number">2.2.3</span> The Bayesian Knowledge Tracing Model</h3>
<p>The BKT model is a special instance of the Hidden Markov model. To describe the latent state (mastery), the initial state density and the state evolution (efficacy) needs to specified. To describe the observed data (response), the observation matrix needs to be specified.</p>
<p>The BKT model adopts the simplest definition of practice efficacy possible. It only admits one learner type and two mastery states. Consequently, there exists one practice efficacy for each item <span class="math inline">\(\ell_j^{1;0,1} = P(X_t=1|X_{t-1}=0,Z=1,A_t=j)\)</span>. For the rest of the thesis, denote the BKT efficacy as <span class="math inline">\(\ell\)</span>, according to the convention.</p>
<p>It is necessary to have a prior belief of the probability that a learner has mastery at the first practice occasion. Formally, such prior belief can be written as <span class="math inline">\(\pi_1=P(X_1=1)\)</span>. <span class="math inline">\(\pi_1\)</span> is called prior knowledge in the learning analytics literature, but initial density in this thesis to be consistent with the Markov Chain Monte Carlo algorithm introduced in Chapter Three. Once <span class="math inline">\(\pi\)</span> and <span class="math inline">\(\ell\)</span> are known, the unconditional state density at each practice occasion <span class="math inline">\(t\)</span> can be recursively defined as</p>
<p><span class="math display">\[
\pi_t = (1-\pi_{t-1})\ell + \pi_{t-1} \quad \forall t&gt;1
\]</span></p>
<p>The inference of latent mastery is based on observed responses, <span class="math inline">\(Y_1,\dots,Y_T\)</span>. The response of each practice occasion is a noisy measure of the concurrent mastery. The measure is noisy because a learner without mastery can answer correctly due to luck while a learner with mastery can answer incorrectly due to carelessness. The probability of a blind-luck correct is called guess rate, <span class="math inline">\(g=P(Y_t=1|X_t=0)\)</span>. The probability of a careless incorrect is called slip rate <span class="math inline">\(s=P(Y_t=0|X_t=1)\)</span>.</p>
<p>In sum, the BKT model describes the data generating process of observed response with four parameters: initial density (<span class="math inline">\(\pi\)</span>), efficacy (<span class="math inline">\(\ell\)</span>), guess rate (<span class="math inline">\(g\)</span>), and slip rate (<span class="math inline">\(s\)</span>).</p>
</div>
<div id="learner-heterogeneity" class="section level3">
<h3><span class="header-section-number">2.2.4</span> Learner Heterogeneity</h3>
<p>The efficacy of the BKT model is homogeneous for all learners given the same item. The LTP model allows an item to have different effects on different learners. The learner heterogeneity has already been built into the working definition of practice efficacy. One type of heterogeneity is state heterogeneity: Given the same type (<span class="math inline">\(z\)</span>) and same mastery goal (<span class="math inline">\(n\)</span>), learners from different starting mastery state (<span class="math inline">\(k,m\)</span>) progress at different speeds. <span class="math inline">\(\ell_j^{z,k,n}\neq\ell_j^{z;m,n}\)</span>. The other type of heterogeneity is type heterogeneity: Given the same origin (<span class="math inline">\(m\)</span>) and same mastery goal (<span class="math inline">\(n\)</span>), learners of different types (<span class="math inline">\(z,z&#39;\)</span>) progress at different speed.<span class="math inline">\(\ell_j^{z,m,n}\neq\ell_j^{z&#39;;m,n}\)</span>.</p>
<p>To complete the likelihood function of latent mastery, its initial density must be specified. Each learner type has a different distribution of mastery levels at the first practice occasion. The initial densities of each type are denoted as <span class="math inline">\(\pi_1^{z;k}\)</span> and they sum up to one.</p>
<p><span class="math display">\[\sum_{k=0}^{M_X-1} \pi_1^{z;k} = 1\]</span></p>
<p>The generation of the observed response only depends on a learner’s mastery level, not on her type. Intuitively, it means that learners at the same level produce similar responses, no matter how fast they get to that level. The observed response only take value 0/1 in this thesis. The conditional probability of the observed response is</p>
<p><span class="math display">\[
c^k_j \equiv P(Y_{t}=1|X_t=k,A_t=j)
\]</span></p>
<p>For example, in the BKT model, <span class="math inline">\(g_j = c_j^0\)</span> and <span class="math inline">\(s_j = 1-c_j^1\)</span>. This notation will be used in Chapter Three.</p>
</div>
<div id="learner-engagement" class="section level3">
<h3><span class="header-section-number">2.2.5</span> Learner Engagement</h3>
<p>So far, this chapter has implicitly assumed that the learner can engage in learning for as long as the instructor wishes and as focused as the learning task requires. Both the duration and the intensity of learner engagement are imperfect in a low stake learning environment, which is typical of most applications of the intelligent tutoring system. The learner engagement is not only another set of observed data that can be used to infer latent mastery, but engagement also influences the learning process directly. Therefore, it is important to include learner engagement as a component of the LTP model. This subsection includes two aspects of learner engagement: The stop decision describes the duration of learner engagement and the effort decision describes the intensity of learner engagement.</p>
<div id="event-sequence" class="section level4">
<h4><span class="header-section-number">2.2.5.1</span> Event Sequence</h4>
<p>The addition of learner engagement makes the data generating process more complicated that the vanilla BKT process where only latent mastery and observed response are involved. The following event sequence clarifies the new learning process.</p>
<ol style="list-style-type: decimal">
<li><p>A learner is presented with a practice question.</p></li>
<li><p>The learner exerts a level of effort based on her state of latent mastery</p></li>
<li><p>The learner produces a response based on the effort level and her state of latent mastery.</p></li>
<li><p>The learner receives feedback on the observed response.</p></li>
<li><p>If the learner has exerted effort, she learns probabilistically. Otherwise, she does not learn.</p></li>
<li><p>The learner can choose or be forced to exit. If the learner continues, repeat from (1); else data collection stops.</p></li>
</ol>
</div>
<div id="the-effort-decision" class="section level4">
<h4><span class="header-section-number">2.2.5.2</span> The Effort Decision</h4>
<p>The effort (<span class="math inline">\(E_t\)</span>) is assumed a binary choice with value 0 for not exerting effort and 1 for exerting effort. Conditional on the type of the learner, the state of latent mastery and the item id, the probability of exerting effort is constant.</p>
<p><span class="math display">\[
\gamma_{j}^{z;k} \equiv P(E_t=1|Z=z, X_t=k, A_t=j)
\]</span></p>
<p>Furthermore, assume that if the learner does not exert effort, the response is always completely incorrect.</p>
<p><strong>Assumption 6</strong>: <span class="math inline">\(P(Y_t=0|E_t=0,A_t=j) = 1 \quad \forall j,t\)</span></p>
</div>
<div id="the-stop-decision" class="section level4">
<h4><span class="header-section-number">2.2.5.3</span> The Stop Decision</h4>
<p>The stop decision (<span class="math inline">\(H_t\)</span>) denote whether a learner stop practice sequence at occasion <span class="math inline">\(t\)</span>. <span class="math inline">\(H_t=0\)</span> means a learner continues to practice whiel <span class="math inline">\(H_t=1\)</span> means a learner is no longer practicing. The hazard rate of occasion <span class="math inline">\(t\)</span> (<span class="math inline">\(\eta_t\)</span>) is the probability that a learner stops at occasion <span class="math inline">\(t\)</span> conditional on she continues at <span class="math inline">\(t-1\)</span>.</p>
<p><span class="math display">\[
\eta_t = P(H_t=1|H_{t-1}=0)
\]</span></p>
<p>Whether hazard rates depend on the response or the latent mastery merits a brief discussion. The issue is examined in details in Chapter Five. There are two types of stop decisions: stop-by-rule and stop-by-choice. An example of stop-by-rule is the “X-Strike” rule: A learner always keeps practicing unless she is forced to stop after accumulating X successes/failures. An example of stop-by-choice is the differential impact of boredom and frustration <span class="citation">(Baker et al. <a href="#ref-baker2010better">2010</a>)</span>: A learner without mastery is frustrated by further practice while a learner with mastery is bored by it. In the case of stop-by-rule, the stop decision can only depend on the responses because they are what the system observes. In the case of stop-by-choice, the stop decision can reasonably depend on either the response or the latent mastery, but more likely the latter. Therefore, this thesis equates the stop-by-rule decision with a response dependent hazard model and the stop-by-choice decision with a mastery dependent hazard model. In the following analysis, Let the dependence states be <span class="math inline">\(S\)</span>. <span class="math inline">\(S =\{Z,X\}\)</span> or <span class="math inline">\(S=\{Y\}\)</span>.</p>
<p>This thesis assumes that the conditional hazard rate is independent of item characteristics. This is largely a by-product of the simple stop-by-rule accounting method: Most stop-by-rule decision weighs each correct or incorrect answer equally regardless of its item. A more nuanced item-dependent hazard function requires the knowledge of a specific learning product. For the sake of generalization, such feature is excluded from the LTP model</p>
<p><strong>Assumption 7</strong>: <span class="math inline">\(P(H_t=1|H_{t-1}=0,S,A_t=j) = P(H_t=1|H_{t-1}=0,S,A_t=l) \quad \forall j,l\)</span></p>
<p>Furthermore, the functional form of hazard rate curve can be specified as parametric or nonparametric. A nonparametric hazard function is analogous to that of the observed response or effort choice. A different parameter is assigned to the hazard rate of each state at each practice occasion. Formally, the nonparametric model postulates</p>
<p><span class="math display">\[
\eta_t^s \equiv P(H_t=1|H_{t-1}=0,S_t=s)
\]</span></p>
<p>In contrast, a parametric hazard function has one set of parameters for each state. The popular choice is the proportional hazard model, which allows for different base rates (<span class="math inline">\(\lambda_s\)</span>) and different duration dependence (<span class="math inline">\(\beta_s\)</span>) among states.</p>
<p><span class="math display">\[
\eta_t^s\equiv \lambda_s e^{\beta_{s} t} 
\]</span></p>
<p>If the data generating process of the stop decision is parametric, both specifications are consistent but the parametric specification is efficient. Otherwise, the non-parametric specification is consistent while the parametric specification is inconsistent.</p>
</div>
</div>
<div id="an-example-of-input-data" class="section level3">
<h3><span class="header-section-number">2.2.6</span> An Example of Input Data</h3>
<p>Table 2.1 shows an example of the input data, which is used in Chapter Six. The first column is user id. The second column is item id. The third column is the practice occasion, which ranks the items into a sequence. The fourth column to the sixth column is observed data, responses, effort decisions and stop decisions respectively. The learner 30002 made a valid effort in the first item (<span class="math inline">\(E_1=1\)</span>) but got it wrong (<span class="math inline">\(Y_1=0\)</span>). She gave up on the second item (<span class="math inline">\(Y_2=E_2=0\)</span>). However, she made an attempt for the third item and got it right (<span class="math inline">\(E_3=Y_3=1\)</span>). The practice sequence is only three items long so the first two stop decision was no 0 (<span class="math inline">\(H_1=H_2=0\)</span>) while the third stop decision was yes (<span class="math inline">\(H_3=1\)</span>).</p>
<table>
<caption>(#tab:unnamed-chunk-1)An Example of the LTP Input Data</caption>
<thead>
<tr class="header">
<th align="center">User ID(i)</th>
<th align="center">Item ID(j)</th>
<th align="center">Occasion(t)</th>
<th align="center">Response(Y)</th>
<th align="center">Effort(E)</th>
<th align="center">Stop(H)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">30002</td>
<td align="center">Q_10201056649366</td>
<td align="center">1</td>
<td align="center">0</td>
<td align="center">1</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td align="center">30002</td>
<td align="center">Q_10201058056988</td>
<td align="center">2</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
</tr>
<tr class="odd">
<td align="center">30002</td>
<td align="center">Q_10201056666357</td>
<td align="center">3</td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">1</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="express-learning-theories-with-the-ltp-model" class="section level2">
<h2><span class="header-section-number">2.3</span> Express Learning Theories with the LTP model</h2>
<p>Because the LTP model relaxes the constraint of the restrictive structural representation of the Bayesian Knowledge Tracing model, it can have an arbitrary representation of the learning process, to the extent that the data allows for unique identification. To avoid abusing the LTP model as a pure data mining exercise, the representation of the learning process should be guided by learning theories. By the same token, because the LTP model is capable of expressing the testable implications of learning theories, the researcher can decide if the learning data fit her choice of the learning theory. Although this thesis does not carry out the empirical test, it is meaningful to point out the possibility of doing such test.</p>
<div id="zone-of-proximal-development" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Zone of Proximal Development</h3>
<p>The zone of proximal development <span class="citation">(Vygotsky <a href="#ref-vygotsky1978interaction">1978</a>)</span> postulates three types of relationship between the mental development of a learner and the mental requirement of a learning task. If the learner’s mental development lags the mental requirement of the learning task, the learning is slow, the engagement is low, and the performance is poor. If the learner’s mental development falls shorts of the requirement of the learning task if when she works independently but within her reach under the guidance or in collaboration, the learning speeds up, the engagement is high, and the performance improves. This is the zone of proximal development. If the learner’s mental development exceeds the requirement of the task, the performance is high albeit there is no learning.</p>
<p>This theory naturally calls for a three-state latent mastery (<span class="math inline">\(M_x=3\)</span>). It has empirically testable implications on the observed response and the effort rate. If the learner is not ready to learn (<span class="math inline">\(X=0\)</span>), she will fail the problem with high probability, exert little effort, and has no chance to suddenly master the skill.</p>
<p><span class="math display">\[
\begin{aligned}
P(Y_t=0|X_t=0) &amp;\approx 1\\
P(E_t=0|X_t=0) &amp;\approx 1\\
P(X_t=2|X_{t-1}=0) &amp;\approx 0\\
\end{aligned}
\]</span></p>
<p>If the learner has achieved mastery (<span class="math inline">\(X=2\)</span>), she will not fail the problem completely. <span class="math display">\[
P(Y_t=0|X_t=2) \approx 0
\]</span></p>
<p>If the learner is in the zone of the proximal development (<span class="math inline">\(X=1\)</span>), the learning process has no obvious constraints implied by Vygotsky’s theory.</p>
</div>
<div id="reinforcement-learning" class="section level3">
<h3><span class="header-section-number">2.3.2</span> Reinforcement Learning</h3>
<p>The observation that people repeat actions that reward them with pleasure and avoid actions that punish them with pain is well-established in behavioral science. Reinforcement learning is not only a stylized fact of how we learn <span class="citation">(John Robert Anderson <a href="#ref-anderson2000learning">2000</a>)</span> but may also be the biological foundation of how we learn <span class="citation">(Holroyd and Coles <a href="#ref-holroyd2002neural">2002</a>)</span>. In the context of learning through practice, reinforcement learning means that the more successes a learner get, the more engaged she is; the more failures a learner gets, the less engaged she is.</p>
<p>For the practice duration, if the stop decision depends on the latent mastery and the hazard rate is negatively correlated with the latent mastery, the LTP model generates the following pattern: The more successes the learner has in the preceding sequence, the more likely she is going to continue practice; the more failures the learner has in the preceding sequence, the less likely she is going to continue practice. This pattern arises because a higher rate of success implies a higher level of mastery, and consequently a lower probability to stop.</p>
<p>The same is true for the practice intensity. If the effort rate is positively correlated with the latent mastery, unconditional on the latent mastery, the more success the learner has enjoyed, the more effort she is likely to put into practice; the more failures the learner has suffered, the less effort she is likely to exert. This pattern arises because a higher rate of success implies a higher level of mastery and consequently a higher probability to exert effort.</p>
</div>
</div>
<div id="adaptive-practice-recommendation" class="section level2">
<h2><span class="header-section-number">2.4</span> Adaptive Practice Recommendation</h2>
<p>The classical Bayesian Knowledge Tracing (BKT) model cannot generate adaptive practice recommendations within a knowledge/skill domain. This somewhat surprising result is a consequence of the homogeneous learning assumption imposed by the BKT model: If the learner has no mastery, she learns with a constant learning rate. If the learner has mastery, she does not learn. To highlight the problematic implication of this assumption, consider an extreme case that a college freshman and a first grader learn first order differentiation. According to the assumption of the BKT model, if both learners have not learned the skill, they learn at the same rate for any given material.</p>
<p>This defect does not harm the Intelligent Tutoring Systems (ITSs) developed in the United States too much because major US ITSs focus on between domain objective individualization rather than within domain practice individualization. Use the math learning as an example. The ITS tries to sequence learning objectives that the learner needs to master in an optimal way. For instances, Khan Academy builds a knowledge tree and encourages learners to pass the test on parent nodes first before proceeding to their children nodes. The Cognitive Tutor follows a similar strategy to break a large learning goal into smaller skill-building blocks. Assessments and Learning in Knowledge Spaces (ALEKS) system adaptively changes the next learning objective given the learner’s accomplished objectives. For objective individualization, the key question is what pre-requisite objectives the learner should achieve to ensure that the current objective is attainable <span class="citation">(Doignon and Falmagne <a href="#ref-doignon2012knowledge">2012</a>)</span> and if such process does describe how people learn to acquire certain skills <span class="citation">(John R Anderson <a href="#ref-anderson2013language">2013</a>)</span>. Within the learning objective, the practice question is usually generated by an algorithm rather than selected from a pre-existing content library. These computer-generated practice items are highly substitutable, thus the question is whether the practice engine is effective rather than which question the engine generated is most effective <span class="citation">(Ritter et al. <a href="#ref-ritter2007cognitive">2007</a>)</span>. Therefore, practice individualization within a knowledge domain is not the major concern for the learning analytics community.</p>
<p>However, for some education service providers, the main problem is not objective individualization but practice individualization. Consider the problem of helping a teacher compile a homework from an existing content pool. The learning objective sequencing is set by the teacher (or the curriculum), while the item sequencing is left to the algorithm. The BKT model family is not very useful for solving the item sequencing problem, so some service providers, such as Knewton, resort back to the literature of computerized adaptive testing, which may not be a good framework for instructional recommendations because it assumes no learning possible in the first place.</p>
<p>The LTP model addresses the problem of practice individualization by introducing learner’s heterogeneous gain from practice. The construction of practice efficacy (<span class="math inline">\(\ell^{z;m,n}_j\)</span>) defines two types of learner heterogeneity. The state heterogeneity refers to differential learning gains for learners with different levels of mastery conditional on the same learner type. The type heterogeneity refers to differential learning gains for learners with different types conditional on the same mastery level. Reconsider the previous “learning first order differentiation” example. With three levels of mastery inspired by Vygotsky’s zone of proximal development, the first grader is defined as level-1 mastery who is not ready to learn, the college freshman is defined as level-2 mastery who is ready to learn but not proficient in the skill yet. The instruction has little efficacy on the level 1 learners but some efficacy on the level-2 learners. This is called state heterogeneity. For the college freshmen, some may be more versed in the mathematical reasoning than others, therefore the instruction may have different efficacies on learners with different susceptibilities. This is called type heterogeneity.</p>
<p>The learner engagement component of the LTP model introduces another dimension to practice individualization. Because item differs in its effort appeal condition on the type and the mastery state of the learner, there exists state heterogeneity and type heterogeneity in the effort appeal of the item as well. In general, the optimal practice item is both high in practice efficacy and strong in effort appeal. Heterogeneities on both aspects can give rise to a rich set of recommendation strategies.</p>
<!--chapter:end:02-model.Rmd-->
</div>
</div>
<div id="identification" class="section level1">
<h1><span class="header-section-number">3</span> Model Identification</h1>
<!--chapter:end:03-identification.Rmd-->
</div>
<div id="estimation" class="section level1">
<h1><span class="header-section-number">4</span> Model Estimation</h1>
<!--chapter:end:04-estimation.Rmd-->
<div id="refs" class="references">
<div id="ref-anderson2013language">
<p>Anderson, John R. 2013. <em>Language, Memory, and Thought</em>. Psychology Press.</p>
</div>
<div id="ref-anderson2000learning">
<p>Anderson, John Robert. 2000. “Learning and Memory.” John Wiley New York.</p>
</div>
<div id="ref-baker2010better">
<p>Baker, Ryan SJd, Sidney K D’Mello, Ma Mercedes T Rodrigo, and Arthur C Graesser. 2010. “Better to Be Frustrated Than Bored: The Incidence, Persistence, and Impact of Learners’ Cognitive–affective States During Interactions with Three Different Computer-Based Learning Environments.” <em>International Journal of Human-Computer Studies</em> 68 (4). Elsevier: 223–41.</p>
</div>
<div id="ref-doignon2012knowledge">
<p>Doignon, Jean-Paul, and Jean-Claude Falmagne. 2012. <em>Knowledge Spaces</em>. Springer Science &amp; Business Media.</p>
</div>
<div id="ref-holroyd2002neural">
<p>Holroyd, Clay B, and Michael GH Coles. 2002. “The Neural Basis of Human Error Processing: Reinforcement Learning, Dopamine, and the Error-Related Negativity.” <em>Psychological Review</em> 109 (4). American Psychological Association: 679.</p>
</div>
<div id="ref-pavlik2009performance">
<p>Pavlik Jr, Philip I, Hao Cen, and Kenneth R Koedinger. 2009. “Performance Factors Analysis–A New Alternative to Knowledge Tracing.” <em>Online Submission</em>. ERIC.</p>
</div>
<div id="ref-ritter2007cognitive">
<p>Ritter, Steven, John R Anderson, Kenneth R Koedinger, and Albert Corbett. 2007. “Cognitive Tutor: Applied Research in Mathematics Education.” <em>Psychonomic Bulletin &amp; Review</em> 14 (2). Springer: 249–55.</p>
</div>
<div id="ref-vygotsky1978interaction">
<p>Vygotsky, Lev. 1978. “Interaction Between Learning and Development.” <em>Readings on the Development of Children</em> 23 (3): 34–41.</p>
</div>
</div>
</div>
<!--bookdown:body:end-->
            </section>

          </div>
        </div>
      </div>
<!--bookdown:link_prev-->
<!--bookdown:link_next-->

<!--bookdown:config-->

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
