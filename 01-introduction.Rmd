---
output:
  pdf_document: default
  html_document: default
---

```{r, echo=FALSE}
library(knitr)
proj_dir = getwd()
```

# Introduction {#intro}

Intelligent Tutoring System is a key component of the future education with human expertise and artificial intelligence working in conjunction. It will prove to be a revolution that changes the productivity of education as we know it. Learning through practice, or learning by doing, is among many areas where such collaboration is possible. 

Practice, as defined by the Merriam-Webster dictionary, means "to do something again and again in order to become better at it". Although repetition is a necessary condition for improvement, it is not sufficient. A quantitative measurement of efficacies of practice to heterogeneous learners is the cornerstone of building an effective intelligent tutoring system that can generate practice recommendations adaptive to individual learnerâ€™s progress. It will further our understanding of what to practice and how to practice so that a learner can get better at what she is practicing. To achieve this goal, this thesis proposes a framework to define and estimate the practice efficacy that can be applied to describe a wide variety of learning processes.



## The Instructor's Problem and the Dual Role of Practice

If an algorithm recommends a sequence of practice questions to a learner, it should mimic what a good human instructor does when he interacts with a learner. The instructor assesses what the learner knows and what she does not know, then devises and executes a teaching plan. In the next interaction, the instructor uses the assessment again to gather feedback on the previous teaching plan, so that he can improve the current teaching plan. The two steps constitute an instruction loop that is repeated until the learner masters the material. If the instruction is limited to practice questions, it should also perform the two tasks. As an assessment tool, the result of the practice anchors inference of the learner's mastery. As an instruction tool, the process of practice elevates the learner's mastery. Given a fixed question bank, the challenge is to orchestrate the intelligent tutoring system to select and sequence question items. 

Computerized Adaptive Testing(CAT) offers a well-documented adaptive strategy of selection and sequencing. CAT aims to measure the testee's ability with a pre-specified precision by using as few questions as possible. Although the underlying mathematical mechanisms differ, CAT favors items that sharply differentiates whether the testee's ability is above or below a certain threshold. If the testee's ability is above the threshold, she will solve the problem; otherwise, she will fail. Although it is a sound principle for assessment, it is not a sound strategy for instruction. Good practice enables a learner to solve problems that were previously out of her reach with the help of instructions or hints. A good practice question can be a bad exam question, and vice versa. Therefore, CAT is not sufficient to build a good intelligent tutoring system because it is not the optimal solution for creating practice as instruction.

The necessity for an adaptive instructional strategy stems from the learners' differential gain from exposure to the same instruction, known as the learner heterogeneity. Otherwise, a one-size-fits-all teaching plan would suffice. If efficacies of a practice problem to different types of learners are known, as more responses are observed, the system can better infer the learner's information and choose the next practice item most effective to her type and mastery level. Therefore, a quantitative measurement of the practice efficacies to heterogenous learners is key to completing the instruction step of the intelligent tutoring system. 

## Mastery, Learning, and Practice Efficacy

The previous section highlights the importance of developing a measure of "practice efficacy". This section conceptually defines efficacy in relation to a learning process. The learning process is defined in relation to the concept of mastery.

Mastery is defined as the capability to solve a  problem or perform a task in a particular domain. Although the definition does not exclude transferable skill or creativity, this thesis focuses on the muscle memory aspect of the mastery: the ability to solve similar problems. For example, the mastery of solving two-variable equations requires the learner to solve any set of linear equations with two unknowns. However, it does not require the learner to recognize that such technique can solve for the price-quantity equilibrium of a one-good linear supply and demand market system. Further, the notion of "capability" implies that the mastery is a conceptual construction and thus not directly observable. The observed response to a problem is a noisy measure of the true mastery. A learner without mastery can solve the problem out of sheer good luck, while a learner with mastery may fail it due to bad luck. Although the mastery is unobserved, it can be inferred from the observed solution to the problem.

Learning is defined as a process in which a learner becomes capable of solving a problem that she is unable to previously. There are many modes of learning other than problem-solving, e.g. self-reflection on previous mistakes. This thesis only focuses on learning in terms of problem-solving. Practice is defined as solving a sequence of problems. The notion of "sequence" emphasizes the repetitive nature of practice, which is echoed in the Merriam-Webster's definition. The old idiom "practice makes perfect" still rings true. Repetitive practice is the cornerstone of expertise in many professions [@ericsson1993role], be it in sports, arts or science. The bad reputation of "teaching to the test" is a result of a misguided construction of mastery. If the test authentically represents the problems a learner needs to solve in order to be successful, teaching to the test is a good instructional strategy. This thesis will not venture to discuss how mastery should be defined, but rather to study the impact of practice on learning given a pre-defined mastery and a set of problems.

To characterize learning, it is necessary not only to characterize the current level of the mastery but also the change of mastery. Had the observed responses been a perfect signal of mastery, the characterization would have been an easy task. Consider the simple case that a learner solves two problems, failing the first time then succeeding the second time. If the outcome accurately reflects the state of latent mastery, one can infer the learner has no mastery while attempting the first time but has since gained mastery. Unfortunately, the observed response is only a noisy measure. Given the same sequence of observations as above, one can argue the learner has no mastery and she only got lucky the second time, or that the learner has always had mastery and only got sloppy the first time, or that the learner gains mastery by learning from her first failure. The fog of inference requires the analysis to take a stand on whether the level of mastery can change during the practice sequence. It is a subtle but critical difference. The analysis can either view the practice sequence as a static description of the current mastery, or view it as a dynamic trajectory of the historical mastery levels. The static view reduces the complexity of inference at the cost of eliminating the possibility of learning through practice. 

The prevailing psychometrics literature takes the static view. The family of models derived from Item Response Theory (IRT)  [@rasch1960probabilistic;@Carlson2013IRT] requires local independence as a fundamental identification assumption [@lord1980applications]: Conditioned on the latent ability, the response to each item is independent of one another. Although the local independence assumption does not explicitly state that the analysis is conditioned on a constant latent ability, IRT models commonly assume that it is the case. Because the static view excludes the possibility of learning, the IRT models are unsuitable to study a learning process.

When taking the dynamic view of mastery, the measurement of the pedagogical efficacy of practice quantifies the capability of the problem to change the latent mastery. The operational definition of efficacy depends on the operational definition of mastery and change. For example, if mastery is assumed to be a binary variable and change is defined as the transition from the non-mastery (0) to mastery (1), the efficacy can be defined as the probability of such change. If mastery is assumed to be a continuous variable and change is defined as the increment of the mastery score, the efficacy can be defined as the magnitude of such change. The next section describes the Bayesian Knowledge Tracing model family that makes the binary mastery assumption.


## The Bayesian Knowledge Tracing Model

In learning analytics literature, the Bayesian Knowledge Tracing (BKT) model is a classic representation of the dynamic learning process [@corbett1994knowledge]. The BKT model formalizes the intuition that "practice makes perfect": A learner achieves mastery by repeated exercise in a probabilistic learning process. The BKT model family is the main user modeling engine in many intelligent tutoring systems (ITS), most notably the Cognitive Tutor [@aleven2002effective;@koedinger2006cognitive;@ritter2007cognitive] by Carnagie Learning LLC and the ASSISTment [@pardos2010modeling] by Worcester Polytechnic Institute.

The Bayesian Knowledge Tracing model assumes that the learner's mastery level has only two states: non-mastery and mastery. A learner's initial state is characterized by the probability of having mastery. If the learner has no mastery, at each practice opportunity, she has a probability to transit from non-mastery to mastery. The transition probability is called learning rate in the literature and pedagogical efficacy in this thesis. If the learner attains mastery, she never regresses upon further practice. Because the observed response is a noisy measure of the latent mastery, the BKT model uses a guess rate to measure the probability that the learner guesses correctly when she has no mastery and a slip rate to measure the probability that the learner makes an accidental error when she has mastery. 

The Bayesian Knowledge Tracing model breeds a family of learning models that employ the same structural representation of the learning process. The main extension of the BKT model is to relax the strong assumption of learner homogeneity by allowing learners to be different in their initial mastery probability [@pardos2010modeling], the slip and guess rate [@d2008more] or the learning rate [@lee2012impact;@yudelson2013individualized]. However, none of the authors question the assumption of two-state latent mastery. The binary assumption is not innocuous because it rules out the possibility of positive feedback. 


The model identification is another major research interest in the BKT literature. Beck and Chang [-@beck2007identifiability] first argued that the BKT model is not uniquely identified. They claimed that different parameter sets with very different interpretations of the learning process result in the same learning curve, therefore the model is not identified. Their work motivated some later development of the BKT model to use individualized parameters to address the identification issue [@d2008more;@rai2009using;@pardos2010modeling]. Unfortunately, Beck and Chang are wrong about the BKT model being unidentified, because different parameter sets produce the same learning curve but different likelihoods. In short, the identification of the BKT model is not properly analyzed in the literature. 

This thesis intends to address both problems in the current research: the restrictive latent model structure and the lack of formal analysis on the model identification.


## Learner Engagement

Instead of defining all these concepts and constructing an abstract model, an alternative, and simple, approach is to measure the instructional efficacy as the difference in the probabilities of correctly answering a question before and after the pedagogical intervention. One such example is the "Learning Gain" metric used by the Khan Academy, a popular K-12 online learning platform, to evaluate the instructional quality of its videos [@faus2015systems].  If a learner is engaged in learning for as long and as focused as the pedagogical method requires, such measure is qualitatively the same as the pedagogical efficacy defined by a structural model in a large sample. However, the learner engagement is not perfect in low-stakes learning environments, such as school lectures or self-learning with a computer tutor. 

There is an emerging literature on the intensity and the duration of learner engagement. As for the intensity of learner engagement, Ryan Becker [-@baker2004off;-@baker2004detecting] first introduced the concept of "gaming the system" to describe the learner's off-task behavior when using an intelligent tutoring system. Later research shows that such off-task behavior is not only prevalent during the instruction [@baker2010better] or using the hints [@koedinger2007exploring;@wixon2012wtf] of the intelligent tutoring system but also in classroom learning [@pardos2013affective]. As for the duration of learner engagement in the context of practice, Murray et al [-@murray2013revealing] shows that the learner's practice sequence length varies substantially in the Cognitive Tutor and such differential attrition biases the inference of learning progress if only the aggregate data are used.

Imperfect learner engagement implies that a practice problem may improve learner's mastery under the ideal condition, but would fail to do so in reality. The distinction between efficacy, the impact of such method under ideal conditions, and effectiveness, the impact of such method under realistic conditions, is well understood by the medical literature [@flay1986efficacy;@glasgow2003don;@flay2005standards]. Although effectiveness is the ultimate goal, the research may better understand the nature of the ineffectiveness by separating inefficacious trials and efficacious but ineffective trials. The development of an intelligent practice system can also benefit from the same insight. If the practice has no efficacy, the material should be eliminated from the instructional content pool. If the practice is efficacious but the learner is not engaged, the instructor (or the learning product manager) can provide more incentives and improve the interface to better engage the learner.

However, in contrast to the well-controlled lab environment in medical research, learning data are usually collected in an imperfect learning environment where learners do not always exert their best effort. The challenge is to separate the efficacy from the effectiveness in such dataset. There have been attempts to account for engagement intensity [@feng2009addressing] and duration [@pelanek2016impact] within the Bayesian Knowledge Tracing framework. However, these adjustments are ad hoc and do not have a good theoretical foundation. This thesis incorporates the learner's engagement as a part of the Learning Through Practice model. By doing so, it provides an analysis of the bias as well as the method to correct it.


## Overview of the Thesis

The second chapter lays out the learning through practices (LTP) model. The chapter first describes the learning process without learner engagement as a Hidden Markov Model, which admits an arbitrary number of states of latent mastery and observed response. It shows that the Bayesian Knowledge Tracing model is a special case of the LTP model. The chapter then uses the stop decision to describe the engagement length and the effort decision to describe the engagement intensity. The learning model is extended to account for the impact of these decisions on the learning dynamics. The LTP model augmented with learner engagement can be used to describe a variety of learning processes, such as reinforcement learning and zone of proximal development.

The third chapter discusses the identification assumptions of the LTP model. By reparametrizing the LTP model as a multinomial distribution, the sample frequencies of the joint distribution of item assignments, observed responses, stop decisions and effort decisions are sufficient statistics of the system and moment conditions for identification. It can be proved that a local optimum parameter set exists only if the number of the parameters are smaller than or equal to the number of moment conditions, and that Jacobian matrix of the moment conditions at the optimum solution has full column rank. This necessary condition for identification puts an upper limit on the number of model parameters. In the special case of the Bayesian Knowledge Tracing model, this chapter will provide the sufficient identification conditions by solving the moment conditions explicitly. The estimation algorithm used in this thesis also requires rank order conditions to prevent label switching.

The fourth chapter describes the Monte Carlo Markov Chain algorithm used to estimate the LTP model. The MCMC routine first augments the latent states with Forward Recursion Backward Sampling algorithm given the parameters, then uses the Gibbs sampler to update the parameters given the augmented data. In the second step, if the conditional likelihood has a conjugate prior distribution, the Gibbs sampler draws from the conjugate posterior distribution. If it does not have a conjugate prior distribution, the Gibbs sampler draws new parameters by the Adaptive Rejection Sampling algorithm. This chapter uses simulation data to demonstrate that the MCMC algorithm can estimate the parameters of a learning model that have three states of latent mastery, the stop decision, and the engagement decision with the reasonable precision.

The fifth chapter applies the LTP model to analyze the dynamic selection bias of estimated efficacy due to selective sample attribution. Different lengths of practice sequence are a common feature in a learning dataset. This chapter shows that selective sample attrition is not a sufficient condition to bias the estimation of practice efficacy of the BKT model. If learners exit the practice sequence based on an exogenous rule (stop-by-rule), even though it may be a selective attrition, both the LTP model and the LTP model consistently estimate the pedagogical efficacy. If learners exit the practice sequence based on their own choice (stop by choice), only the LTP model consistently estimates the pedagogical efficacy. The chapter applies the LTP model to a quiz dataset on two digit multiplication and long division. In that dataset, the majority of the observed change of the success rate may be attributed to selective sample attrition rather than practice efficacy. The selective attrition accounts for at least 50% of the observed success rate increment in the two digit multiplication quiz and 75% of that in the long division quiz.


The sixth chapter applies the LTP model to evaluate efficacy in low stake learning environment where measure errors (i.e. frivolous wrong response) due to a lack of effort abound. Randomized Control Trial is an important method to evaluate relative the pedagogical efficacy of different practice materials. Usually, the data are analyzed with a Difference in Difference (DID) regression which measures the relative effectiveness of the practice materials. This chapter shows that the relative effectiveness may not have the same ranking order of relative efficacy when the effort-induced measurement error is present. It further argues that the LTP model that accounts for mastery-dependent effort decision correctly recovers the relative efficacy of the experimental data. The chapter applies the LTP model to an experiment that compares the efficacies of practice questions with or without a video instruction set. The case study details the classification of effort-induced error and the evidence of differential effort rates between two groups. Whereas the DID estimator shows no significant difference between two questions, the LTP model strongly suggests that the question with video instruction has a superior efficacy when controlled for effort input, which is consistent with the pedagogical expert's prior expectation. 
