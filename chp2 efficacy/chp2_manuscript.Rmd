---
title: "Experimental Evaluation of Content Effectiveness"
author: "Junchen Feng"
date: "Saturday, June 9th, 2016"
output: pdf_document
header-includes:
- \usepackage{setspace}
- \doublespacing
- \usepackage{amsmath}
bibliography: reference.bib
---


# 1 Introduction

The past few years have seen a boom of digital learning service provider around the globe. Adaptive learning recommendation is a popular concept in this wave digital education. Although adaptive learning is intuitively appealing, there is little empirical evidence that it is effective. [**TODO: cite Khan evaulation paper and knewton white paper**]. 

This is in sharp contrast to the success story of recommendation engine in other service sectors, from online shopping, such as Amazon, to entertainment, such as Netflix, or even dating, such as OKcupid. The common feature of empirical successful recommendation system is that they measure and cater to the latent preference, which is assumed to be stable. For example, it is enough for Netflix to know that the viewer likes the genre of dark political drama and then to recommend the "House of Cards". 

In contrast, adaptive learning need to measure and change the latent ability. At least for K-12, the curriculum aims to raise student's latent ability to a certain standard, rather than to only evaluate it. In the Netflix analogy, the common core requires the viewer to enjoy dark political drama yet the viewer currently hates it. An effective adpative learning system needs to recommend sequence of movies or tv shows that change the viewer's taste. Obviously, it is a much harder question to answer. This may partly explain why recommendation system built on item response theory(IRT) models has yet to report empirical success: The IRT model is good at describing the level of the user latent ability but is bad at predicting the change in user ability.

To understand how people learn, one looks to cognitive learning theory for help. Unfortunately, it does not provide an effective framework to guide the design of learning system. Or if there is one, the learning science community has yet to pick it up. The only learning science model that describes the leanring process is the learning curve. However, the learning curve assumes that content is homogeneous. For routine learning task, the homogeneous content effectiveness is a reasonable assumption. However, for learning task that requires understanding, tranferring skill or creativity, what content to learn and how to learn it seems to be critically important. After all, there is a reason for teacher to exists.

Ironically, the lack of learning theory is met with an abundance of learning data. Thanks to the the spread of smart computing devices and internet access, the collection of response data is cheap and standard (and soon to be ubiquitous). An educated guess is that billions of response data are stored every day in various company server. There is on going effort to add process data and behavior data into the mix so that the learning environment and learning process can be reconstructed later as well. Yet without a theory, the more data are collected, the harder it is to make sense.

Experiment is one way to bridge theory and data. However, offline evaluation experiment is expensive and time consuming. The online experiment is a new frontier where learning theory can be tested frequently and cheaply. Data driven pedagoical research sounds like a no brainer. After all, the necessity for rigorous evaluation method is well established in the education community (** what works clearing house citation needed**) and using experiments to improve service quality has been an import theme for online service sector[@scott_multi_2015].

Naturally, there has been proposal to conduct experiments in the online education platform to advance learning science and improve learning performance[@williams_mooclet_2014]. There has been a few pilot large scale experiments implemented on learning service platform (Notably Standford Lab, **citation required**). However, the data driven micro-approach has yet to catch fire. Other than bias and prejudice, the online mass educational experiment suffers from methodological weakness resulted from the nature of the new medium, if the quantity of interest is learning outcome, sample attrition and (effort induced) measurement error.

# 2 Weakness of Massive Online Educational Experiment 
## 2.1 User Attrition

Compared to offline experiments, massive online experiment has an extremely high attrition rate due to the voluntary nature of the online learning service.  

Unlike instaneous outcome usually measured in online experiments, such as click rate or email response rate, learning gain requires at least two measurements to identify. The success rate per se does not indicate the learning gain. A difficult question can be enlightening. Users enrolled in the baseline test may not be observed again in the post test, attriting from the sample for analytical purpose. In contrast, offline education experiment conducted in a school setting does not need to sweat over user attrition as much because school attendance or high stake test is somewhat mandatory. 

The sample attrition does not equal selection bias for the average treatment estimation. First of all, the "population" of interest needs to be defined. For practical purpose, the population of interest is the active user of the website/app. Except for the notable few exception such as facebook, month by month user retention rate falls between 30% to 60%. For example, if the learning service provider has a monthly retention rate of 20%, an experiment that runs for a month is expected to loose 80% of the enrolled participants, yet still be representative of the active users, as long as the user drop out is orthogonal to the experiment per se.

However, the possibility that user stop using the service because of the experiment cannot be ruled out logically. Such user attrition results in selection bias. If the experiment is implemented in one module, the risk of selective attrition is not eligible. From field experience, the most likely confounding factor of learning gain and user attrition is grit. Learning gain is only made if users overcomes their previous ignorance/failure. If they run away at the first sight of trouble, they are less likely to produce positive outcome.

The natural attrition leads to loss in precision, which can be compensated by the large user base. The selective attrition leads to loss in consistency, which has to be addressed delibrately by research design. One way to distinguish the natural attrition from selective attrition is to check if the users continue to be active other modules. However, depends on the nature of the product environment, such method may not always be reliable. For example, if the experiment is hosted in a paid product with free "tasting" period, users that are active in the website but inactive in the specific product may have nothing to do with the experiment. In general, without product specific auxilary information, natural attrition and selective attrition is observational equivalent.

There are two approaches to attach the high user attrition. The first approach is to use some kind of instrument to explictly model the selective attrition. Appendix I outlines a solution that uses exogeneous incentive difference to mitigate selective attrition. It should be noted that such remedy still recovers LATE, rather than ATE. In addition, its engineering complexity makes it relative expensive to apply in mass. The second approach is to compress the experiment into one session so as to minimize attrition. This approach is simple and effective but limits what the experiments can do. This paper mainly describes how one-session experiment system can be used to measure content effectiveness.



## 2.2 Effort Induced Measurement Error
The large fraction of measurement error is due to the low stake of the online learning task.  

Knolwedge mastery is a latent variable. It can only be inferred from observed test response. The classical model of learning analytics, be it Item Response Model or Bayesian Knowledge Tracing, assumes a link function where knolwedge mastery is the only argument.

However, a growing body of research distinguishes low performance due to lack of mastery and low performance due to lack of effort. Baker et al has (**citation needed**)studied the "off-task" behavior on digital learning while Levitt et al (**citation needed**) has shown that lack of effort may account for about 2/3 of the race gap in achievement as measured by low stake test. This paper attributes knowledge mastery to cognitive skill while effort, for a lack of better terms, to non-cognitive skills. For example, when african american students score low on the performance test due to the culture of "not acting white"**(Citation required)**, the low score does not necessary imply that they have a low level of cognitive skills. Other non-cognitive traits also contribute to the level of student's mental effort, such as resistence to boring task or grit over challenges. 

In online learning environement, the learning task is usually low stake because they are used as formative assessment or auxilary self-learning material. In addition, most online learning system allows student to try again after the first failure. The benevolent intention to allow for experimentation unintentionally breeds moral hazard, where student exploits the design to peek the answer by fail blindly in the first attempt, especially if the remedial exercises results in handsome reward.

With moderate assumption, it can be proved that effort induced measurement error biases the average treatment effect upwards. Appendix II *[TODO]* lays out a sketch of formal proof. The intuition is that give-up attenuates the success rate, more so in the lower ability region. Because treatment moves students to higher level of the latent mastery, the attenuation is lessened, thus the treatment impact is over-estimated. 

The identification of effort induced measurement error is not a trivial task. Without the behavior data that characterize how students answer the question, the error due to a lack of trying and the error due to a lack of mastery is observational equivalent. Even with the aid of response time and response answer, it is not an easy task to distinguish between error due to effort and error due to mastery. In the extreme case where student gives up immediately and leave a blank answer, it can still be blamed on error due to mastery: student has failed the same task before and they ratioanlly give-up the second chance if the goal is to maximize reward collection rather than learning gain. It is imprecise to characterize continuous effort by a binary status, but a necessary generalization to make the identificaiton feasible.

The current paper defines the error due to a lack of effort as "wrong answers except for reasonable error". In the experiment analysis section, the definition and identification will be discussed in details. However, this method is impractical for large scale adoption because such identification relies heavily on costly human identification. Appendix III *[TODO]* describes a more algorithm friendly definition of effort induced measurement error. It is less accurate but more automatic with few meta parameter to tune. 


# 3.Experimental Evaluation of Content Effectiveness

Content provider is very interested in evaluating the instructional effectiveness of the item of the content inventory. Instructional effectiveness is different from measurement effectiveness, the later of which is usually described by parameters of item response theory models. To author's the best knowledge, the rating of content instructional effectiveness relies on expert judgement or the reputation of the content source. To illustrate the later point, the best selling test prep book for Chinese College Entrace Exam (CCEE), "3 year 5 year", is a compilation of the test instrument of the CCEE for the last five years and that of the CCEE-prep exam administrated by provincial or municipal education authorities of the last three years. The popularity of the "3 year 5 year" series is a testimony of the importance of content source authority, rather than empirical evidence or pedagogical theory, in branding the content effectivness. 

This paper uses the evaluation of content effectivness as a case study of how empirical data can support and advance pedagogical research. The experiment documented here itself maybe less interesting, it sheds import light on the over-arching research question is whether or not content effectiveness can be evaluated by experimental method.

## 3.1 Research Desgin

### 3.1.1 The Learning Objective

The primary learning objective is calculate the circumference and area of a rectangle merged from two small rectangles. The secondary target is the reverse process. To be specific, the original question, shown as figure 1, asks the student o calculate the circumference and area of a large rectangle joined from two smaller rectangle by the length.

![orig](C:/Users/junchen/Desktop/pedexp/fig/f1.png)

Figure 1: The Original Question

The assessment for the primary objective (shown as figure 2) asks student to calculate the same quantity for two rectangles joined by width. Using a similar yet non-identical item enhance the measurement validity yet avoid student memorizing the answer.

![assess1](C:/Users/junchen/Desktop/pedexp/fig/f3.png)

Figure 2: Assessment for the Primary Objective

The assessment for the seconary objective (shown as figure 3) asks student to calculate the additional circumference if a large rectangle are broken into two smaller rectangles by width. 

![assess2](C:/Users/junchen/Desktop/pedexp/fig/f4.png)

Figure 3: Assessment for the Secondary Objective



### 3.1.2 The Learning Task
The training question is a modified version of the original question(shown as figure 4). The question keeps everything from the original other than the value of the length and width, to 6 and 4 respectively. It should be noted that the training question also preserves the feature that the joined width is the new length.

![train](C:/Users/junchen/Desktop/pedexp/fig/f2.png)

Figure 4: Train question

The variation of learning task is in the format that the training question is delivered.

The first teaching method is to use sub-questions to mimic the actual teacher intervention. The sub questions involves the following 3 steps:

(1) What is the length and width of the new rectangle
(2) What is the circumference of the new rectangle
(3) What is the area of the new rectangle

The second teaching method is to use a video instruction that explains the 3 steps in cartoon animation. 

The control group gets the training question and nothing else. The treatment I group receives the training question with scalefolding. The treatment II group receives the training question with video lecture. All three groups receive the original question and the two assessment questions.

The learning task is restricted to 4 items so that student can finish them in one setting, consequently minimizing the user attrition. Previous experiment has demonstrated that a 3 month duration experiment has a retention rate of 0.1% and 12% for a 14 day duration experiment. The experiment reported here has a retention rate of 84%, not perfect yet qualitatively reassuring. 

### 3.1.3 The Learning Environment
The experiment is carried out in a paid self-learning product offered by a Chinese online learning service provider. The product is presented as a role playing game where user can claim reward once they clear a level, defined as acculumating 12 right answers. The user can re-try a similar question of the the wrong one in a separate unit to claim additional reward. In addition, if the user drops out in the middle of a level, when they log in again, the wrong items will be shown again while the right items are skipped. The overall design aims to encourage students to "practice until right".

The reward is not cash incentive since it cannot be directly cashed out. They are virutal currency that can only be used to purchase in game gears or exchange real gifts. When exchange for gift, the exchange rate is 1:10000. The reward per level is about 10, which translates to a tenth of a peny in RMB, thus negligible in real value. That said, users have exihibited severe "nominal illusion": they mine the virtual currency at about the same rate desipte waves of massive inflation.

A few efficient reward mining patterns have been spotted. One pattern is to skip any difficult items by submitting blank answer to fish for easy questions. The other pattern is to jump out the level whenever makes a wrong answer then enter again with the knowledge of the right answer peeping from the hint section. Because these non-learning oriented behavior, the collected data have non-trivial measurement error.

### 3.1.4 Experiment Design

The experiment is administrated from June 9th to June 10th to third grade users who used the paid learning product. Users are divided into five groups by remainder of their user id divided by 5. The remainder 0,2,4 are control, treatment I and treatment II respectively. About 10 thousand users are recruited. Users are not aware that they are part of an experiment.

## 3.2. Hypothesis and Identification Strategy

The expert postulates that both scalefolding and video lecture are better than the vanilla training questions, which simply changes the number. She also postulate that although scalefolding and video lecture may be as effective in reaching the primary learning object, the video lecture is more effective at obtaining the secondary learning objective.

It should be noticed that the experiment is not trying to pin down the precise learning gain of the content but its relative ranking. The key conceptual difficulty in measuring the learning gain is to distinguish between true elevation of latent mastery and a surge of short term memory. Yet under the assumption that short term memory surge is same for all groups because the exposure time is about the same, the difference in learning gain can be recovered.

The experiment is designed with a difference in difference identification strategy in mind. Denote the baseline question as T=0 and the assessments questions as T = 1. Let variable $D^1$ takes value 1 if the user is in the treatment I, variable $D^2$ takes value 2 if the user is in treatment II. A fully specified DID model is 

$$
Y_{i,T} = \beta_1 D^1_i + \beta_2 D^2_i + \gamma T + \delta_1 D^1_i T + \delta_2 D^2_i T + \epsilon_{i,T} 
$$

To facilitate the test whether the two treatment has the same effect, let $\tilde{D}_i$ take value 1 if the user is not in the control group. Rewrite 

$$
\delta_1 D^1_i T + \delta_2 D^2_i T = (\delta_0 + \delta_2 D^2_i)(\tilde{D}_i) T 
$$

Therefore, the three hypothesis can be written as the following 

(1) Both treatment are better than vanilla. $\delta_0^{Primary}>0$, $\delta_0^{Secondary}>0$

(2) Scalefolding and video lecture are as effective in reaching the primary objective. $\delta_2^{Primary} = 0$

(3) Video lecture is more effective in reaching the secondary objective. $\delta_2^{Secondary} > 0$


### 3.2.1  Give-up identification
Since the give up is identified as "wrong response without a reasonable answer", one must define what reasonable answer is, which depends on the context of the question. This section illustrate the process with the original question. The full identification process is described in the appendix III.

The right answer for the original is that circumference is 26 and the area is 80. Reasonable answer can be errors of the following type or a combination of the following type:
(1) quadruple the sum of length and width of the small rectangle
(2) quadruple the area
(3) calculate the circumference and the area of the small rectangle
(4) Calculate either circumference or area
(5) add extra 0(s) to the answer

In general, the paper leans toward leinent definition of effort. Any manifestied brain work qualifies as effort. For example, circumference 23, area 40 is a pretty far fetch answer since the student even fails to correctly calculate the parameter for the small rectangle, yet it is classified as valid effort.
```{r,echo=FALSE,warning=FALSE,error=FALSE,message=FALSE}
library(dplyr)
library(tidyr)
library(ggplot2)
library(stargazer)
load("C:/Users/junchen/Desktop/pedexp/data/work_dataset_exp_giveup.RData")

baseline_log = meta_24921 %>% filter(eid=='Q_10201056666357')

wrong_pct = round(mean(1-baseline_log$atag)*100,2)
giveup_pct = round(mean(baseline_log$giveup)/mean(1-baseline_log$atag)*100,2)

```

The miss rate for the baseline question is `r wrong_pct` %. Among all the wrong answers, the giveup rate is `r giveup_pct`%. 

## 3.3 Results

The first set of models retain the first attempt and users who have done all four items. The retention rate is 74% on average, 77% for the control group, 72% for the scalefolding and 70% for the video lecture.

The result suggests that none of the intervention is statistically better than the the vanilla train question, scalefolding is actually worse in obtaining the primary learning objective. However, the video lecture appears to be more effective than the scalefolding.

```{r,echo=FALSE,warning=FALSE,error=FALSE,message=FALSE,results='asis'}
# ETL
# retain the first attempt
valid_meta_24921 = meta_24921 %>%
    group_by(uid,eid) %>% 
    arrange(uid,eid,cmt_time) %>%
    slice(1) %>% 
    ungroup() %>%
    arrange(uid,cmt_time)

# retain the 0,2,4
valid_meta_24921 = valid_meta_24921 %>% transform(gid=uid%%5) %>% filter(gid%in%c(0,2,4))

# retain the full data
user_sum = valid_meta_24921 %>% 
    group_by(uid,gid) %>%
    summarize(k=length(unique(eid)))  %>%
    transform(is_valid_user = k==4) 
valid_user_data = valid_meta_24921 %>% filter(uid %in% user_sum$uid[user_sum$is_valid_user])


# construct the lamest dataset

y0data = valid_user_data %>% filter(eid=='Q_10201056649366') %>% select(uid,gid,atag_pct,giveup) %>% rename(y=atag_pct)  %>% mutate(t=0)
y1pdata = valid_user_data %>% filter(eid=='Q_10201056666357') %>% select(uid,gid,atag_pct,giveup) %>% rename(y=atag_pct) %>% mutate(t=1)
y1sdata = valid_user_data %>% filter(eid=='Q_10200351208705') %>% select(uid,gid,atag_pct,giveup) %>% rename(y=atag_pct) %>% mutate(t=1)

ydata_p = rbind(y0data,y1pdata)
ydata_p = ydata_p %>% transform(d1=as.numeric(gid==2),d2=as.numeric(gid==4),d=as.numeric(gid!=0))
ydata_p = ydata_p %>% transform(dt=d*t,d2t=d2*t,d1t=d1*t)

mod_1 = lm(data=ydata_p,y~d1+d2+t+d1t+d2t)
mod_1d = lm(data=ydata_p,y~d1+d2+t+dt+d2t)


ydata_s = rbind(y0data,y1sdata)
ydata_s = ydata_s %>% transform(d1=as.numeric(gid==2),d2=as.numeric(gid==4),d=as.numeric(gid!=0))
ydata_s = ydata_s %>% transform(dt=d*t,d2t=d2*t,d1t=d1*t)

mod_2 = lm(data=ydata_s,y~d1+d2+t+d1t+d2t)
mod_2d = lm(data=ydata_s,y~d1+d2+t+dt+d2t)
stargazer(mod_1,mod_1d,mod_2,mod_2d, header=FALSE,type='latex')
```

To understand the puzzling result, sample those who are successful in their first attempt. Since there is neglible chance that they guessed it right, they must have mastered the primary learning objective and thus treatments should be placebo and show no effects. The regression shows that scalefolding has a strong negative effect for the primary learning objective. 

```{r,echo=FALSE,warning=FALSE,error=FALSE,message=FALSE,results='asis'}
placebo_uids = y0data$uid[y0data$y==1]
ydata_p$placebo = 0
ydata_p$placebo[ydata_p$uid %in% placebo_uids] = 1
ydata_s$placebo = 0
ydata_s$placebo[ydata_s$uid %in% placebo_uids] = 1


mod_3 = lm(data=ydata_p %>% filter(placebo==1),y~d1+d2+t+d1t+d2t)
mod_4 = lm(data=ydata_s%>% filter(placebo==1),y~d1+d2+t+d1t+d2t)
stargazer(mod_3,mod_4, header=FALSE,type='latex')

```

The negative effect can be mostly blamed on the increasing likelihood of giveups in the treatment I group. They are too bored to take the easy scalefolding serious.
```{r,echo=FALSE,warning=FALSE,error=FALSE,message=FALSE,results='asis'}
mod_5 = lm(data=ydata_p %>% filter(placebo==1),giveup~d1+d2+t+d1t+d2t)
mod_6 = lm(data=ydata_s%>% filter(placebo==1),giveup~d1+d2+t+d1t+d2t)
stargazer(mod_5,mod_6, header=FALSE,type='latex')
```

Focus on those failed the first attempt, almost all the effects are gone, at least statistically.
```{r,echo=FALSE,warning=FALSE,error=FALSE,message=FALSE,results='asis'}
mod_7 = lm(data=ydata_p %>% filter(placebo==0),y~d1+d2+t+d1t+d2t)
mod_7d = lm(data=ydata_p %>% filter(placebo==0),y~d1+d2+t+dt+d2t)

mod_8 = lm(data=ydata_s%>% filter(placebo==0),y~d1+d2+t+d1t+d2t)
mod_8d = lm(data=ydata_s %>% filter(placebo==0),y~d1+d2+t+dt+d2t)

stargazer(mod_7,mod_7d,mod_8,mod_8d, header=FALSE,type='latex')
```

Retain those did not give up on the training question, as to ensure they are at least somewhat exposed to the learning task. The regression shows that scalefolding is almost as effective as changing the number while video lecture is statistically more effective in obtaining the primary objective but not so in obtaining the secondary objective. However, it should be noted that the retention rate is about 49% for the control group and the scalefolding but 44% for the video lecture group, the margin is about 5% as well. The positive effect can be proved spurious if the good faith error are misclassified as giveup for the video lecture train question. In short, the experiment 


```{r,echo=FALSE,warning=FALSE,error=FALSE, message=FALSE, results='asis'}
failure_user_log = valid_user_data %>% filter(!(uid %in% placebo_uids))

failure_user_log$seq_id = 2
failure_user_log$seq_id[failure_user_log$eid=='Q_10201056649366'] = 1
failure_user_log$seq_id[failure_user_log$eid=='Q_10201056666357'] = 3
failure_user_log$seq_id[failure_user_log$eid=='Q_10200351208705'] = 4

user_giveup_profile = failure_user_log %>% select(uid, seq_id, giveup) %>% spread(seq_id, giveup) 
names(user_giveup_profile) = c('uid','q1','q2','q3','q4')

#user_giveup_sum = user_giveup_profile %>% transform(gid=uid %% 5) %>% group_by(gid) %>% summarize(mean(q2==0))

retain_user_profile = user_giveup_profile %>% filter(q2==0)


mod_9 = lm(data=ydata_p %>% filter(placebo==0) %>% filter(uid %in% retain_user_profile$uid),y~d1+d2+t+d1t+d2t)
mod_9d = lm(data=ydata_p %>% filter(placebo==0) %>% filter(uid %in% retain_user_profile$uid),y~d1+d2+t+dt+d2t)

mod_10 = lm(data=ydata_s%>% filter(placebo==0) %>% filter(uid %in% retain_user_profile$uid),y~d1+d2+t+d1t+d2t)
mod_10d = lm(data=ydata_s %>% filter(placebo==0) %>% filter(uid %in% retain_user_profile$uid),y~d1+d2+t+dt+d2t)

stargazer(mod_9,mod_9d,mod_10,mod_10d, header=FALSE, type='latex')
```


# 4. Discussion and Future Research
The paper illustrate the possibility of data driven pedagogical research with an experiment on evaluating content effectiveness. Although the threat of selection bias is not eliminated beyond doubt, the method proves to be feasible, scaleable and reasonable. 

The future revision of the paper will include:

(1) Test different way of identifying give-ups

(2) Analyze the impact of different treatment on effort

(3) correct for the selection bias.


# Appendix I: Mitigate Attrition Bias
## I.1 Modelling Sample Attrition

### I.1.1 Notations
The model has two periods, labeled subscript $t$. The first period $t=0$ is the baseline period. The second period $t=1$ is the post treatment period. The treatment period is not explicitly modelled. 

The model has two groups, labeled superscript $D$. $D=1$ is the treatment group, and $D=0$ is the control group.

Let $L=1$ indicate that the user is active in the post treatment period while $L=0$ inactive.

If the user is active but missing from the observation in the second period, the user is labeld as missing $M=1$. 

Denote the observed outcome at $Y$, observed user characteristics for the outcome as $X$ and the exclusion restriction as $Z$.

### I.1.2 An Extended Roy Model
The outcome at baseline is

$$
Y_0 = X\beta_0 + \epsilon_0
$$

The outcome at post treatment is

$$
Y^D_1 = X\beta_1 + \epsilon_1 + (\delta+\eta)*D
$$

$\epsilon_t$ is basic error term at each period. $\eta$ is the treatment growth error term. $cov(\epsilon_0, \epsilon_1) \neq 0$. In general we can also assume $cov(\epsilon_t, \eta) \neq 0$

The cost in the second period is 
$$
C = Z\gamma + \epsilon_c
$$

If the net benefit is negative, the participant drops out from the second period.
$$
M^D = I(Y^D_1 - Y_0 - C<0) = I( \xi^D < X(\beta_0-\beta_1)+ Z\gamma - \delta*D)
$$
where $\xi^D = \epsilon_1-\epsilon_0+\eta*D-\epsilon_c$

The expected natural growth is $X(\beta_1-\beta_0) + E_{\xi^D}(\epsilon_1-\epsilon_0)$.  For brievity, let $E(x)$ stand for$E_{\epsilon_1,\epsilon_0,\eta,\epsilon_c}(x)$ unless otherwise specified. The true average treatment effect is  $\delta+E(\eta)$



### I.1.3 Natural Attrition and Selective Attrition
Assume the treatment assignment is strictly exogenous
$$
A1: D \perp X, Z, \epsilon, \eta, \xi
$$

If the experiment is negilible to the user's service experience , whether the user is active in the second period is unlikely to be affected by the treatment status. Define attrition that is indepedent of the treatment assignment as natural attrition. However, the observed outcome is conditional indepedent of the natural attrition


$$
\begin{aligned}
A2.1: &L \perp D \\
A2.2: &L \perp Y | W
\end{aligned}
$$


In contrast, if the user is active but missing, he is suspected to attrite due to the treatment assignment. As modeled in the extended Roy model, missing status is correlated with $D$ even if the treatment status is strictly exogeneous. To see the **endogeneous attrition to exogeneous treatment assignment**, consider the probability of missing for the treatment and the control group.

$$
\begin{aligned}
P(M=1,D=1) & = \Pr(\xi^1 < X(\beta_0-\beta_1)+ Z\gamma - \delta)\\
P(M=1,D=0) & = \Pr(\xi^0 < X(\beta_0-\beta_1)+ Z\gamma)\\
\end{aligned}
$$

Even under the constant treatment effect model where $\xi^*=\epsilon_1-\epsilon_0-\epsilon_c$ , the sample attrition is still selective

$$
\begin{aligned}
P(M=1,D=1) & = \Pr(\xi^* < X(\beta_0-\beta_1)+ Z\gamma - \delta)\\
P(M=1,D=0) & = \Pr(\xi^* < X(\beta_0-\beta_1)+ Z\gamma)\\
\end{aligned}
$$

However, assume conditioning on observed and unobserved characteristics, missing status is independent of the treatment assignment.

$$
A3: M \perp D | X, Z, \epsilon_t, \eta, \epsilon_c
$$

## I.2 Characterize the Bias

The quantity of interest is the average treatment effect 
$$
\delta = E(Y^1_1-Y_0|D=1) - E(Y^0_1-Y_0|D=0)
$$

The observed average treatment effect is 

$$
\hat{\delta} = E(Y^1_1-Y_0|D=1,L=1,M=0) - E(Y^0_1-Y_0|D=0,L=1,M=0)
$$

Therefore, the bias can be decomposed as two parts:
$$
\begin{aligned}
Bias_{N} &= [E(Y^1_1-Y_0|D=1) - E(Y^0_1-Y_0|D=0)] - [E(Y^1_1-Y_0|D=1,L=1) - E(Y^0_1-Y_0|D=0,L=1)]\\
Bias_{S} &= [E(Y^1_1-Y_0|D=1,L=1) - E(Y^0_1-Y_0|D=0,L=1)] \\
         &- [E(Y^1_1-Y_0|D=1,L=1,M=0) - E(Y^0_1-Y_0|D=0,L=1,M=0)]\\
\hat{\delta} - \delta &= Bias_{N} + Bias_{S}
\end{aligned}
$$



### I.2.1 The Bias of Natural Attrition
Because of A2.1 and A2.2, $Bias_{N}$ can be corrected by matching techniques, espeically the inverse probability weighting method[@wooldridge_inverse_2007a]. Since the focus of this paper is the selective attrition, a detailed discussion will be spared. For the following discussion, suppress the conditions on natural attrition for brievity.


### I.2.2 The Bias of Selective Attrition

let $f^D(p)$ denote the probability density function for the propensity of missing. Denote marginal natural growth with respect to missing propensity as $MG^D = E(Y^D_1-Y_0|p(L=1,M=1|D))$

The average treatment effect can be expressed as [@heckman_structural_2005]:
$$
\delta = \int_{\Theta_p} MG^1(p) (1-f^1(p)) dp - \int_{\Theta_p} MG^0(p) (1-f^0(p)) dp
$$

Use the propensity score transformation

$$
\begin{aligned}
\delta(L=1)  &= E(Y^1_1-Y_0|D=1,P(M=0|D=1)) - E(Y_1-Y_0|D=0,P(M=0|D=0))\\
&= \int_{\Theta_p} MG^1(p) (1-f^1(p|M=0)) dp - \int_{\Theta_p} MG^0(p) (1-f^0(p|M=0)) dp
\end{aligned}
$$


Thus the bias of the selective attrition is

$$
\begin{aligned}
Bias_{S} &= \int_{\Theta_p} MG^1(p) [f^1(p)-f^1(p|M=0)] dp - \int_{\Theta_p} MG^0(p) [f^0(p)-f^0(p|M=0)] dp \\
&= \int_{\Theta_p} MTE(p) [f^1(p)-f^1(p|M=0)] dp + \int_{\Theta_p} MG^0(p) [MTME(p)-MTME(p|M=0)] dp \\
\end{aligned}
$$

where marginal treatment effect(MTE) is $MTE(p) = MG^1(p)-MG^0(p)$ and marginal treatment missing effect (MTME) is $MTME(p) = f^1(p)-f^0(p)$.

If M is exogenous to $D$ then $f^D(p)=f^D(p|M=0)$, $MTME(p)=MTME(p|M=1)$ the bias will be 0.


## I.3. Empirical Strategy

### I.3.1 MTE Correction
MTE correction essentially applies the MTE-IV approach twice [@heckman_understanding_2006] for the treatment and the control group.

For each group, first estimate the selective attrition propensity by a classification model. Assuming joint normality, the propensity can be esimtated by the probit model. Assuming no parametric form for $\xi^D$, the propensity can be estimated with machine learning techniques, such as soft margin SVM, random forest or gradient boosting.

Once the propensity specification is identified, the probability density function of $\widehat{f^D(p)}$ can be estimated based on the distribution of $X,Z|L=1$.

Esitmate the marginal growth by the following polynomial non-parametric regression (where K is selected by cross validation)

$$
Y^D_1-Y^D_0 =  X \beta^D + \theta^D_0 + \sum_{k=1}^{K} \theta^D_k [\widehat{p^D(M=1|X,D)}]^k + \epsilon
$$

and the marginal growth is 
$$
\widehat{MG(p)} = \sum_{k=1}^{K} \widehat{\theta^D_k} p^{k-1}
$$

Now estimate the average treatment effect through numerical integration of the following quantity where $\Theta^D_p$ is the empirical support 

$$
\int_{\Theta^1_p} \widehat{MG^1(p)} (1-\widehat{f^1(p|M=0)}) dp - \int_{\Theta^0_p} \widehat{MG^0(p)} (1-\widehat{f^0(p|M=0)}) dp
$$


### I.3.2 Heckman Correction
Heckman correction [@heckman_shadow_1974] can be applied to correct the selective attrition bias if additional assumptions are applied.
$$
A4: \eta = 0
$$

$$
A5: \epsilon_1, \epsilon_0, \epsilon_c \sim N(\mu, \Sigma)
$$

Under the setup, the observed DID is 
$$
\hat{ATE} = \delta + E(\epsilon_1 - \epsilon_0 | \epsilon_0 - \epsilon_1 + \epsilon_c \geq g(X,Z)+\delta) - E(\epsilon_1 - \epsilon_0 | \epsilon_0 - \epsilon_1 + \epsilon_c \geq g(X,Z))
$$

To correct the bias, the conditional expectation of $\epsilon_1 - \epsilon_0$ needs to be estimated. Heckman correct uses inverse miller ratio to proxy the selection on the unobservables. Estimate a separate probit model of selective attrition status for the treatment and control group. Compute the inverse miller ratio $\lambda^D$ for the observed sample. The average treatment effect is the estimated $\hat{\gamma}$ from the following regresion

$$
Y_t^D =  X \beta_0+  tX\beta_1 + \gamma Dt + \theta_D D+\theta_t t +\theta_M\lambda^D+\theta_{M,D}\lambda^D D + \epsilon
$$

## Appendix II: Proof of the upward bias of the observed ATE
## II.1 Average Treatment Effect and Latent Mastery Model

Let student's latent knowledge mastery to be $\theta$, which is drawn from distribution $F(\theta)$. Denote the treatment status as $D$, a binary variable where the treatment group takes value 1. The true aim is to evaluate the average treatment effect on the latent understanding
$$
\begin{aligned}
\delta_{\theta} &= E_{\theta}(\theta|D=1) - E_{\theta}(\theta|D=0)
\end{aligned}
$$

Since the mastery is not observable, it has to be inferred by an observed response $Y$. The latent mastery and the observed response is linked by a function $f(Y|\theta)$.Assume the link function depends on both latent knowledge matery and effort ($\gamma$). Denote the link function as
$$\Pr(Y=1|\theta,\gamma) = p(\theta,\gamma)$$

The following analysis makes a key assumption that student's effort level is binary. Student can either exert full effor or they exert no effort at all. A possible critique of the assumption is that student can try before giving up, however such hypothesis is not supported by the empirical finding that it takes student very short time to decide if they are going to attempt. Furthermore, assumes that the observed response is incorrect for sure if the student gives up. Such assumption rules out item type such as multiple choice where an uneducated guess takes almost no effort and has a non-trivial probability of being right.

To describe the data generating process explicitly, this paper borrows the structure from Roy model where a two stage decision process determines the output.

(1) Knowing his own $\theta$, the student assesses the success probability $p(\theta)$. The reward of success is $\beta$ and the expected payoff is $\beta p(\theta)$.  

(2) The students' mental cost is drawn i.i.d from a distribution $\epsilon \sim G$. The student only exerts effort if the expected benfit is larger than the cost.
$$\Gamma=I(\beta p(\theta)>\epsilon)$$

(3) If the student exerts the effort($\Gamma=1$), the response is drawn from $Y \sim B(1, p(\theta))$. Otherwise $Y=0$


## II.3 The Bias of the Observed Average Treatment Effect

Under the assumption of joint input, the average treatment effect is the difference in performance between groups with full effort. 
$$
\delta_{Y^*} = E_{\theta}(p(\theta)|D=1) - E_{\theta}(p(\theta)|D=0)
$$

However, the observed response $Y$ is a joint event of the true response $Y^*$ and $\Gamma$. Let $Y = I(Y^*=1, \Gamma=1)$. Let $Y^*$ follow bernoulli distribution with mean of $p(\theta)$, let $\epsilon \sim G$. Assume $\theta$, $Y^*$ and $\epsilon$ are indpendent variables, 

$$
\begin{aligned}
E(Y) &= E_{\theta}[E_{Y^*,\epsilon|\theta}[I(Y^*=1, \Gamma=1|\theta)]]\\
     &= E_{\theta}[E_{Y^*}[I(Y^*=1|\theta)]E_{\epsilon}[I(\Gamma=1|\theta)]]\\
     &= E_{\theta}[p(\theta)G(\beta p(\theta))]\\
\end{aligned}
$$

The observed average treatment effect is the difference of expectation of $E(Y)$ taken over the distribution of $\theta$ for the treatment group and control group  
$$
\delta_{Y} = E_{\theta}[p(\theta)G(\beta,\theta)|D=1] - E_{\theta}[p(\theta)G(\beta,\theta)|D=0]
$$

where $\Pr(\Gamma=1)=G(\beta,\theta)$ is the probability of observing a valid response. 


For simplicity, assume that $F_1(\theta)$ and $F_0(\theta)$ has valid probability density function $f_1(\theta)$ and $f_0(\theta)$ and the support is the same for both groups. Under these further assumptions, the marginal bias conditional $\theta$ is
$$
\frac{\partial (\delta_{Y} - \delta_{Y^*})}{\partial \theta} = p(\theta)[G(\beta,\theta)-1][f_1(\theta) - f_0(\theta)]
$$

Previous assumptions implies$\frac{\partial p(\theta)}{\partial \theta}>0$,  $\frac{\partial G(\beta,\theta)}{\partial \theta}>0$. With additional assumption that intervetion does not decrease $\theta$, it is easy to see that $\delta_{Y} > \delta_{Y^*}$. It means that the observed average treatment effect is the upward bound of the true average treatment effect. 

# Appendix III
## III.1 

