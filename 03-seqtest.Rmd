# Continuous Improvement with Multi-arm Bandit Algorithm {#seqtest}

```{r, echo=FALSE,message=FALSE,warning=FALSE}
library(dplyr)
library(ggplot2)
proj_dir = getwd()

output_file_path = paste0(proj_dir,'/_data/03/production_data.RData')
load(output_file_path)
```

## Introduction

Randomized control trials (RCT) has long been held as the golden standard of program evaluation[@lalonde1986evaluating;@imbens2009recent]. Since the call for "Scientifically Based Research" as a part of the No Child Left Behind initiative, RCT has gained dominance among researchers and practitioners in education evaluation, symbolized by the establishment of What Works Clearninghouse[@slavin2002evidence] that only consider experiment or quasi-experiment as acceptable evidences. However, there are two major disagreements between the classical randomized control trial and the iteration cycle of continuous product improvement. Fixed balance sample and t-test for hypothesis testing:

(1) Sequential Arrival of observations allow for imbalance sample and early termination. The primary benefit of imbalance sample and early termination is the reduction in opportunity cost. Because the experiments are conducted in live production, it is desirable to limit the exposure of the inferior option. The sequential test technique is supported by the ethnical argument for early termination in medical experiment [@mukhopadhyay_sequential_2009] and the business argument for imbalance sample in online advertising[@scott_multi_2015].


(2) The cost of inaction is usually higher than the cost of action. In another words, type II error is more costly than type I error under the Null Hypothesis of Null effect. Because continuous improvement usually aim at incremental changes, missing a true improvement (Type II error) is worse than preferring one of two equivalent alternatives (Type I error). There is a famous anecdote that Marrisa Mayer "commissioned" an A/B test to pick the optimal hue of blue for the "Google" button from more than 40 shades of blue. 


Randomized control trial(RCT) is no stranger to education literature[@schweinhart1993significant;@nye2000effects;@puma2010head;@angrist2013explaining]. Yet very few publication has used experiments as a tool for ex ante improvement discovery, rather than expost policy evaluation. This is in sharp contrast with the "A/B Test" everything mentality in the online service sectors where experiment is a primary tool of knowledge discovery[@kohavi2007practical]. Multi-armed bandit algorithm has been widely used for such purposes in online advertisement[@scott_modern_2010] and content recommendation[@li2010contextual]. Thompson sampling[@thompson1933likelihood] is the oldest heuristic solution to the Multi-armed bandit problem,  among various algorithms proposed[@white_bandit_2012]. It is a bayesian algorithm that balances the tradeoff of exploration and exploitation. It has strong heuristics, good empirical performance[@chapelle_empirical_2011] and a regret bound for finite time sequences[@kaufmann_thompson_2012]. The bound guarantees that the optimal arm is chosen exponentially more times than inferior alternatives as the experiment continues. 

There is a debate over the merit of the multi-arm bandit(MAB) algorithm as a randomized control trial method over classical t-test on the Internet[@Hanov2012twenty;@Chopra2012why;@Stucchio2015dont]. This debate fundmentally mistake the nature of the MAB algorithm: It is an optimizion technique rather than a statistical hypothesis test. The most important difference is that MAB algorithm per se does not have a stopping condition or an evidence criteria, compared to sequential likelihood test or t-test. The choice of the stopping condition is rather haphazard and attracts little attention in the academic field. The first contribution of this chapter is to examine the properties of Thompson sampling coupled with potential value remaining stopping condition as a stastical test for binary random variables. 

In addition, this chapter replicates the analysis on the dataset used in chapter two to demonstrate the practical benefit of MAB algorithm. It significantly shortens the experiment duration and increased pedagogical efficacy of the experiment.


## Thompson Sampling with Potential Value Remaining Stop Condition

### Multi-armed Bandit Algorithm
There are $n$ experimental options (arm), each with a unknown return parameter $\mu_i$. The researcher designs a sequence of experiments that samples one option at a time. The goal is to design an allocation policy that achieves the highest return. Denote the best arm has an expected return $\mu^*$. Regret for each period $t$ is defined as the difference between $\mu^*$ and the expected return of the chosen arm $\mu_t$. The dual problem of maximizing the return is minimizing the regret during the tenure of the policy.

### Optimal Policy and Thompson Sampling
The multi-armed bandit algorithm is notoriously hard to solve. Although the problem was raised during World War II, the first theoretical optimal solution did not surface until Gittins[-@gittins_bandit_1979] proposed the index policy. Although it provides an optimal solution for certain special cases, the exact policy is hard to compute. The family of Upper Confidence Bound(UCB) algorithms [@lai_adaptive_1987;@lai_asymptotically_1985] describes a series of algorithms based on the mean return of each arm. The UCB algorithm ensures that the optimal arm is played exponentially more often than any of the suboptimal arm. The UCB policy family is hard to extend to dependent return between arms, such as the contextual test design.

There are an array of heuristic driven policies in addition to UCB algorithm. The key heuristic is the trade-off between exploration and exploitation. Exploitation means the policy plays the winner more often, while exploration emphasizes on giving the loser a second chance because of the uncertainty around the parameter estimation, especially at the begining of the experiment where the sample size is small. Random probability matching(RPM), also known as Thompson sampling, is another heuristic strategy, although Kaufmann et al[-@kaufmann_thompson_2012] provide a UCB like bound on the regret. Thompson sampling follows a stratified sampling strategy that weighs the different options proportionally to the estimated return of the options, instead of equal weights as in the $\epsilon$-decreasing strategy.  At time $t$, RPM allocates the sample according to a weight matrix ${w_{1t},\dots,w_{nt}}$ where $y_t$ is the observed return sequence to time $t$ and 

$$
w_{at} = Pr(\mu_a = max\{\mu_1, \dots, \mu_n\}|y_t)
$$

The simple sampling scheme combines the two heuristics elegantly. The stratified sampling strategy allows for enough exploration at the begining of the experiment when the posterior distributions of the return are similar. As more evidences are incorporated into the posterior distribution, superior alternatives have higher probabilities to be sampled so that the heuristic leans toward exploitation if there are strong evidence that one or more options are indeed superior.

There are also engineering advantages for choosing Thompson sampling. Because the reward is not updated instantaneously but rather in batches, a policy with random allocation rather than deterministic allocation can reduce the regret due to over-exploiting the inferior arm in batch assignments. In addition, Thompson sampling can easily accommodate prior information, heterogeneous reward distribution, and contextual test design by modifying the prior and the posterior update procedure.


### Potential Value Remaining Stop Condition

Thompson sampling is not a statistical test per se because the multi-armed bandit algorithm is designed to minimize regret over a time horizo,n rather than  reach a decision on whether to reject the null hypothesis. To analyze the properties of Thompson sampling as a statistical test, it is necessary to provide a stop condition and the null hypothesis. The stop condition analyzed here is the "potential value remaining" (PVR) stop condition used by Google Analytics. It is a practical choice of convenience.

"Potential value remaining"(PVR) measures the expected regret of the chosen arm. Given the posterior distribution of return as  $\theta_i|Y$, draw $M$ samples from the posterior distribution. For each sample $m$, denote the max return as $\theta^*(m)$. The same sample also yields the returns of the chosen optimal arm $i^*$ for each sample, denoting as $\theta_{i^*}(m)$. The value remaining distribution is inferred from the following sample

$$
VR(m) = \frac{\theta^*(m)-\theta_{i^*}(m)}{\theta_{i^*}(m)} = \frac{\theta^*(m)}{\theta_{i^*}(m)} - 1 
$$

The stop condition has two parameters ($\epsilon$,$\delta$), the ratio threshold $\epsilon$ and the cumulative probability threshold $\delta$. Mathematically, the experiments stop when the following inequality holds

$$
\Pr(VR > 1+\epsilon) < \delta
$$

the potential value remaining (PVR) stop condition can be interpreted as p-value: Given that the posterior belief is true, there is less than $\delta$ probability to observe the alternative arm outperforms the chosen arm by $\epsilon$ percent . 


### The Imbalance Sample Assignment

Various results in the following discussion hinges on the imbalance sample assignment of the thompson sampling. 

For binary treatment, the rule of thumb is to do a 50-50 sample split a prior. The rationale behind this conventional wisdom is that it minimizes the sample variance and minimizes the sample size. For more treatment levels, imbalance sample design is sometimes desired so as to concentrate on the region where precise estimation is required[@list2011so]. Even in those circumstances, the imbalance is set a prior and is not subject to change as the data are collected.

Thompson sampling's bayesian stratified sampling defies this rule of thumb. It gives more sample to arms with higher estimated mean return. The original intention is to generate higher return for the sampling policy. As an unintented consequence, it also minimizes sample variance under alternative hypothesis for binary outcome: higher mean return is accompanied with higher variance. However, combined with potential value remaining stop condition, such imblanace variance shrinkage results in higher type I error under the null hypothesis.



## Power and Significance Level

### Setup

Throughout the chapter, the null hypothesis of interest is 

$H_0: \quad p_0 \geq p_1$

where the sample is the draws from a Bernoulli distributions with probility $p_i$. The specific hypothesis allows for a good representation of the test properties with a small number of simulations. This paper considers three types of base rate ($p_0$)

Type | Values
--- | ---
Low | 0.05,0.09
medium| 0.15, 0.25
high| 0.4, 0.5

The effect size ($\triangle$) is measured by percentage (relative) rather than percentage points(absolute). The effect sizes are 10%, 20%, 50%, 100%, representing small to huge effect size. For a fair comparison, the maximum sample size($T$) for the sequential test is the same as the sample size required for a 5% significance and 95% power of one-sided Welch t-test, which can be calculated precisely for each base rate and effect size combinations. For each combinations, 100 datasets are simulated.


### Thompson Sampling Test Procedure

The Thompson sampling with value remaining stop condition (hence referred to as bandit test) rejects the Null hypothesis if the stop condition is met within sample limitation. For power analysis, only count the cases where the true superior arm is identified.

The bandit test requires two tuning parameters, in addition to the $\epsilon$ and $\delta$ required by the potential value remaining condition. The first is the prior distribution and the second is the inspection interval, i.e. how often the stop condition is checked. The prior distribution for each arm is Beta(2,2), standard uninformative beta prior. The inspection interval matters because it controls the "repeated significance test" problem, otherwise known as early peeking. The inspection interval is set at such value that there are 25 inspections before the sample runs out. The choice is made to facilitate visualization, rather than to optimize the test performance.

### Power 
Power is usually not the primary concern in econometric studies. Maybe it is to avoid excessive false positives. Maybe it is because power calculation requires the knowledge of unknown parameter of effectize. That said, as argued in the introduction, for continuous improvement, the opportunity cost of type I error outweighs that of the type II error. Therefore, power, rather than significance level, is the key property to investigate.

#### Sequential Probablity Ratio Test Procedure

Sequential analysis is not a new topic in statistics. Sequential Probability Ratio Test(SPRT) has well studied frequentist statistical test properties. SPRT builds test procedure to allow for early termination while provides an controllable significance level under the null hypothesis. However, SPRT requires model parameters that are difficult to calibrate and permits a limited scope of the null hypothesis to be tested. The sequential test procedure used here is the modified sequential probability ratio test[@siegmund_sequential_2013]. Compared to the classical sequential probability ratio test, it adds another t-test when the sample limit is reached to give extra power.


Because a comparison of two means is difficult to carry out in sequential test, the modified SPRT defines a new random variable, the "adverse event", so that it turns into a sequential test of sample mean . Let $X_{i,n}$ be the drawing from group i at time n. Define $Y_n=0$ if $X_{0,n}=1,X_{1,n}=0$ and $Y_n=1$ if $X_{0,n}=0,X_{1,n}=1$. When $X_{0,n}=X_{1,n}$, skip the update. Therefore, the equivalent null hypothesis for equal sample mean is

$$
\mu_Y = 0.5
$$


Define Wald statistic as  
$W(n) = nH(\frac{\sum_{n=1}^NY_n}{N})$ where $H(x) = xlogx+(1-x)log(1-x)+log2$.

Define the rejected time as 

$$
T = \inf\{n:n \geq m_0, W(t)\geq \alpha\}
$$
where $m_0$ is the minimum time of rejection.

The sampling stops at $\min(T,m)$ and reject the null hypothesis if the sample stops before $m$. However, such configuration has low power given significance level. The sequential test can be modified to have a second test at time $m$ and a higher value of $d$ to compensate the false positive in the second test. Therefore, the modified rejection is $T \leq m$ or $T>m, mH(\frac{\sum_{j}Y_j}{m}) \geq d$.

The modified SPRT needs to be fed parameter of $(\alpha, d, m, m_0)$. The calibration of the parameters is detailed in Appendix III.1.

#### Result
Figure 4.1 compares the power of three tests. The columns are effect size ratios and the rows are base rate. In each sub-graph, the x-axis represents the progression of time: The total sample is cut into 25 intervals for easy visualization. Y axis is the probability of rejecting null hypothesis had the sample runs out at that time.

```{r, echo=FALSE,message=FALSE,warning=FALSE, fig.cap = "Power of T-test, Sequential Likelihood Ratio Test and Bandit Test on Simulation Data", fig.align='center'}
power_result = read.table(paste0(proj_dir,'/_data/03/power_result.txt'), sep=',', col.names = c('p','ratio','method','i','power'))

# t test is used as guideline
power_result$power[power_result$method==' ttest'] = 0.95
power_result$method[power_result$method==' ttest'] = '95%'

power_result = power_result %>% filter(p %in% c(0.03,0.07,0.15,0.25,0.4,0.5))

qplot(data=power_result,x=i,y=power,col=method,geom='line', facets = p~ratio)

```


Overall, the bandit has superior performance on power. Compared to the t-test, the bandit test achieves similar power in a large sample but the concavity means it attains high power with relatively small sample size. Compared to the sequential test, the bandit test has larger power for a given sample size, except for cases where effect size is large and the base rate is low (upper right corner). Even in those regions, bandit test is almost as good as the sequential test. The result is more remarkable considering the sequential test is optimally calibrated to each simulation environment while the bandit test relies on a global configuration. In a practical scenario where the true parameters are unknown, the bandit test can perform even better.


### Significance Level 

Characterizing the type I error rate for one sided hypothesis is quite difficult under frequentist framework. Since the bandit test has a high power when the two options are indeed different, it is safe to say its type I error rate under Null hypothesis $\mu_0 > \mu_1$ is quite low. The true concern is under the null hypothesis of equal mean. $\mu_0 = \mu_1$.It is not clear how to specify a sample size when the true effect size is zero so the simulation runs 300000 rounds and calculate the statistics every 1000 rounds. The type I error for the bandit test is calculated as the percentage of simulations that stops for a given sample size. 

The significance level of the t-test is independent of the sample size and base rate in theory. The significance level of the bandit test does not enjoy these constant properties:

(1) As the sample size grows, the bandit test is more prone to type I error. 

(2) As the base rate increases, the bandit test is more prone to type I error. 

```{r,message=FALSE,warning=FALSE,echo=FALSE,fig.cap = "Significance leve of Bandit Test on Simulation Data", fig.align='center'}
sig_Thompson = read.table(paste0(proj_dir, '/_data/03/sig_thompson.txt'), sep=',', col.names=c('p','k','sig'))
sig_Thompson = sig_Thompson %>% filter(p %in% c(0.03,0.07,0.15,0.25,0.4,0.5))

sig_Thompson$p = factor(sig_Thompson$p)

qplot(data=sig_Thompson, x=k, y=sig, col=p, geom='line', linetype=p)
```


However, the posterior probability of either arm being the optimal arm is close to 50%. Despite the correct bayesian inference, the stop condition nevertheless arrives at the wrong decision. The contradition can be explained by the imbalance sample allocation due to the tendency favoring exploitation. If the sample is equally distributed among the two arms, the simulation shows that type I error decreases slowly as the sample size grow, albeit still at a high level (on average 40% for the exact sample size of 300,000). However, the favored arm receives more sample than the other, shrinks its beta posterior faster than the alternative. The asymmetric shrink thickens the right tail of the potential value remaining metric, leading to an increase of type I error rate. The higher base rate, the higher the variance of the posterior distribution, and the larger the imbalance sample effect is. 


## Regret
Maximizing the benefit during the experiment is an important concern for the industry, especially if experiments are continually runned on the service. For medical experiment, it means lower mortality or mobidity rate for the inferior group. For online advertising, it means higher return on the investment. For education, it means higher learning gain for the users. In the terminology of the multi-armed bandit problem, define the regret as the expected loss of payoff had the experiments always chosen the optimal arm. Maximizing the benefit is the dual problem of minimizing regret.

### Simulated Data

The simulation parameter triple is $(p, \triangle, N)$, where $p$ is the base rate, $\triangle$ is the effect size ratio and $N$ is the sample size calibrated for 0.05 significance 0.95 power welch t-test .

The expected regret of the welch t-test is fixed at 
$$
r_{welch} = N*p*\triangle
$$

The expected regret a sequential test ended in period t is 
$$
r_{x}(t) = N_0(t)*p*\triangle
$$
where $N_0$ is the number of participants assigned to the inferior option  

Therefore, the expected regret for sequential test ended before $T$ is 

$$
E(r_x|t\leq T) = \int_{m_0}^{T} r_{x}(t) f(t|t\leq T) dt
$$

Both the conditional regret function $r_{x}(t)$ and the conditional probability density function $f(t|t\leq T)$ is estimated by average sample sample in bins. The bin size is $\frac{N}{24}$

Since the relative regret saving, rather than the absolute regret saving, is the key quantity of interest, define the regret ratio as 

$$
R_x(T) = \frac{E(r_x|t\leq T)}{r_{welch}}
$$


Figure 4.3 shows the relative regret of the sequential test and the bandit test compared to fixed sample student t-test. The bandit regret is about 10% compared while the sequential test regret is about 30%. More importantly, the sequential test regret percentage grows as the sample size increases, while the bandit regret percentage flats out. The slow growth of regret of the bandit test may be attributed to the stratified sampling strategy. In the later stage, the sample is more frequently assigned to the optimal arm, in contrast to sequential probability ratio test's equal assignment due to pairing.


```{r,message=FALSE,warning=FALSE,echo=FALSE, fig.cap = "Relative Regret Saving compared to t-test", fig.align='center'}
# calculate the cumulative regrets
regret_result = read.table(paste0(proj_dir,'/_data/03/regret_result.txt'), sep=',', col.names = c('p','ratio','method','i','avg_reg','pdf'))

regret_result = regret_result  %>% filter(p %in% c(0.03,0.07,0.15,0.25,0.4,0.5))

for (k in seq(2,25)){
    regret_stat = regret_result %>% filter(i<=k) %>%
                    group_by(p, ratio, method) %>%
                    mutate(cond_pdf = pdf/sum(pdf)) %>%
                    mutate(cum_reg = cumsum(avg_reg*cond_pdf)) %>% 
                    select(p, ratio, method, i, cum_reg)
    
    
    ttest_stat = regret_stat %>% filter(method == 'ttest') %>% rename(benchmark_reg=cum_reg) %>% select(p, ratio, i, benchmark_reg) 
    tmp = merge(regret_stat, ttest_stat, by=c('p','ratio','i'))  %>%
        transform(regret_ratio = cum_reg/benchmark_reg) %>%
        filter(method.x!='ttest') %>% select(-method.y) %>%
        filter(i==k)
    if (k==2){
        regret_res = tmp
    }else{
        regret_res = rbind(regret_res, tmp)
    }
}


qplot(data=regret_res,x=i,y=regret_ratio,col=method.x,geom='line', facets = p~ratio) + xlab('Time') + ylab('Regret Ratio')

```

### Simulated Experiments

#### Setup
The experiments described in Chapter Three is conducted by the fixed sample design. However, since the arrive time is in the dataset, it is possible to reconstruct an experiment simulation where observations are sampled sequentially from the sub-sample of each group, keeping the total sample size constant. In the event that observations are depleted from one group, a random draw from the whole sub-sample is used to supply the queue.

The simulated experiments are performed on three different datasets. The full datasets includes all learners who failed the original questions. The filtered dataset with automatic measurement error classification excludes learners who submit a blank response to the question. The filtered dataset with manual measurement error classification exclude learners who submit a response that is identified as "give-up"(details see appendix II.2 in Chapter 3). The measurment error decreases from the full dataset to the automatic filter dataset to the manual filter dataset.

Before looking at the other properties of the bandit test, it is helpful to understand the power of the bandit test. The bandit stopping rate is the probability that the potential value remaining condition is reached, no matter what group the algorithm chooses. The T-test significance rate calcalutes the percentage of pvalue, the null hypothesis of null effect, that is smaller than 5%. Overall, the bandit algorithm makes much more aggressive decision than the accompanying t-test, or the standard DID regression on the full sample. The decision result varies significantly depending on the degree of measurement. It shall be noted that the algorithm is almost always choose the treatment group over the control group.


```{r, message=FALSE,warning=FALSE,echo=FALSE}

# replicate the regression results
y0data = workdata %>% filter(eid=='Q_10201056649366') %>% mutate(t=0)
y1pdata = workdata %>% filter(eid=='Q_10201056666357') %>% mutate(t=1)
y1sdata = workdata %>% filter(eid=='Q_10200351208705') %>% mutate(t=1)

ydata_p = rbind(y0data,y1pdata)
ydata_p = ydata_p %>% transform(d1=as.numeric(gid==2),d2=as.numeric(gid==4),d=as.numeric(gid!=0))

ydata_s = rbind(y0data,y1sdata)
ydata_s = ydata_s %>% transform(d1=as.numeric(gid==2),d2=as.numeric(gid==4),d=as.numeric(gid!=0))

ydata_p = ydata_p %>% transform(dt=d*t,ddt =d2*t, d2t=d2*t,d1t=d1*t)
ydata_s = ydata_s %>% transform(dt=d*t,ddt =d2*t, d2t=d2*t,d1t=d1*t)

mod_7 = lm(data=ydata_p%>% filter(is_placebo==0),y~t+d1t+d2t)
mod_8 = lm(data=ydata_s%>% filter(is_placebo==0),y~t+d1t+d2t)

mod_9 = lm(data=ydata_p%>% filter(is_placebo==0&is_retain==1),y~t+d1t+d2t)
mod_10 = lm(data=ydata_s%>% filter(is_placebo==0&is_retain==1),y~t+d1t+d2t)


y0data = bkp_data %>% filter(eid=='Q_10201056649366') %>% mutate(t=0)
y1pdata = bkp_data %>% filter(eid=='Q_10201056666357') %>% mutate(t=1)
y1sdata = bkp_data %>% filter(eid=='Q_10200351208705') %>% mutate(t=1)

ydata_p = rbind(y0data,y1pdata)
ydata_p = ydata_p %>% transform(d1=as.numeric(gid==2),d2=as.numeric(gid==4),d=as.numeric(gid!=0))

ydata_s = rbind(y0data,y1sdata)
ydata_s = ydata_s %>% transform(d1=as.numeric(gid==2),d2=as.numeric(gid==4),d=as.numeric(gid!=0))

ydata_p = ydata_p %>% transform(dt=d*t,ddt =d2*t, d2t=d2*t,d1t=d1*t)
ydata_s = ydata_s %>% transform(dt=d*t,ddt =d2*t, d2t=d2*t,d1t=d1*t)

mod_11 = lm(data=ydata_p%>% filter(is_placebo==0&is_retain==1),y~t+d1t+d2t)
mod_12 = lm(data=ydata_s%>% filter(is_placebo==0&is_retain==1),y~t+d1t+d2t)

wk_power_data = stat_wk %>% filter(type=='power') %>% mutate(delta=round((bpower-tpower)*100,1), opt_pct = round((bpower1/bpower)*100,1)) %>% select(q,g,delta, tpower, bpower, opt_pct)
wk_retain_power_data = stat_wk_retain %>% filter(type=='power') %>% mutate(delta=round((bpower-tpower)*100,1)) %>% select(q,g,delta,tpower, bpower)
bkp_retain_power_data = stat_bkp_retain %>% filter(type=='power') %>% mutate(delta=round((bpower-tpower)*100,1)) %>% select(q,g,delta,tpower, bpower)

```


Assessment | treamtnet   | Bandit Test Stopping Rate            | T-Test Significant Rate | DID Coefficient(p_value)|
---        | ---         | ---                          | ---                                         | ---                                         |
Routine    | Vocabulary  | `r round(bkp_retain_power_data$bpower[1]*100,2)`%  | `r round(bkp_retain_power_data$tpower[1]*100,2)`%          | `r round(coef(summary(mod_7))[3, 1],2)`(`r round(coef(summary(mod_9))[3, 4],2)`)        |
Routine    | Video       | `r round(bkp_retain_power_data$bpower[2]*100,2)`%  | `r round(bkp_retain_power_data$tpower[2]*100,2)`%          | `r round(coef(summary(mod_7))[4, 1],2)`(`r round(coef(summary(mod_9))[4, 4],2)`)         |
Transfer   | Vocabulary  | `r round(bkp_retain_power_data$bpower[3]*100,2)`%  | `r round(bkp_retain_power_data$tpower[3]*100,2)`%          | `r round(coef(summary(mod_8))[3, 1],2)`(`r round(coef(summary(mod_10))[3, 4],2)`)        |
Transfer   | Video       | `r round(bkp_retain_power_data$bpower[4]*100,2)`%  | `r round(bkp_retain_power_data$tpower[4]*100,2)`%          | `r round(coef(summary(mod_8))[4, 1],2)`(`r round(coef(summary(mod_10))[4, 4],2)`)         |

Table: Full Sample


Assessment | treamtnet   | Bandit Test Stopping Rate            | T-Test Significant Rate | DID Coefficient(p_value)|
---        | ---         | ---                          | ---                                         | ---                                         |
Routine    | Vocabulary  | `r round(wk_retain_power_data$bpower[1]*100,2)`%  | `r round(wk_retain_power_data$tpower[1]*100,2)`%          | `r round(coef(summary(mod_11))[3, 1],2)`(`r round(coef(summary(mod_11))[3, 4],2)`)        |
Routine    | Video       | `r round(wk_retain_power_data$bpower[2]*100,2)`%  | `r round(wk_retain_power_data$tpower[2]*100,2)`%          | `r round(coef(summary(mod_11))[4, 1],2)`(`r round(coef(summary(mod_11))[4, 4],2)`)         |
Transfer   | Vocabulary  | `r round(wk_retain_power_data$bpower[3]*100,2)`%  | `r round(wk_retain_power_data$tpower[3]*100,2)`%          | `r round(coef(summary(mod_12))[3, 1],2)`(`r round(coef(summary(mod_12))[3, 4],2)`)        |
Transfer   | Video       | `r round(wk_retain_power_data$bpower[4]*100,2)`%  | `r round(wk_retain_power_data$tpower[4]*100,2)`%          | `r round(coef(summary(mod_12))[4, 1],2)`(`r round(coef(summary(mod_12))[4, 4],2)`)         |

Table: Automatic Measurement Error Filter

Assessment | treamtnet   | Bandit Test Stopping Rate            | T-Test Significant Rate | DID Coefficient(p_value)|
---        | ---         | ---                          | ---                                         | ---                                         |
Routine    | Vocabulary  | `r round(wk_power_data$bpower[1]*100,2)`%  | `r round(wk_power_data$tpower[1]*100,2)`%          | `r round(coef(summary(mod_9))[3, 1],2)`(`r round(coef(summary(mod_9))[3, 4],2)`)        |
Routine    | Video       | `r round(wk_power_data$bpower[2]*100,2)`%  | `r round(wk_power_data$tpower[2]*100,2)`%          | `r round(coef(summary(mod_9))[4, 1],2)`(`r round(coef(summary(mod_9))[4, 4],2)`)         |
Transfer   | Vocabulary  | `r round(wk_power_data$bpower[3]*100,2)`%  | `r round(wk_power_data$tpower[3]*100,2)`%          | `r round(coef(summary(mod_10))[3, 1],2)`(`r round(coef(summary(mod_10))[3, 4],2)`)        |
Transfer   | Video       | `r round(wk_power_data$bpower[4]*100,2)`%  | `r round(wk_power_data$tpower[4]*100,2)`%          | `r round(coef(summary(mod_10))[4, 1],2)`(`r round(coef(summary(mod_10))[4, 4],2)`)         |

Table: Manual Measurement Error Filter

#### Results
The regret is hard to define in the simulated experiment since the true superior arm is unknown. However, the resulting success rate is a proxy for learning gain since the data set conditions on making initial mistakes. 




```{r, message=FALSE,warning=FALSE,echo=FALSE}

wk_save_data = stat_wk %>% filter(type=='ret') %>% mutate(delta=round((simexp/origin-1)*100,1)) %>% select(q,g,delta)
bkp_retain_save_data = stat_bkp_retain %>% filter(type=='ret') %>% mutate(delta=round((simexp/origin-1)*100,1)) %>% select(q,g,delta)
wk_retain_save_data = stat_wk_retain %>% filter(type=='ret') %>% mutate(delta=round((simexp/origin-1)*100,1)) %>% select(q,g,delta)

```


Assessment | treamtnet   | Full Data            | Filtered Data (Automatic Measurement Error Classification) | Filtered Data (Manual Measurement Error Classification) |
---        | ---         | ---                          | ---                                         | ---                                         |
Routine    | Vocabulary  | `r wk_save_data$delta[1]`%  | `r bkp_retain_save_data$delta[1]`%          | `r wk_retain_save_data$delta[1]`%         |
Routine    | Video       | `r wk_save_data$delta[2]`%  | `r bkp_retain_save_data$delta[2]`%          | `r wk_retain_save_data$delta[2]`%         |
Transfer   | Vocabulary  | `r wk_save_data$delta[3]`%  | `r bkp_retain_save_data$delta[3]`%          | `r wk_retain_save_data$delta[3]`%         |
Transfer   | Video       | `r wk_save_data$delta[4]`%  | `r bkp_retain_save_data$delta[4]`%          | `r wk_retain_save_data$delta[4]`%         |




## Sample Size

In practice, traffic volume is usually the largest opportunity cost to conduct experiments for continuous improvement in education. Because each learner usually practices a limited number of exercies per day, When the number of simultaneous experiments goes up, even a large user base can be easily depleted. Espeically when the effect size is expected to be small, the sample size required to detect the difference by t-test can be ineconomically large[@lewis2015unfavorable]. The previous analysis on power has already primed the result in this section. Because of the imbalance sample design, Thompson sampling aglorithm with potential value remaining algorithm can save substantial amount of samples. 

### Simulated Data
Because t-test has a well defined sample size, it can serve as a benchmark for the savings in sample size. For sequential test and bandit test, denote the sample size as the sample required when the experiments ends. The sample size is thus a function of the inspection interval. The inspection interval is 4% of the t-test sample size, allowing for 25 opportunities to call an early termination of the experiments. 

Figure 4.4 shows a significant saving in sample size in reaching the conclusion. Especially when the base rate is low and the effect size is large. When the effect size is only 10%, sequential test on average consumes one third of the sample required by t-test, but the bandit is a half of or a third of the sample size required by sequential test.


```{r,message=FALSE,warning=FALSE,echo=FALSE, fig.cap = "Relative Sample Saving compared to t-test", fig.align='center'}
# calculate the cumulative regrets
sample_result = read.table(paste0(proj_dir,'/_data/03/sample_result.txt'), sep=',', col.names = c('p','ratio','method','pct'))

sample_result = sample_result  %>% filter(p %in% c(0.03,0.07,0.15,0.25,0.4,0.5))



qplot(data=sample_result,x=pct,col=method, geom='density', facets = p~ratio) + xlab('Sample Saving Pct')

```

### Simulated Field Experiments



```{r, message=FALSE,warning=FALSE,echo=FALSE}

wk_save_ratio = stat_wk %>% filter(type=='runs') %>% mutate(ratio=100-round(simexp/origin*100,0)) %>% select(q,g,ratio)
wk_retain_save_ratio = stat_wk_retain %>% filter(type=='runs') %>% mutate(ratio=100-round(simexp/origin*100,0)) %>% select(q,g,ratio)
bkp_retain_save_ratio = stat_bkp_retain %>% filter(type=='runs') %>% mutate(ratio=100-round(simexp/origin*100,0)) %>% select(q,g,ratio)

```

Assessment | treamtnet   | Full Data            | Filtered Data (Automatic Measurement Error Classification) | Filtered Data (Manual Measurement Error Classification) |
---        | ---         | ---                          | ---                                         | ---                                         |
Routine    | Vocabulary  | `r wk_save_ratio$ratio[1]`%  | `r bkp_retain_save_ratio$ratio[1]`%          | `r wk_retain_save_ratio$ratio[1]`%         |
Routine    | Video       | `r wk_save_ratio$ratio[2]`%  | `r bkp_retain_save_ratio$ratio[2]`%          | `r wk_retain_save_ratio$ratio[2]`%         |
Transfer   | Vocabulary  | `r wk_save_ratio$ratio[3]`%  | `r bkp_retain_save_ratio$ratio[3]`%          | `r wk_retain_save_ratio$ratio[3]`%         |
Transfer   | Video       | `r wk_save_ratio$ratio[4]`%  | `r bkp_retain_save_ratio$ratio[4]`%          | `r wk_retain_save_ratio$ratio[4]`%         |






## Conclusion
This paper introduces the multi-armed bandit problem, outlines several algorithms and describes Thompson sampling algorithm in details. Further, this paper describes the value remaining stop condition, which enables the Thompson sampling algorithm to function like a hypothesis test. For binary data generated by two static latent parameters, the simulation shows that the Thompson sampling with value remaining test has better power compared to a modified sequential probability ratio test for given sample size and similar power to fixed sample welch t-test in large sample size. However, the imbalanced sample assignment also leads to higher type I error rate. In addition, the Thompson sampling test has a significant lower regret than frequentist test procedures. In short, for atheoretical optimization experiment, high power and low cost make Thompson sampling test an attractive hypothesis testing procedure.  

Future work can be extended to study the power of multiple hypothesis comparison and the PAC stop condition family with the form that $\Pr(p_1 - p_0 \geq \epsilon)<\delta$ where $\epsilon$ does not vary with the base rate. Such stop condition is likely to have lower type I error rate for two-sided null hypothesis testing without sacrificing too much test power, and thus can be used in a wider range of experiment scenarios.



## Appendix III: 

### 1.Parameter Search for Modified Sequential Probability Ratio Test

The sequential probability ratio test is constructed as the following [@siegmund_sequential_2013]:

$$
T = \inf\{n:n \geq m_0, W(t)\geq \alpha\}
$$



Siegmund[@siegmund_sequential_2013] provides an approximation for choosing the parameter $m$, $a$ and $d$. However, in Monte Carlo simulation, the chosen parameter does not achieve the alleged power and significance combination. Therefore, the test parameters are chosen by brutal force grid search. It should be noticed that the only parameter relevant is the effect size ratio because of the adverse event transformation. The significance and power are calibrated based on 500 repetitions.

```{r, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
tabl <- " 
Ratio | a   | d  | m  | sig    | power
---   | --- | ---| ---| ---    | ---
0.1   | 3.4 | 2.1| 6711| 0.046 | 0.964
0.2   | 3.4 | 2.2| 1985| 0.046 | 0.958
0.5   | 3.2 | 2.4| 496 | 0.046 | 0.968
1.0   | 2.8 | 2.2| 139 | 0.046 | 0.974
"
cat(tabl) # output the table in a format good for HTML/PDF/docx conversion
```

Notice that $m$ is the number of pairs of different values, namely the adverse events. The expected number of adverse events is related to the base rate by $2*N*p*(1-p)$.

The minimum reject time $m0$ is set at 10.




### 2. Power Analysis on the Simulated Experiments
