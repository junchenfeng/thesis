---
title: "The Power of Thompson Sampling Algorithm with Value Remaining Stop Condition"
author: "Junchen Feng"
date: "Thursday, December 1st, 2015"
header-includes:
   - \usepackage{setspace}
   - \doublespacing
output: 
   pdf_document
bibliography: reference.bib
---

```{r,message=FALSE,warning=FALSE,echo=FALSE}
library(dplyr)
library(ggplot2)
library(gridExtra)
proj_dir = 'C:/Users/junchen/Documents/GitHub/multiarmedbandit/data/'
```

```{r,message=FALSE,warning=FALSE,echo=FALSE}
result = read.table(paste0(proj_dir,'power_result.txt'), sep=',',
                    col.names = c('p','ratio','method','i','power'))
result$power[result$method==' ttest'] = 0.95

saving_result = read.table(paste0(proj_dir,'saving_result.txt'), sep=',',
                           col.names = c('p','ratio','method','i','avg_reg','pdf'))

```

# 1. Motivation
A/B test has been a key drive of continued improvement for the online service sector[@scott_multi_2015]. Thanks to the ubiquitous data collection drived by the wide adoption of smart devices and censors, more experiments are going to be implemented on Internet[@einav_economics_2014]. Digital education service is likely to benefit from the continued improvement as well. For such purpose, there are two major motivations to move away from the classical fixed sample student-t test procedure. 

The first motivation is the ethical consideration. When experimenting with education content, one may want to minizes the damage to recipients with the inferior treatment, either by assigning less participants to the suspected inferior alternative or by terminating the experiment early had one alternative proves to be superior statistically.  Such ethical consideration has long been an important reason for medical experiments to adopt sequential test[@mukhopadhyay_sequential_2009]. 

The second motivation is the relative imporance of the type II error for optimization. The experimenter aims to determine which alternative yields the best result rather than to estimate the true treatment effect with precision. Continuous improvements usually deals with small effect size, thus power is the key concern for the test.

The multi-armed bandit (MAB) algorithms fits the first motivation very well because they are designed to minimize the regret. However, it is not clear whether experiments based on MAB algorithm has higher power than fixed sample student-t test or sequential test procedure, which has type I and type II error rate and the desirable feature of optional stopping.


# 2.Literature Review
Various algorithms of the Multi-armed bandit problem [@white_bandit_2012;@scott_modern_2010] have been used for website optimization. Thompson sampling is a bayesian algorithm that balances the trade off of exploration and exploitation. It has stong heuristics[@scott_modern_2010], good emprirical performance[@chapelle_empirical_2011] and a regret bound for finite time sequences[@kaufmann_thompson_2012]. The bound guarantees that the optimal arm is chosen exponetially more times than inferior alternatives as the experiment continues.

Desipte its popularity as A/B test methodology, the stop condition of the test and its statistical property has not been well documented. Google Analytics uses a stop condition called potential value remaining (PVR). On the help page for Google analytics, the power of such stop condition is illustrated with one simulation, while its general performance is not mentioned. Even-Dar et al[@even-dar_action_2006] proposed a stop condition based on a strategy that eliminates the worst half arms in each decision iteration. Although Even-Dar et derives a PAC bound for such policy, the elimination strategy itself may not be as good as the Thompson sampling algorithm.

This paper contributes to the literature by providing an extensive comparison of type I and type II error rate among Thompson sample with value remaining stop condition, fixed sample student t test and maximized sequential ratio probability test under frequentist assumption. It documents the frequentist test performance of the MAB algorithm and the importance of stop condition.

# 3.Multi-armed Bandit Algorithm
## 3.1 The Bandit Problem
There are $n$ experimental options (arm), each with a unknown return parameter $\mu_i$. The researcher designs a sequence of experiments (pulls) that samples one option at a time. The goal is to design an allocation policy that achieves the highest return. The name "multi-armed bandit" comes from the analogy to the slot machine. 

Denote the best arm has a expected return $\mu^*$, regret for each pull is defined as the difference between $\mu^*$ and the expected return of the chosen arm $\mu_t$. The dual problem of maximizing the return is to minize the regret during the tenure of the policy.

## 3.2 Optimal Policy and Thompson Sampling
The multi-armed bandit algorithm is notoriously hard to solve. Although the problem was raised during World War II, the first theoretical optimal solution did not surface until Gittins[@gittins_bandit_1979] proposed the index policy. Although it provides optimal solution for certain special cases, the exact policy is hard to compute. The family of Upper Confidence Bound(UCB) alogrithms [@lai_adaptive_1987;@lai_asymptotically_1985] describes a series of algorithm based on the mean return of each arm. The UCB algorithm has a weaker but more general guarantee than the Gittins index policy. It ensures that optimal arm is played exponentially more often than any of the suboptimal arm. Although the original UCB policy is hard to compute as well, later algorithm improves substantially [@scott_modern_2010]. The UCB policy family is hard to extend to dependent return between arms, such as the contextual test design.

In addition to the theorem derived optimal policy, there is an array of heursitic driven policies. The key trade-off heuristics are exploration versus exploitation. Exploitation refers to the heuristic that player the winner more often, while exploration emphasizes on giving loser a second chance because of the uncertainty around the parameter estimation, especially in small sample. $\epsilon$-decreasing strategy [@auer_finite_2002] is a hybrid of exploitation and exploration. At each pull, with probability $1-\epsilon$ play the estimated best arm, with probability $\epsilon$ choose a random arm. $\epsilon$ decreases as the time goes by, because the uncertainty decreases over time.

Random probability matching(RPM), also known as Thompson sampling, is mostly a heuristic strategy, although Kaufmann et al[@kaufmann_thompson_2012] provides a UCB like bound on the regret. The key improvement over the $\epsilon$-decreasing strategy is a stratified sampling strategy that weighs the different options proportionally to the respective estimated returns instead of equal weights. The additional practical benefit is no tuning parameter such as $\epsilon$ and its decay rate. At time $t$, RPM allocates the sample according to a weight matrix ${w_{1t},\dots,w_{nt}}$ where $y_t$ is the observed return sequence to time $t$ and 

$$
w_{at} = Pr(\mu_a = max\{\mu_1, \dots, \mu_n\}|y_t)
$$

There are also engineering advantages for choosing Thompson sampling. Because the reward is not updated instaneously but rather in batches, a policy with random allocation rather than deterministic allocation can reduce the regret due to over exploiting the inferior arm in batch assignments. In addition, Thompson sampling can easily accomendate prior information, heterogeous reward distribution and contextual test design by modifying the prior and the posterior update procedure.

In short, Thompson sampling provides a good balanance in the trade off between exploration and exploitation, a flexible statistical framework and good empirical perforamce. Among all algorithms to the MAB problem, it is the most ideal choice for field deployment. 

# 4. Multi-armed Bandit Algorithm as a Statistical Test

Bandit algorithm is not a statistical test per se because it is designed to minimize regret over an time horizon rather than  reach a decision on whether to reject the null hypothesis. Therefore, statistical testing with multi-armed bandit algorithm requires a stop condition. That said, the bandit problem and statistical test are intrinsically linked.


## 4.1 Potential Value Remaining Stop Condition
This paper analyzes the "potential value remaining" (PVR) stop condition used by Google Analytics. For now, it is a choice of convenience since it is the most widely used stop condition.

"Value remaining" measures the regret of the chosen arm. Given the posterior distribution of return as  $\theta_i|Y$, draw $M$ samples from the posterior distribution. For each sample, denote the max return as $\theta^*(m)$. The same sample also yields the returns of the chosen optimal arm $i^*$ for each sample, denoting as $\theta_{i^*}(m)$. The value remaining distribution is estimated by the following formula 
$$
VR = \frac{\theta^*(m)-\theta_{i^*}(m)}{\theta_{i^*}(m)} = \frac{\theta^*(m)}{\theta_{i^*}(m)} - 1 
$$

Potential value remaining stop condition has two parameters ($\epsilon$,$\delta$), the ratio threshold $\epsilon$ and the cumulative probabily threshold $\delta$. Mathematically, the experiments stops when the following inequality holds

$$
\Pr(VR > 1+\epsilon) < \delta
$$

PVR stop condition can be interpreted as a PAC bound that $\Pr(\theta^*-\theta>\epsilon*\theta) < \delta$. The difference threshold varies according to the base rate. Alternatively, the PVR stop conditions can be morphed into a narrative of p-value: There is less than $\delta$ probability to observe the the alternative arm outperforms the chosen arm by $\epsilon$ percent given that the posterior belief is true. 

## 4.2 Simulation Analysis
### 4.2.1 Parameters
The working hypothesis is
$$
\begin{aligned}
H_0&: p_0 < p_1\\
H_1&: p_0 \geq p_1
\end{aligned}
$$

Since power is a frequentist concept, the true value of the parameter is fixed. This paper considers three types of base rate ($p_0$)

```{r, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
tabl <- " 
Type | Values
--- | ---
Low | 0.03,0.05,0.07,0.09
medium| 0.15, 0.2, 0.25, 0.3
high| 0.4, 0.5
"
cat(tabl) # output the table in a format good for HTML/PDF/docx conversion
```

For practical purpose, the effect size ($\triangle$) is measured by percentage (relative) rather than percentage points(absolute). The effect sizes are 10%, 20%, 50%, 100%, representing small to huge effect size. 


For a fair comparison, the maximum sample size($T$) for sequential test is the same as the sample size required for a 5% significance and 95% power of one sided welch-t test. 


Therefore, the basic simulation parameter tripl is $(p_0, \triangle, T)$.


### 4.2.2 Test Procedures

The fixed sample t-test is a test of two sample mean test with unequal variance, or welch t-test. The test statistics is given by
$$
\begin{aligned}
t &= \frac{\bar{X_1} - \bar{X_0}}{\sqrt{\frac{s_1^2}{n_1}+\frac{s_0^2}{n_0}}} \\
\end{aligned}
$$
The test is done by the scipy package.


The sequential test is modified sequential probability ratio test[@siegmund_sequential_2013]. Define 
$W(n) = nH(\frac{\sum_{n=1}^NY_n}{N})$ where $H(x) = xlogx+(1-x)log(1-x)+log2$, $Y_n=0$ if $X_{0,n}=1,X_{1,n}=0$, $Y_n=1$ and $X_{0,n}=0,X_{1,n}=1$. Define the reject time as 

$$
T = \inf\{n:n \geq m_0, W(t)\geq \alpha\}
$$

The sampling stops at $min(T,m)$ and reject the null hypothesis if $T \leq m$ or $T>m, mH(\frac{\sum_{j}Y_j}{m}) \geq d$. The test parameter is $(\alpha, d, m, m_0)$.


The Thompson sampling with value remaining stop condition (hence refered to as bandit test) rejects the Null hypothesis if the stop condition is met within sample limitation and the chosen arm generates $X_1$.

The bandit test requires two tuning parameter. The first is prior distribution and the second is inspection interval, i.e. how often the stop condition is checked. The prior distribution for each arm is Beta(2,5), which incorporates some information about the distribution of the base rates. The inpsection interval matters because it controls the "repeated significance test" problem, otherwise known as early peeking. The inspection interval is set at such value that there are 49 inspections before the sample run out. The choice is made to facilitate visualization, rather than to optimize the test performance.

### 4.2.3 Analysis of power
Figure 1 is an illustration of the power comparison. The colums are effect size ratios and the rows are base rate.

```{r,message=FALSE,warning=FALSE,echo=FALSE}
qplot(data=result,x=i,y=power,col=method,geom='line', facets = p~ratio)
```

Overall, the bandit has superior performance on power. Compared to the t-test, the bandit test achieves similar power in large sample. Compared to the sequential test, the bandit test has larger power for given sample size, except for cases where effect size is large and the base rate is low. It shall be noticed that the superior performance is achieved when the sequential test is calibrated to each simulation environment while the bandit test relies on a global configuration. In practical scenario where true paramter is unknown, the bandit test can be even better.


# 5. Discussion

## 5.1 The Importance of Type I Error
Consider the null hypothesis of two sided tests where the experiment tries to distinguish the impact on click rate between a dark blue button and a light blue button. This example is motivated by the famous anecdote that Marrisa Myer "commissioned" a 40 shades of blue on all Google buttons. 
$$
\begin{aligned}
H_0: & p_{dark} = p_{light}\\
H_1: & p_{dark} \neq p_{light}
\end{aligned}
$$

For an engineering decision, the cost of type I error is nearly zero. If the click rate is indeed equivalent, there is no difference in choosing either dark blue or light blue. However, type II error is potentially expensive. For large traffic, even tiny improvements accumulate to be significant overtime. In addition, when the orgnization adopts an "A/B test everything" mentality, systemic low power test can be even more costly. Consequently, Scott[@scott_modern_2010] claims that, for website optimization, type I error costs almost nothing while type II error costs dear.

There are certain areas of education service where design is not driven by theory, or the guiding theory has not been developed. A notable example is the optimal difficulty profile for practice item recommendation based on item response theory. The predicted succuess rate has high accuracy yet it is not clear what the optimal composition is.

However, the critique of the type I error is in fact a critique of bad hypothesis. Rarely does the analyst (or product manager) wants to know if two solutions are equivalent, but rather if the alternative solution is superior to the default solution. The type I error matters for a better formulated hypothesis.

In addition, if the experiment aims to make casual inference, rather than to make a product decision, the stake for type I error is high. If the experiment is motivated by some visual cognitition theory, whose result informs the hue selection for the entire website design, then it is a costly mistake to assert difference where none exists.



# Conclusion
This paper introduces the mult-armed bandit problem, outlines several algorithms and describes Thompson sampling algorithm in details. Further, this paper describes the value remaining stop condition, which enables the Thompson sampling algorithm to function like a hypothesis test. For binary data generated by two static latent parameters, simulation shows that the Thompson sampling with value remaing test has better power compared to a modified sequential probability ratio test for given sample size and similar power to fixed sample welch t-test in large sample size. In addition, the Thompson sampling test has significant lower regret than frequentist test procedures. In short, for atheoretical optimization experiment, high power and low cost makes Thompson sampling test an attractive hypothesis testing procedure.  

Future work can be extended to study the power of multiple hypothesis comparison and the PAC stop condition family with the form that $\Pr(p_1 - p_0 \geq \epsilon)<\delta$ where $\epsilon$ does not vary with the base rate. Such stop condition is likely to have lower type I error rate for two sided null hypothesis testing without sacrificing too much test power, and thus can be used in a wider range of experiment scenarios.



# Appendix I: Parameter Search for Modified Sequential Probability Ratio Test

Consider null hypothesis $H_0:p_0=p_1$, then $P(Y_1=1|Y_1 \neq Y_0)= 0.5$. Similarly, if $p_0<p_1$, $P(Y_0=1|Y_1 \neq Y_0)<0.5$; if $p_0>p_1$, $P(Y_0=1|Y_1 \neq Y_0)>0.5$. Therefore, the null hypothes is $H_0:p_0<p_1$ is equivalent to $P(Y_0=1|Y_1 \neq Y_0)<0.5$.

Motivated by the maximized likelihood ratio, the test statistics is $W(n) = nH(\frac{\sum_{j}Y_j}{n})$ where $H(x) = xlogx+(1-x)log(1-x)+log2$ and $Y_j=1$ if $X_1=1,X_0=0$, $Y_j=0$ if $X_1=0,X_0=1$. $X_1,X_0$ are draws from the respective bernoulli distribution.

The sequential probability ratio test is constructed as the following [@siegmund_sequential_2013]:

$$
T = \inf\{n:n \geq m_0, W(t)\geq \alpha\}
$$

The sampling stops at $min(T,m)$ and reject the null hypothesis if the sample stops before $m$.

$m_0$ is the minimum time of rejection. However, such configuration has low power given significance level. The sequential test can be modified to have a second test at time $m$ and a higher value of $\alpha$ to compensate the false positive in the second test. Therefore, the modified rejection is $T \leq m$ or $T>m, mH(\frac{\sum_{j}Y_j}{m}) \geq d$.

Siegmund[@siegmund_sequential_2013] provides an approximiation for choosing the parameter $m$, $a$ and $d$. However, in monte carlo simulation, the chosen parameter does not achieved the alleged power and significance combination. Therefore, the test parameters are chosen by brutal force grid search. It should be noticed that the only parameter relevant is the effect size ratio.

```{r, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
tabl <- " 
Ratio | b   | c  | m  | sig    | power
---   | --- | ---| ---| ---    | ---
0.1   | 3.4 | 2.1| 6711| 0.046 | 0.964
0.2   | 3.4 | 2.2| 1985| 0.046 | 0.958
0.5   | 3.2 | 2.4| 496 | 0.046 | 0.968
1.0   | 2.8 | 2.2| 139 | 0.046 | 0.974
"
cat(tabl) # output the table in a format good for HTML/PDF/docx conversion
```

Notice that $m$ is the number of pairs with different values, namely the adverse events. The expected number of adverse events is related to the base rate $2*N*p*(1-p)$.

# Appendix II: Type I Error Rate 
The prior for the Thompson sampling is Beta(2,5) and the stop threshold for potential value remaining is (1%, 5%). For each simulation, the max iteration for Thompson sampling is 30,000. Each base rates runs 100 simulations. The type I error is calculated as the percentage of simulations that stops before reaches the max iteration.


The simulation demostrates two observations for the type I error for "two sided tests":
(1) As the sample size grows, the Thompson test is more prone to type I error
(2) As the base rate increases, the Thompson test is more prone to type I error

However, the posterior probability of either arm being the optimal arm is close to 50%. Despite the correct bayesian inference, the stop condition nevertheless arrives at the wrong decision. 

```{r,message=FALSE,warning=FALSE,echo=FALSE}
sig_Thompson = read.table(paste0(proj_dir, 'sig_Thompson.txt'), sep=',', col.names=c('p','k','sig'))
sig_Thompson = sig_Thompson %>% filter(p!=0.1 & p!=0.01) 
sig_Thompson$type='Thompson'

sig_Thompson$p = factor(sig_Thompson$p)

qplot(data=sig_Thompson, x=k, y=sig, col=p, geom='line')
```

The two observations can be explained by the imbalance sample allocation due to the tendency favoring exploitation. If the sample are equally distributed among the two arms, simulation shows that type I error decreases slowly as the sample size grow, albeit still at a high level (on average 40% for the exact sample size of 30,000). However, the favored arm receives more sample than the other, shrinks its beta posterior faster than the alternative, thickens the right tail and increases the type I error rate. Similarly, the higher base rate, the higher the variance of posterior distribution, and the larger the imbalance sample effect is.  

# Appendix III Savings of Sequential Tests 

Define the regret as the expected loss of payoff had the experiments always chosen the optimal arm. The simulation parameter triple is $(p, \triangle, N)$, where $p$ is the base rate, $\triangle$ is the effect size ratio and $N$ is the sample size calibrated for 0.05 significance 0.85 power welch t-test .

The expected regret of the welch t-test is fixed at 
$$
r_{welch} = N*p*\triangle
$$

The expected regret a sequential test ended in period t is 
$$
r_{x}(t) = N_0(t)*p*\triangle
$$
where $N_0$ is the number of participants assigned to the inferior option  

Therefore, the expected regret for seqential test ended before $T$ is 

$$
E(r_x|t\leq T) = \int_{m_0}^{T} r_{x}(t) f(t|t\leq T) dt
$$

Both the conditional regret function $r_{x}(t)$ and the conditional probability density function $f(t|t\leq T)$ is estimated by average sample sample in bins. The bin size is $\frac{N}{24}$

Since the relative regret saving, rather than the absolute regret saving, is the key quantity of interest, define the saving ratio as 

$$
R_x(T) = \frac{E(r_x|t\leq T)}{r_{welch}}
$$


Figure 2 shows the relative saving of the sequential test and the bandit test. The bandit regret is about 10% compared to the fixed sample student t test while the sequential test regret is about 30%. More importantly, the sequential test regret percentage grows as the sample size increases, while the bandit regret percentage flats out. The slow growth of regret of the bandit test may be attributed to the stratified sampling strategy when explores. In later stage, the sample are more frequently assigned to the optimal arm, in contrast to sequential probability ratio test's equal assignment due to pairing.

```{r,message=FALSE,warning=FALSE,echo=FALSE}
# calculate the cumulative regrets
for (k in seq(2,25)){
    saving_stat = saving_result %>% filter(i<=k) %>%
                    group_by(p, ratio, method) %>%
                    mutate(cond_pdf = pdf/sum(pdf)) %>%
                    mutate(cum_reg = cumsum(avg_reg*cond_pdf)) %>% 
                    select(p, ratio, method, i, cum_reg)
    
    
    ttest_stat = saving_stat %>% filter(method == 'ttest') %>% rename(benchmark_reg=cum_reg) %>% select(p, ratio, i, benchmark_reg) 
    tmp = merge(saving_stat, ttest_stat, by=c('p','ratio','i'))  %>%
        transform(saving_ratio = cum_reg/benchmark_reg) %>%
        filter(method.x!='ttest') %>% select(-method.y) %>%
        filter(i==k)
    if (k==2){
        saving_res = tmp
    }else{
        saving_res = rbind(saving_res, tmp)
    }
}


qplot(data=saving_res,x=i,y=saving_ratio,col=method.x,geom='line', facets = p~ratio)

```



# References

