# Evaluating Pedagogical Efficacy in the Low Stake Environment {#efficacy}

```{r, echo=FALSE,message=FALSE,warning=FALSE}
library(dplyr)
library(ggplot2)
library(stargazer)
library(gridExtra)
rm(list=ls())

proj_dir = getwd()
input_file_path = paste0(proj_dir,'/_data/02/production_data.RData')
load(input_file_path)
```

## Pedagogical Efficacy and its Measurement
### Definition
The pre-requisite to discuss pedagogical efficacy is an understanding how people learn in general. It is a very broad field[@bransford1999people] and many foundings are open to debate, even on what counts as instruction[@kirschner2006minimal;@hmelo2007scaffolding]. 

A general definition of pedagogical efficacy is beyond the scope of this dissertation. Since most of the learning services rely on practice question to gauge the learner's mastery level, this chapter defines pedagogical efficacy in reference to user's practice result.  Learning gain is defined as the absolute performance difference in quizzes before and after the pedagogical intervention. It is a metric Khan Academy used to improve their video instructions in experiments[@faus2015systems]. The first difference approach implicitly assumes that the measurement errors, if exists, are the same before and after the intervention.

This chapter defines pedagogical efficacy similar to the learning gain metric with a slight twist. The pedagogical efficacy is defined as the difference in learning gain compared to a naive repetition strategy. The difference in difference approaches only requires the trend of the measurement error between two periods are the same for both groups.

### Three Obstacles

Conducting educational experiments online is not new ideas[@williams_mooclet_2014]. Many experiments have been carried out in the industry , such as Khan Academy[@Wang2014Khan] and Duolingo[@Ahn2012User]. However, with few exceptions[@paunesku2015mind], the idea is yet to catch fire in the academia. By the highest academic standard, online education experiments suffer from two critical weakness: selection bias induced by differential user attrition and measurement error induced by slacking. 

#### High Attrition Rate
Compared to offline experiments that are usually coupled with the regular school year, online experiments have an extremely high attrition rate due to the voluntary nature of the online learning service.  
Unlike instantaneous outcome that requires only one measurement, such as click rate, learning gain requires at least two measurements. Users enrolled in the baseline test may not be observed again in the post-test. In contrast, offline experiments conducted in a school setting does not need to sweat over user attrition as much because school attendance or high stake test is somewhat mandatory. 

The sample attrition does not equal selection bias for the average treatment effect. First of all, the "population" of interest needs to be defined. For practical purpose , the population of interest can be reduced to the active user of the learning service. The natural attrition rate is very high even for social apps, Except for the notable few exception such as facebook, monthly user retention rate falls between 30% to 60%[@Efrati2015Which].  If a learning service provider has a monthly retention rate of 30%, an experiment that runs for a month is expected to lose 70% of the enrolled participants, yet still be representative of the active users, as long as the drop out decision is independent to the experiment per se. Because online experiments are usually small tweaks to the overall user experience, the missing at random assumption is likely to hold.

However, differential user attrition rate caused by the experiment cannot be ruled out logically. From field experience, the most likely confounding factor of learning gain and user attrition is grit. Learners need the grit to overcome their previous failure in order to achieve learning gain. Users need the grit to log into the learning service regularly.

The natural attrition leads to a loss of precision, which can be compensated by the large user base. The selective attrition leads to a loss in consistency, which has to be addressed a deliberate research design. 

#### Measurement Error

The low stake of the online learning task does not incentivize student to exert their best effort. Error due to a lack of effort rather than a lack of mastery is considered as measurement error when trying to measure pedagogical efficacy. Here mastery stands for cognitive skill while effort, for a lack of better terms, stands for the non-cognitive skills.

Incorrect responses have been traditionally associated with ONLY with a lack of mastery, such as in Item Response Theory or in Bayesian Knowledge Tracing model. In a high-stake test environment, it may be innocuous to assume that learners exert full effort. However, a growing body of research has shown that is not the case in low stake test environment. Baker et al [-@baker2004off] have studied the "off-task" behavior on digital learning while Fryer [-@fryer2010financial] has shown that lack of effort may account for about 2/3 of the racial gap in achievement as measured by low stake test. In an online learning environment, the learning task is usually low stake because they are used as a formative assessment or optional self-learning material, where an incorrect response has little negative consequence. A student without adequate grit is likely to shirk from exerting full effort.

The identification of effort induced measurement error is not a trivial task. Without the behavior data that characterize how students answer the question, the error due to a lack of trying and the error due to a lack of mastery is observational equivalent. Furthermore, the sign of the bias in the average treatment effect is not definite (Appendix II.1 provides a formal discussion). Thus one cannot use Minsky bound to provide an interval estimation.



 
## Case Study: Efficacy Evaluation of Scaffolding Delivery 

This section describes a field experiment that evaluates the pedagogical efficacy of remedial exercises in primary school math learning. Remedial exercises give learners extra practice opportunities to strengthen their understanding of certain knowledge points or problem-solving skills. It is a highly valued pedagogical method in China. A learning service providers usually have a large stock of similar items on a particular knowledge point or skill, but it is not clear what item to present and whether or not the remedial exercises actually achieve the learning gain. These are the questions that can be answered by a series of online experiments.

The experiment evaluates the efficacy of different delivery methods for remedial scaffolding. The scaffolding is delivered either by vocabulary description or by video instruction. The efficacies on both routine skill and transfer skill are evaluated.


### The Learning Environment

The experiment is carried out in a paid self-learning product offered by a Chinese online learning service provider. The product is framed as a role-playing game where the learner can claim the reward by clearing a level, defined as accumulating 12 correct answers. There is no punishment for an incorrect answer and the correct answer is shown to them before they go to the next question. The learner can practice a similar item to those that they have made a mistake on, earning an additional but smaller reward. If the learner drops out in the middle of a level,  when they log into the same level again, the incorrect items on this level will be shown while the correct items are skipped. The overall design aims to "encode success into exercises"[@lemov2012practice].

The reward is virtual currency that can be used to exchange for gifts or in-game objects. Although the currency cannot be cashed out but there is an exchange rate to cash(RMB) because of the gift exchange. During the experiment, the reward for a correct response on the first attempt is about  a tenth of a penny in RMB (about $0.00014). The low stake encourages students to skip difficult items by submitting a blank answer or non-sense answer such as emoji in hope to get an easier one to score a correct response on the first attempt. Some learners also exploit the system design to "mine" rewards efficiently by repeat log in&out of the level to peep at the correct answer and explanation. All these system gaming behavior results in significant measurement error in the dataset.


The experiment is administrated from June 9th to June 10th to third-grade students whose parents paid for the learning product. By then, in theory, all users should have been taught the required knowledge point in the school. Users were not aware that they were participating in an experiment.

### The Learning Tasks

The primary learning objective is to apply the circumference and area formula of rectangles. 

The original question (Figure 3.1) asks the learner to calculate the circumference and area of two identical rectangles joined by length, given the length and width of the small rectangle.

<center>![Figure 3.1: The Original Question](fig/f1.png)</center>

The remedial exercise is a modified version of the original question(Figure 3.2). The question is identical to the original except for the value of the length and width. It also preserves the feature that the joined width is the new length.

<center>![Figure 3.2: The Remedial Exercise](fig/f2.png)</center>

The scaffolding guides the learner to find the width and length of the new shape and then apply the formula. The scaffolding is drawn from the teaching experiment of in-house tutor experts. To wit:

(1) What is the length and width of the new rectangle

(2) What is the circumference of the new rectangle

(3) What is the area of the new rectangle

The assessment of routine skill (Figure 3.3) asks the learner to calculate the same quantities for two rectangles joined by width. A highly similar yet not identical item enhances the measurement validity while prevents student memorizing the answer.

<center>![Figure 3.3: Assessment for the Routine Skill](fig/f3.png)</center>

The assessment of transfer skill (Figure 3.4) asks the learner to calculate the areas of the smaller squares when split the large rectangle in half by length, given the difference in circumferences. The learner cannot blindly apply the scaffolding of the previous items, but have to use the properties of the square.

<center>![Figure 3.4: Assessment for the Transfer Skill](fig/f4.png)</center>

### Group Assignment

Each learner receives a packet of 4 items: the original question, the remedial exercise, and two assessments. Packets differ only in the remedial exercise. The control group receives the item without scaffolding. The first treatment group receives the item with vocabulary description of the scaffolding that the learner solves step by step as sub-questions. The second treatment group receives the item with a link to the animation of the scaffolding with human voice over. If the learner cho0ses not to open the link and directly answers the item, it is effectively the same as the naive repetition.

The recruited users are randomly assigned with one of three item packets based on the remainder of their user id divided by 5: 0 is the control group, 2 vocabulary treatment, and 4 video treatment. The user id is randomly generated.



## Attrition Rate and Measurement Error


### Missing at Random

Acknowledging that attrition is common in field experiments and only differential attrition matters, What works clearinghouse(WWC) evidence standard 3.0[@clearinghouse2008wwc] provides a guideline on the acceptable user attrition profile. Figure 3.5 characterizes three types of attrition profile: without reservation(green), with reservation(yellow), and unacceptable(red). The x-axis is the overall attrition rate while the y-axis is the differential attrition rate between the treatment and the control. 

<center>![Figure 3.5: Relationship between overall attrition rate and differential attrition rate, from WWC evidence standard 3.0 figure III.2](fig/attrition_boundary_wwc.png)</center>


```{r, echo=FALSE,message=FALSE,warning=FALSE}
final_retention = user_retention_stat %>% filter(k==4)

tot_retention_rate = round(sum(final_retention$n)/sum(final_retention$N)*100,2)
ctrl_retention_rate = round(final_retention$pct[final_retention$gid==0]*100,2)
tr1_retention_rate = round(final_retention$pct[final_retention$gid==2]*100,2)
tr2_retention_rate = round(final_retention$pct[final_retention$gid==4]*100,2)

ctr_ret = user_retention_stat %>% ungroup(gid) %>% filter(gid==0) %>% select(k,pct) %>% rename(ctr_pct=pct)
tr_ret = user_retention_stat %>% filter(gid!=0) %>% select(k,pct)
ret_dif_composition = merge(tr_ret,ctr_ret,by='k') %>% mutate(ret_dif = ctr_pct-pct)

```

Previous experiments in the same learning environment have shown an attrition rate of 99.9% for a 3-month experiment and 88% for a 14-day experiment. To be within the green zone of the WWC standard, the current experiment deliberately chooses a small packet so that the student can finish in one session On average, the learners spent less than 2 minutes on the packet. 

|Items| Time (Seconds) |
|:-------------| -------------------:|
|Original | 34 |
|Naive Repetition | 27 |
|Vocabulary Scaffolding | 24 |
|Video Scaffolding | 40 |
|Routine Assessment | 25 |
|Transfer Assessment | 8 |
Table: Table 3.1 Average Time Spent on Items

Despite the short time required to finish the experiment,  only `r tot_retention_rate`% finished all four items. The attrition rates differ among the three groups. The maximum differential attrition rate between groups is about 7%. Such profile meets the WWC evidence with reservation. 

|Group|Attrition Rate(%)|
|:-------------| -------------------:|
|Control|`r 100-ctrl_retention_rate`|
|Vocabulary Treatment|`r 100-tr1_retention_rate`|
|Video Treatment|`r 100-tr2_retention_rate`|
Table: Table 3.2 Retention Rate by Assignment Group


Further examination of the attrition rate by item shows that the largest difference occurs in the first item(Figure 3.1). Since the learners have not experienced the treatment, the differential attrition can probably be attributed to chance. Thus the meaningful attrition rate shall exclude the dropouts from the first item. By this calculation, the maximum differential attrition rate is 3%. Such profile meets the WWC evidence without reservation. 

```{r, echo=FALSE,message=FALSE,warning=FALSE, fig.cap = "Relative Attrition Rate Compared to the Control Group", fig.align='center'}
ret_dif_composition$group = factor(ret_dif_composition$gid, labels=c('Vocabulary','video'))

ggplot(data=ret_dif_composition,aes(x=k,y=-ret_dif,fill=group)) +
  geom_bar(stat='identity',position = "dodge") +
  ggtitle('Relative Attrition Rate Compared to the Control Group') +
  ylab('Attrition Rate Difference') + 
  scale_x_discrete(name ="Item Sequence", 
                    limits=c("Original","Remedial","Routine","Transfer"))

```

In sum, although the experiment has substantial attrition rate, there is no substantial difference in the attrition rate among groups. Therefore, the following analysis is carried under the assumption of missing at random. The analysis does not use the inverse probability weighting to compensate the precision loss because the dataset does not contain enough auxiliary learner information to model the sample attrition. 


### Measurement Error Due to A Lack of Effort
Thanks to the log level data collection, it is possible to directly classify incorrect responses into two categories: error due to a lack of mastery (valid attempt) and error due to a lack of effort (give-up). However, even with the knowledge of response time and response answer, the classification is not very accurate. Future high-resolution data are likely to include more behavior data that shed light on the learner's affective state[@baker2014extending], but now this chapter has to do with the limited information.

Since there are numerous ways to provide an incorrect answer (emoji included) but manageable amount of "innocent" mistakes, this chapter adopts the principle of "guilty until proved innocent" and only count "innocent" mistakes as valid attempts, whose definition is in Appendix II.2. Following this classification, about `r round(mean(c2a2f1_data$giveup)*100,2)` % of the wrong response are classified as give-ups. Although there is no way to measure the true classification error, an examination of the distribution of response time can serve as a check of face validity. If the learner is not going to try, he is unlikely to spend much time on the exercises. Therefore the distribution of response time for the give-up is expected to severely skew to the left. Empirically, the response time is defined as the time between the user's requests for next item. It is not guaranteed that the learner is on task for the entire duration. The empirical distribution of response time (Figure 3.2) conforms to the prior expectation. However, the give-up has significant density mass in the longer duration. This can be either classification error or the possibility that the learner's attention wander off between two clicks. Appendix II.3 replicates the following analysis with an alternative definition of the give-up (submitting a blank answer), the pattern of the finding remains robust.



```{r,echo=FALSE,message=FALSE,warning=FALSE, fig.cap = "Distribution of Time Spent on Item by Error Types", fig.align='center'}
# create new error type
c2a2f1_data$type = c2a2f1_data$giveup
c2a2f1_data$type[c2a2f1_data$is_blank_ans==1] = 2
c2a2f1_data$type = factor(c2a2f1_data$type, labels=c('A lack of Mastery','A lack of Effort-Non blank', 'A lack of Effort-Blank'))

qplot(data=c2a2f1_data %>% filter(cmt_timelen<=60), x=cmt_timelen, geom='density',col=type) +
  ggtitle('Distribution of Time Spent on Item by Error Types(<= 60s)') +
  xlab('Seconds')
```



## Analysis

### Identification Strategy

The identification strategy is the Difference in Difference specification. Denote the baseline question as T=0 and the assessments questions as T = 1. Let variable $D^1$ takes value 1 if the user is in the treatment I, variable $D^2$ takes value 2 if the user is in treatment II. A fully specified DID model is 

$$
Y_{i,T} = \beta_1 D^1_i + \beta_2 D^2_i + \gamma T + \delta_1 D^1_i T + \delta_2 D^2_i T + \epsilon_{i,T} 
$$

However, since the remedial exercise means the student to make mistakes in the first place, the baseline mean is zero. Thus the fixed effect can be dropped.
$$
Y_{i,T} = \gamma T + \delta_1 D^1_i T + \delta_2 D^2_i T + \epsilon_{i,T} 
$$

Alternatively, to facilitate the comparison between two treatment alternatives, let $\tilde{D}_i$ denotes if the learner is in either of the treatment group:
$$
\tilde{D}_i = D^1_i + D^2_i
$$

The specification can be expressed as a combination of the level and the difference.

$$
\delta_1 D^1_i T + \delta_2 D^2_i T = \delta_{level}\tilde{D}_i T + \delta_{difference} D^2_i T
$$
where $\delta_{level} = \delta_1$ and $\delta_{difference} = \delta_2 - \delta_1$

The alternative regression model is thus
$$
Y_{i,T} = \gamma T +\delta_{level}\tilde{D}_i T + \delta_{difference} D^2_i T + \epsilon_{i,T} 
$$




### Baseline Results
The summary statistics of the post-treatment data are presented in Appdenix II.3

```{r, echo=FALSE,message=FALSE,warning=FALSE}
y0data = workdata %>% filter(eid=='Q_10201056649366') %>% mutate(t=0)
y1pdata = workdata %>% filter(eid=='Q_10201056666357') %>% mutate(t=1)
y1sdata = workdata %>% filter(eid=='Q_10200351208705') %>% mutate(t=1)

ydata_p = rbind(y0data,y1pdata)
ydata_p = ydata_p %>% transform(d1=as.numeric(gid==2),d2=as.numeric(gid==4),d=as.numeric(gid!=0))

ydata_s = rbind(y0data,y1sdata)
ydata_s = ydata_s %>% transform(d1=as.numeric(gid==2),d2=as.numeric(gid==4),d=as.numeric(gid!=0))

ydata_p = ydata_p %>% transform(dt=d*t,ddt =d2*t, d2t=d2*t,d1t=d1*t)
ydata_s = ydata_s %>% transform(dt=d*t,ddt =d2*t, d2t=d2*t,d1t=d1*t)

# keep integer answer
ydata_p$y = as.numeric(ydata_p$y==1)
ydata_s$y = as.numeric(ydata_s$y==1)

```

Apply the identification to the full dataset, the results (Table 3.5) shows nothing at all.

```{r, echo=FALSE,message=FALSE,warning=FALSE, results='asis'}
mod_1d = lm(data=ydata_p%>% filter(is_placebo==0),y~t+dt+ddt)
mod_2d = lm(data=ydata_s%>% filter(is_placebo==0),y~t+dt+ddt)

mod_1 = lm(data=ydata_p%>% filter(is_placebo==0),y~t+d1t+d2t)
mod_2 = lm(data=ydata_s%>% filter(is_placebo==0),y~t+d1t+d2t)

stargazer(mod_1, mod_2, mod_1d,mod_2d,
          header=FALSE,type='latex',
          dep.var.labels = 'Response',
          keep=c('d1t','d2t','dt','ddt'),
          covariate.labels=c('vocabulary','video','level','difference'), 
          column.labels=c('Routine','Transfer','Routine','Transfer'), 
          keep.stat=c("adj.rsq","n")
          )
```


### Differential Measurement Error Trend in the Placebo
Because the package is administrated to all learners, the learners who correctly answered the original question also answered the following questions. Since there is negligible chance that they guessed it right, they must have mastered the learning objective. The placebo group thus shall report a null effect if the assumption of the same measurement error trend holds.

However, the vocabulary scaffolding reports a significant drop in success rate for the routine assessment (Table 3.6), compared to the naive repetition. The rate  of give-up climbs almost one for one for the vocabulary scaffolding compared to the naive repetition. There is no similar phenomenon for the transfer assessment. The result suggests that the vocabulary scaffolding has the unintended consequence of decreasing the learner's engagement in the learning task: They are too bored to make an effort.

```{r, echo=FALSE,message=FALSE,warning=FALSE, results='asis'}
mod_3 = lm(data=ydata_p %>% filter(is_placebo==1),y~t+d1t+d2t)
mod_4 = lm(data=ydata_p %>% filter(is_placebo==1),giveup~t+d1t+d2t)

mod_5 = lm(data=ydata_s %>% filter(is_placebo==1),y~t+d1t+d2t)
mod_6 = lm(data=ydata_s %>% filter(is_placebo==1),giveup~t+d1t+d2t)

stargazer(mod_3, mod_4,mod_5,mod_6, 
          header=FALSE,type='latex', 
          dep.var.labels = c('Response','Giveup','Response','Giveup'),
          keep=c('d1t','d2t'), 
          covariate.labels=c('vocabulary','video'), 
          column.labels=c('Routine','Routine','Transfer','Transfer'), 
          keep.stat=c("adj.rsq","n")
          )
```


Excluding observations with measurement error violates the assumption of missing at random because some unobserved factor can affect both effort and performance, such as grit. The research design does not offer an identification strategy that can separate the clean response from measurement error. It requires an instrument variable that changes learner's choice of effort while not affect their performance, such as a variation in incentive scheme[@fryer2010financial]. Such design was declined by the platform due to the issue of implementation cost and user experience consistency. Therefore, the analysis corrects for the measurement error by excluding observations that are identified as give-ups in their exposure to the treatment.

The retention rates are not equal across three groups but are almost identical for the treatment groups. It suggests that the comparison between the treatment and the control shall be taken with a grain of salt, the comparison between two treatment groups is still very likely to be valid.

```{r, echo=FALSE,message=FALSE,warning=FALSE}
retain_rate_stat =  ydata_p %>% filter(is_placebo==0) %>% group_by(gid) %>% summarize(pct=mean(is_retain)) 
```

|Group|Attrition Rate(%)|
|:-------------| -------------------:|
|Control|`r 100-round(retain_rate_stat$pct[1]*100)`|
|Vocabulary Treatment|`r 100-round(retain_rate_stat$pct[2]*100)`|
|Video Treatment|`r 100-round(retain_rate_stat$pct[3]*100)`|
Retention Rate by Assignment Group

The analysis on the filtered dataset (Table 3.7) shows vocabulary scaffolding is almost as effective as naive repetition in both routine and transfer assessment but video scaffolding is more effective in both cases. The pedagogical superiority of the video instruction over vocabulary instruction is only confirmed in the transfer assessment. The result confirms in-house pedagogical expert's expectation at large. She postulates that both vocabulary scaffolding and video scaffolding are more effective than the naive repetition. She also postulates that both delivery methods are equivalent at the routine assessment, while the video scaffolding is more effective at the transfer assessment.

```{r, echo=FALSE,message=FALSE,warning=FALSE, results='asis'}
mod_7d = lm(data=ydata_p%>% filter(is_placebo==0&is_retain==1),y~t+dt+ddt)
mod_8d = lm(data=ydata_s%>% filter(is_placebo==0&is_retain==1),y~t+dt+ddt)

mod_7 = lm(data=ydata_p%>% filter(is_placebo==0&is_retain==1),y~t+d1t+d2t)
mod_8 = lm(data=ydata_s%>% filter(is_placebo==0&is_retain==1),y~t+d1t+d2t)

stargazer(mod_7, mod_8, mod_7d,mod_8d,
          header=FALSE,type='latex',
          dep.var.labels = 'Response',
          keep=c('d1t','d2t','dt','ddt'),
          covariate.labels=c('vocabulary','video','level','difference'), 
          column.labels=c('Routine','Transfer','Routine','Transfer'), 
          keep.stat=c("adj.rsq","n")
          )
```


## Discussion

Although the central theme of this chapter is the evaluation of pedagogical efficacy, the previous analysis demonstrates the importance of addressing the effort induced measurement error in a low stake test environment. The measurement error can result in qualitatively different findings in a randomized control experiment where the true effect is moderately small. Actual text answer to the question is instrumental in identifying the effort induced measurement error, but higher resolution data are required to improve the identification accuracy. In short, it is not enough to know what response the learner gives, but also how he arrives at it. 




## Appendix II


### 1. The sign of Bias from Measurement Error

Let learner's latent knowledge mastery be $\theta$, which is drawn from distribution $F(\theta)$. The true state of $\theta$ is not observable but can only be inferred from an observed binary response $Y$. The link function is $p(\theta)$

To simplify, assume learner can choose a binary effort level $\Gamma$. Further assume When the learner gives up ($\Gamma=0$), the observed response ($Y=0$) is wrong with probability 1. Such assumption rules out item type such as multiple choice where blind guess still has a non-trivial probability of being right.

The data generating process borrows from the Roy model:

(1) Knowing his own $\theta$, the learner assesses the success probability $p(\theta)$. The reward of success is $\beta$ and the expected payoff is $\beta p(\theta)$.  

(2) The learner's mental cost is drawn i.i.d from a distribution $\epsilon \sim G$. The learner only exerts effort if the expected benefit is larger than the cost.
$$\Gamma=I(\beta p(\theta)>\epsilon)$$

(3) If the learner exerts the effort($\Gamma=1$), the response is drawn from $Y \sim B(1, p(\theta))$. Otherwise $Y=0$

To make the comparison explict, denote the observed response under full effort as $Y^*$. The true average treatment effect is thus
$$
\delta_{Y^*} = E_{\theta_1}(p(\theta)) - E_{\theta_0}(p(\theta))
$$

The observed average treatment effect is 

$$
\begin{aligned}
\delta_{Y} &= E_{\theta_1}[E_{\epsilon}[I(Y^*=1, \Gamma=1|\theta)]] - E_{\theta_0}[E_{\epsilon}[I(Y^*=1, \Gamma=1|\theta)]]\\
           &= E_{\theta_1}[p(\theta)G(\beta p(\theta))] -E_{\theta_0}[p(\theta)G(\beta p(\theta))]\\
\end{aligned}
$$

The bias is thus

$$
\delta_{Y} - \delta_{Y^*} = E_{\theta_1}[p(\theta)(G(\beta p(\theta))-1)] -E_{\theta_0}[p(\theta)(G(\beta p(\theta))-1)]
$$



Unfortunately, the sign of the bias cannot be determined. To illustrate, consider the extreme case where there are only two states and the practice moves learner from state $\theta_0$ to state $\theta_1$ with probablity 1. Let $p(\theta_1)= 0.6$ and $p(\theta_0)=0.4$. The true ATE is thus 0.2. If $G(\theta_0) = 0.4$ and $G(\theta_1)=0.6$, the observed ATE is 0.2. If $G(\theta_0)=0.5$ and $G(\theta_1)=0.6$, the observed ATE is 0.16, bias down. If $G(\theta_0)=0.4$ and $G(\theta_1)=0.7$, the observed ATE is 0.26, bias up.

In another mental experiment. If $p(\theta)$ is flat while $G(\beta p(\theta))$ is monotonic. The observed ATE is purely the reduction in measurement error, thus bias upward. If instead $G(\beta p(\theta))$ is flat (neither 0 nor 1), the observed ATE is always a fraction of the true ATE, thus bias downward. 




### 2. Identifcation of Give-up

(1) Add or omit trailing zeros. For example, if the right answer is 120, both 12, 120 and 1200 are admitted as valid attempts.

(2) Add when shall multiply or vice versa. For example, when calculating area with length 6 and width 4, 10 is admitted as a valid attempt.

(3) Apply the wrong formula. For example, when calculating the circumferences of the rectangle with length 6 and width 4, 10(forget to double) or 24(formula of the area) are admitted as valid attempts. 

(4) Calculation mistakes. For example, 13*4 = 42

(5) Fail to understand the question. For example, calculate the circumference and the area of the small rectangle

(6) Typo: For example, 36 as 35


### 3. Summary Statistics

```{r,echo=FALSE,message=FALSE,warning=FALSE, results='asis'}
# for summary statistics
sum_stat_p = ydata_p %>% filter(is_placebo==0)%>% select(gid,t,y, cmt_timelen, giveup,is_blank_ans,is_retain) %>%
  rename(response_time=cmt_timelen,effective_exposure=is_retain)


sum_stat_s = ydata_s %>% filter(is_placebo==0)%>% select(gid,t,y, cmt_timelen, giveup,is_blank_ans,is_retain) %>%
  rename(response_time=cmt_timelen,effective_exposure=is_retain)
```

Routine Assessment - Control

```{r, echo=FALSE,message=FALSE,warning=FALSE, results='asis'}
stargazer(sum_stat_p %>% filter(gid==0&t==1) %>% select(-gid,-t), type = "latex")
```

Routine Assessment - Vocabulary Scaffolding Treatment

```{r, echo=FALSE,message=FALSE,warning=FALSE, results='asis'}
stargazer(sum_stat_p %>% filter(gid==2&t==1) %>% select(-gid,-t), type = "latex")
```


Routine Assessment - Video Scaffolding Treatment

```{r, echo=FALSE,message=FALSE,warning=FALSE, results='asis'}
stargazer(sum_stat_p %>% filter(gid==4&t==1) %>% select(-gid,-t), type = "latex")
```


Transfer Assessment - Control

```{r, echo=FALSE,message=FALSE,warning=FALSE, results='asis'}
stargazer(sum_stat_s %>% filter(gid==0&t==1) %>% select(-gid,-t), type = "latex")
```

Transfer Assessment - Vocabulary Scaffolding Treatment

```{r, echo=FALSE,message=FALSE,warning=FALSE, results='asis'}
stargazer(sum_stat_s %>% filter(gid==2&t==1) %>% select(-gid,-t), type = "latex")
```


Transfer Assessment - Video Scaffolding Treatment

```{r, echo=FALSE,message=FALSE,warning=FALSE, results='asis'}
stargazer(sum_stat_s %>% filter(gid==4&t==1) %>% select(-gid,-t), type = "latex")
```
```{r, echo=FALSE,message=FALSE,warning=FALSE}
# save the result
rm(list=setdiff(ls(),c('proj_dir','mod_1','mod_2','mod_1d','mod_2d','mod_3','mod_4','mod_5','mod_6','mod_7','mod_8','mod_7d','mod_8d')))
output_file_path = paste0(proj_dir,'/_data/02/post_production_data.RData')
save.image(output_file_path)

```

### 4. Alternative Valid Attempts Identification

Alternatively, define submitting a blank answer as an error due to a lack of effort . The pattern of the main analysis remains robust.

Replication of Table 3.5. Null effect.

```{r, echo=FALSE,message=FALSE,warning=FALSE, results='asis'}
rm(list=ls())

proj_dir = getwd()
input_file_path = paste0(proj_dir,'/_data/02/production_data.RData')
load(input_file_path)


y0data = bkp_data %>% filter(eid=='Q_10201056649366') %>% mutate(t=0)
y1pdata = bkp_data %>% filter(eid=='Q_10201056666357') %>% mutate(t=1)
y1sdata = bkp_data %>% filter(eid=='Q_10200351208705') %>% mutate(t=1)

ydata_p = rbind(y0data,y1pdata)
ydata_p = ydata_p %>% transform(d1=as.numeric(gid==2),d2=as.numeric(gid==4),d=as.numeric(gid!=0))

ydata_s = rbind(y0data,y1sdata)
ydata_s = ydata_s %>% transform(d1=as.numeric(gid==2),d2=as.numeric(gid==4),d=as.numeric(gid!=0))

ydata_p = ydata_p %>% transform(dt=d*t,ddt =d2*t, d2t=d2*t,d1t=d1*t)
ydata_s = ydata_s %>% transform(dt=d*t,ddt =d2*t, d2t=d2*t,d1t=d1*t)

ydata_p$y = as.numeric(ydata_p$y==1)
ydata_s$y = as.numeric(ydata_s$y==1)


mod_7d = lm(data=ydata_p%>% filter(is_placebo==0),y~t+dt+ddt)
mod_8d = lm(data=ydata_s%>% filter(is_placebo==0),y~t+dt+ddt)

mod_7 = lm(data=ydata_p%>% filter(is_placebo==0),y~t+d1t+d2t)
mod_8 = lm(data=ydata_s%>% filter(is_placebo==0),y~t+d1t+d2t)

stargazer(mod_7, mod_8, mod_7d,mod_8d,
          header=FALSE,type='latex',
          dep.var.labels = 'Response',
          keep=c('d1t','d2t','dt','ddt'),
          covariate.labels=c('vocabulary','video','level','difference'), 
          column.labels=c('Routine','Transfer','Routine','Transfer'), 
          keep.stat=c("adj.rsq","n")
          )

```

Replication of Table 3.6. Give-up increases sharply for the routine assessment of vocabulary scaffolding group.
```{r, echo=FALSE,message=FALSE,warning=FALSE, results='asis'}
mod_3 = lm(data=ydata_p %>% filter(is_placebo==1),y~d1+d2+t+d1t+d2t)
mod_4 = lm(data=ydata_p %>% filter(is_placebo==1),giveup~d1+d2+t+d1t+d2t)

mod_5 = lm(data=ydata_s %>% filter(is_placebo==1),y~d1+d2+t+d1t+d2t)
mod_6 = lm(data=ydata_s %>% filter(is_placebo==1),giveup~d1+d2+t+d1t+d2t)

stargazer(mod_3, mod_4,mod_5,mod_6, 
          header=FALSE,type='latex', 
          dep.var.labels = c('Response','Giveup','Response','Giveup'),
          keep=c('d1t','d2t'), 
          covariate.labels=c('vocabulary','video'), 
          column.labels=c('Routine','Routine','Transfer','Transfer'), 
          keep.stat=c("adj.rsq","n")
          )
```


Replication of Table 3.7. Slightly different from the main analysis. Video scaffolding appears to be more effective than vocabulary scaffolding, although the difference magnitude is moderated.
```{r, echo=FALSE,message=FALSE,warning=FALSE, results='asis'}
mod_9d = lm(data=ydata_p%>% filter(is_placebo==0&is_retain==1),y~t+dt+ddt)
mod_10d = lm(data=ydata_s%>% filter(is_placebo==0&is_retain==1),y~t+dt+ddt)

mod_9 = lm(data=ydata_p%>% filter(is_placebo==0&is_retain==1),y~t+d1t+d2t)
mod_10 = lm(data=ydata_s%>% filter(is_placebo==0&is_retain==1),y~t+d1t+d2t)

stargazer(mod_9, mod_10, mod_9d,mod_10d,
          header=FALSE,type='latex',
          dep.var.labels = 'Response',
          keep=c('d1t','d2t','dt','ddt'),
          covariate.labels=c('vocabulary','video','level','difference'), 
          column.labels=c('Routine','Transfer','Routine','Transfer'), 
          keep.stat=c("adj.rsq","n")
          )
```
