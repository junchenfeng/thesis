# Identification and Estimation {#estimation}

  


## Identification Assumption

The previous section describes a learning model for an individual. The identification must be performed on the repeated observation because the latent state transition is not reversible and a single observation may not exhaust all states and state transitions[@rabiner1989tutorial]. The critical identification assumption that rationalizes applying a model of individual learner to the population data is:

**Identification Assumption 1:** Users are homogeneous.

Under homogeneity, when the number of users grows, by the law of large numbers, the sample mean converges to the population moment. There are three observable data: the response($\mathbf{Y}$), the effort($\mathbf{E}$), and the exit($\mathbf{H}$). 

The learning analytics have believed that the parameters of the classical Bayesian Knowledge Tracing model is not uniquly identified[@beck2007identifiability;@van2013properties]. They prove that the same learning curve can be generated by different sets of parameters. Although this claim is correct, it does not imply that the underlying hidden markov process is not identified. This section provides a general identification condition for the model of learning through pratices.

To prevent label switching, the zone of proximal development implies the following rank constraint in the the correct rate:

**Assumption 11**: $c^{0,1}_j<c^{1,1}_j<c^{2,1}_j$.

The model also permits recoded partial grade. Recode raw score 0% as 0, raw score 100% as 2, and all partial grade as 1. Furthermore, inspired by the zone of proximal development, if the learner exerts effort, the unprepared learner ($X_t=0$) never produces full correct answer and the mastered learner ($X_t=2$) never produces full wrong answer. This strong assumption is a very informative prior. 
**Assumption 11(a)**: $c^{0,2}_j=c^{2,0}_j=0$

The rank constraint transformed into 

**Assumption 11(a)**: $c^{0,1}_j<(c^{1,1}_j+c^{1,2}_j$, $c^{1,2}_j<c^{2,2}_j$

### One Item Model without Effort Choice and the Stop Choice

First restrict the number of item to one.

Theorem 1: The model for sequence $T$($T\geq3$) is uniquly identified if and only if the model for sequence 2 is uniquly identified, if the data generating process is correctly specified.

Proof of "only if". If the model for sequence $T$ where $T>3$ is idenified, sum over $Y_t$ where $t>2$, it implies that the model for sequence 2 is also identified.

Proof of "if".  Assume the model for sequence 2 is identified, but the model for sequence $T$ is not identified. Let the size of set of parameters that are observation equivalent is m ($m\geq2$): $\Theta_1, \dots, \Theta_m$. Notice that the parameter space is the same for sequence 2 and sequence $T$ under model assumption 3,4,7,10, $\Theta_1, \dots, \Theta_m$ also generates the observation for sequence 2. However, the model for sequence 2 is uniquly identified, therefore $\Theta_1=\dots=\Theta_m=\Theta$.  

Theorem 1 dramatically reduces the complexity of the identification analysis. 


Theorem 2: 
The joint distribution $Y_1,Y_2$ has sufficient statistics of $\{n_{ij},\quad i,j \in J\}$ where $n_{ij}=\sum_{i=1}^N 1(Y_1=i,Y_2=j)$. If the number of the parameters is smaller than or equal to the number of sufficient statistics, the model of sequence 2 is identified.

**TODO: sufficient statistics and identification. GMM like**

Lemma 1: The BKT model is always identified.
Proof: there are four sufficient statistics and four parameters. Apply theorem 2.


It should be noted that the classical assumption $s+g<1$ is not required either. Usually, the hidden markov process is only identified upto the permuation of states. However, BKT model does not have the label switching problem because the label switching is ruled out by the specification of a left right process (assumption 6).  If the sum of the estimated guess rate and slip rate exceeds one, it implies that the model is misspecified.


Lemma 2: The ZPD model is identified.
Proof: there are eight sufficient statistics and eight parameters $\pi_0,\pi_1,c_{01},c_{11},c_{12},c_{22},l_{01},l_{12}$.

The lemma 2 also implies that the tertiary state binary observation model is not identified(4 sufficient stat, 6 parameters). The unrestricted the tertiary state tetiray observation is not identified either(8 sufficient stat, 10 parameters).   

### One Item Model with the Effort Choice and the Stop Choice

The general model still has the structure that the parameter space is the same for any sequence length. Therefore, theorem 1 still holds. The spirit of theorem 2 still holds but the number of sufficient statistics is increased. 


Lemma 3: BKT with effort choice and stop choice is identified.


Lemma 4: ZPD with effort choice and stop choice is identified.



### Multiple Items Model
With multiple items, the property of the same parameter space that generates sequence 2 and sequence T is no longer true. However, the more general property that the same parameter space that generates any sequence is still true. 

Theorem 4: If there are $K$ items, if any particular item occurs not exclusively on the first slot, the number of parameters that can be uniquly identified is limited by a function of the item sequences.
$f(X)g(Y|X)$

$N_x+N_y-1$

Let $M_j$ be the number of observational state of item $j$. For now assume it is constant $M$. Let $C$ be the number of item combinations. Let $S$ be the number of unique item combinations. For example, if the item sequences are (1,2,3) and (3,2,1). C=1 but S=2. All possible response combinations are $M^3$. The reverse sequence adds one extra condition. Therefore the maximum parameters uniquely identified by the structure is $M^3+1$. In this case, if both latent state and observation state are binary, the system has 3+3+1 parameters and there are 8+1 parameters. So the model is identified.

For another example, if the sequence is (1,2,5),(1,3,5),(1,4,5). The observation combinations offer $M^2+3*M-1$ conditions. The state evolving combinations offers $(L-1)^2*3$ conditions. So the total parameters supported are 
$M^2+3*M+(L-1)^2*3-2$

If the latent and observation state are both tetiray, the total parameters supported are 28. For the ZPD model, the total parameters are $2+(4+2)*3=20$. So the model is uniquely identified.

The identification conditions of multiple item learning model has very practical implications. For the model to be uniquly identified, the sequencing of the items cannot be arbitrary. Especially if all learners encounter the same item sequence, the model is likely not identified.


### Identification in the Bayesian Inference 

If one subscribes to the bayesian inference, the uniqueness of the parameter identification is not necessary condition for valid inference. In principle, if there are multiple set of parameters produce the same result, the posterior distribution of the parameters are multi-modal. Although it may be argued that multi-modality poses challenge for convergence, as long as the posterior distribution is a proper distribution. However, for the purpose of calibrating a recommendation system, multi-modality exponentiates the computation complexity of finding the optimal item. Therefore, a valid MAP is more useful and uni-modality is important.  



## Parameter Estimation 

This section discusses the statistical method to identify the parameters. 

### Likelihood Function
Let $\mathbf{Y^{(i)}} = \{Y^{(i)}_{A(1),1}, \dots, Y^{(i)}_{A(t),t}\}$,  $\mathbf{E^{(i)}} = \{E^{(i)}_{A(1),1}, \dots, E^{(i)}_{A(1),1}\}$, and $\mathbf{X^{(i)}} = \{X^{(i)}_1, \dots, X^{(i)}_t\}$ where $i$ is the learner id. The likelihood of observing the data $\{\mathbf{Y^{(i)}},\mathbf{E^{(i)}},H_t\}$ is 

$$
P(\mathbf{Y^{(i)}},\mathbf{E^{(i)}},H^(i)_T) = \sum_{X^{(i)=0}}^1\dots\sum_{X^{(i)}_T=0}^1 P(H^{(i)}_T|\mathbf{X^{(i)}})P(\mathbf{Y^{(i)}}|\mathbf{E^{(i)}},\mathbf{X^{(i)}})P(\mathbf{E^{(i)}},\mathbf{X^{(i)}})
$$

Since the start of the practice sequence is usually logged, the issue of left-censor can be ignored. The probability of observing event $H^{(i)}_T$ is 

$$
P(H^{(i)}_T|\mathbf{X^{(i)}}) = [\prod_{t=1}^{T-1} (1-h_t^{X^{(i)}_t}))] [(h_T^{X^{(i)}_T})^{H^{(i)}_T}(1-h_T^{X^{(i)}_T})^{1-H^{(i)}_T}]
$$

The likelihood of the observed response is

$$
\begin{aligned}
P(mathbf{Y^{(i)}}|\mathbf{X^{(i)}},mathbf{E^{(i)}}) &= \prod_{t=1}^T [1-s_{(A^{(i)}(t))}]^{I(Y^{(i)}_t=1,X^{(i)}_tE^{(i)}_t=1)}[s_{(A^{(i)}(t))}]^{I(Y^{(i)}_t=0,X^{(i)}_tE^{(i)}_t=1)}\\
&\quad[1-g_{(A^{(i)}(t))}]^{I(Y^{(i)}_t=0,X^{(i)}_tE^{(i)}_t=0)}[g_{(A^{(i)}(t))}]^{I(Y^{(i)}_t=1,X^{(i)}_tE^{(i)}_t=0)})]
\end{aligned}
$$

The joint likelihood of the effort and latent state variable is 

$$
\begin{aligned}
P(\mathbf{E^{i}},\mathbf{X^{i}}) &= (\pi_1)^{I(X^{(i)}_1=1)}(1-\pi_1)^{I(X^{(i)}_1=0)}\\
&\quad\prod_{t=2}^T(\ell_A^{(i)}(t))^{I(X^{(1)}_t=1,X^{(i)}_{t-1}=0,E^{(i)}_{t-1}=1)}(1-\ell_{A^{(i)}(t)})^{I(X^{(i)}_t=0,X^{(1)}_{t-1}=0,E^{(i)}_{t-1}=1)}\\
&\quad(e^0_{A^{(i)}_t})^{I(X^{(1)}_{t}=0,E^{(i)}_{t}=1))}(1-e^0_{A^{(i)}_t})^{I(X^{(1)}_{t}=0,E^{(i)}_{t}=0))}(e^1_{A^{(i)}(t)})^{I(X^{(1)}_{t}=1,E^{(i)}_{t}=1))}(1-e^1_{A^{(i)}(t)})^{I(X^{(1)}_{t}=1,E^{(i)}_{t}=0))}
\end{aligned}
$$



### The MCMC Estimator

It is not a new idea to use MCMC in estimating HMM[@scott2002bayesian]. The general idea of MCMC estimation for HMM model is to first augment the hidden state then update the model parameter with Gibbs sampler.

#### Why not Expectation-Maximization(EM)
In principle, the parameters can be estimated by Expectation-Maximization(EM) method because the likelihood function is known. The disadvantage of EM algorithm is its difficulty in implementation. Although modern constrained optimization technique only requires gradient to converge efficiently, it is still a daunting task to write down the gradient of the likelihood function. In addition, EM algorithm also requires specifying the prior distribution for the parameter, which puts it on an equal footing with the MCMC algorithm.

The major advantage of the Em algorithm is its computation speed, which makes it feasible in a large data set. However, the computation power is getting cheaper. The most expensive part of the MCMC algorithm is augmenting the latent state, which can be parallelized. Therefore the MCMC algorithm is not  prohibitively computational expensive for large dataset. Another important benefit of the MCMC algorithm is it recovers the distribution of the parameter, thus it allows for fast online computation of the credible interval of the learner's knowledge mastery.

In addition, if the model is correctly specified, the MCMC algorithm, in theory, converges to the true parameter distribution and a consistent point estimator with probability 1. The same convergence property does not hold for the EM algorithm. That said, for the general model specified in the thesis, under sufficient identification assumptions, simulations show that the MCMC algorithm and EM algorithm estimate the parameter with similar precision.

In short, the MCMC algorithm and the EM algorithm behave quite similarly but the MCMC algorithm is favored for its extensibility 

#### The Forward Recursion and Backward Sampling Algorithm

Various latent state sampling schemes have been proposed, on which Scott[-@scott2002bayesian] provides an extensive survey. This chapter uses forward recursion and backward sampling scheme.

Let $\tilde{\pi}_t(i)$ denote the posterior marginal state density. $\tilde{\pi}_t(i) = P(X_t=i|Y_1,\dots,Y_t, E_1,\dots,E_t,H_t, \Theta)$. Let $P_t$ denote the posterior state transition matrix and $P_{t}(i,j)$ be the $(i,j)$th element of transit matrix. $P_{t}(i,j) = P(X_{t-1}=i,X_t=j|Y_1,\dots,Y_t, E_1,\dots,E_t,H_t,\Theta)$

The forward recursion calculates the posterior state density and state transition matrix by the following algorithm.

(1) Initialize the marginal state density by $\tilde{\pi}_t(i) = \frac{P(X_1=i)P(E_1|X_1=i)P(H_1|X_1=i)P(Y_1|X_1=i,E_1)}{\sum_{j=0}^1P(X_1=j)P(E_1|X_1=j)P(H_1|X_1=j)P(Y_1|X_1=j,E_1)}$

(2) Calculate $P_t(i,j)=\tilde{\pi}_{t-1}(i)P(X_t=j|X_{t-1}=i,E_{t-1})P(Y_t|X_t,E_t)P(E_t|X_t)\prod_{k=1}^T(1-h_k^{X_k})$

(3) Calculate $\tilde{\pi}_t(j)=\sum_{i}P_t(i,j)$



After the forward recursion recovers the posterior marginal state density and posterior state transition matrix. Augment the data with latent state by backward sampling:

(1) Initialize the state from last sequence position $T$ by drawing from a Bernoulli distribution with $p=\tilde{\pi}_T(1)$

(2) Give the $X_{t+1}=j$, draw $X_t$ from a Bernoulli distribution with $p=\frac{P_t(1,j)}{\sum_{i=0}^TP_t(i,j)}$

The second step shall permutate the state by $P(X_t|X_{t+1}=j,\mathbf{Y},\mathbf{E},H_T,\Theta)$. By the assumption of first-order Markov chain $X_t\perp\!\!\!\perp Y_{t+2},\dots,Y_T,E_{t+2},\dots,E_T|X_{t+1}=j$, it is equivalent to $P(X_t|X_{t+1}=j,Y_1,\dots,Y_{t+1},E_1,\dots,E_{t+1},H_T|\Theta)$. The last quantity is just $\frac{P_t(i,j)}{\sum_{i=0}^TP_t(i,j)}$. 


Readers familiar with the HMM literature may ask why the backward recursion and forward sampling scheme[@chib1996calculating] is not considered. The recursion trick cannot be used to estimate the hybrid model because the event of observing $H_t$ depends on the survival probability as a function of ${X_1,\dots,X_{t-1}}$ and the backward smoothing factor $P(X_t|X_{t+1},Y_{t+1},\dots,Y_{T})$ cannot be calculated recursively. One could still calculate the likelihood by brute force but it is quite expensive when the chain is long. 


#### Gibbs Sampler for Conjugate Posterior Parameter Update
**TODO: add in the j notation**
Once the state of knowledge mastery $X_t$ is sampled, the parameter is updated by Gibbs sampler(derived in Appendix 1). All parameters have a conjugate Beta prior.

(1) The prior density distribution is sampled from Beta($\beta^{\pi}_1+n^{\pi}_{1}$,$\beta^{\pi}_0+n^{\pi}_{0}$) where $\beta^{\pi}_0$, $\beta^{\pi}_1$ are prior parameters, $n^{\pi}_1 = \sum_i(X^i_1=1)$ and  $n^{\pi}_{0} = \sum_i(X^i_1=0)$

(2) The learn rate is sampled from Beta($\beta^l_1+n^l_{0,1}$,$\beta^l_0+n^l_{0,0}$) where $\beta^l_0$, $\beta^l_1$ are prior parameters, $n^l_{0,1} = \sum_i\sum_{t=2}^{T_i}(X^i_t=1,X^i_{t-1}=0,E^i_{t-1}=1)$ and  $n^l_{0,0} = \sum_i\sum_{t=2}^{T_i}(X^i_t=0,X^i_{t-1}=0,E^i_{t-1}=1)$

(3) The slip rate is sampled  from Beta($\beta^s_1+n^s_{0,1}$,$\beta^s_0+n^s_{1,1}$) where $\beta^s_0$, $\beta^s_1$ are prior parameters, $n^s_{0,1} = \sum_i\sum_t(Y^i_t=1,X^i_t=1,E^i_t=1)$ and  $n^s_{0,0} = \sum_i\sum_t(Y^i_t=0,X^i_t=1,E^i_t=1)$

(4) The guess rate is sampled from Beta($\beta^g_1+n^g_{0,0}$,$\beta^g_0+n^g_{1,0}$) where $\beta^g_0$, $\beta^g_1$ are prior parameters, $n^g_{1,0} = \sum_i\sum_t(Y^i_t=1,X^i_tE^i_t=0)$ and  $n^s_{1,0} = \sum_i\sum_t(Y^i_t=1,X^i_tE^i_t=0)$

(5) The effort rate is sample from
Beta($\beta^e_1+n^e_{1,t,j}$,$\beta^e_0+n^e_{0,t,j}$) where $\beta^e_0$, $\beta^e_1$ are prior parameters and $n^e_{k,j} = \sum_i\sum_t(E^i_t=k,X^i_t=j)$ 

#### Adaptive Rejection Sampler for the Conjugate Posterior Parameter Update
Because the general model specifies a hazard model whose hazard rate depends on time variant covariates, the MCMC algorithm proposed for the classical proportional hazard rate model with time-invariant covariates does not work. However, since the relevant likelihood is log-concave, the hazard rate can be estimated with adaptive rejection sampling method[@gilks1992adaptive]. 

Following the tradition of the proportional hazard model, the hazard rate $h^X_t$ is specified as 
$$
h^X_t = \lambda e^{\beta_1 X_t + \beta_2 t + \beta_3 tX_t} = \lambda e^{\mathbf{X}_{i,t}\beta}
$$
The discrete time likelihood for the stop decision is 

$$
L = \prod_{i=1}^N \prod_{t=1}^{T_i} [1-h_t^{X_{i,t}}]^{1-H_{i,t}}[h_t^{X_{i,t}}]^{H_{i,t}}
$$
The prior distribution of the parameters are chosen to be uniform distribution to facilitate the posterior draw and justify assigning zero mass to certain interval on the parameter space so as to bound the hazard rate between [0,1]. Under uniform prior, the posterior distributions of the parameters all proportional to the following quantity 
$$
\sum_{i=1}^N \sum_{t=1}^{T_i} (1-H_{i,t})log([1-\lambda e^{\mathbf{X}_{i,t}\beta}])+H_{i,t} [log(\lambda)+\mathbf{X}_{i,t}\beta]
$$
It can be easily proved that (in Appendix 2) the posterior distribution is log concave. Thus, by constructing an upper hall and a lower hull to sandwich the true posterior distribution, the adaptive rejection sampler reduces the computational expanses to draw from a non-standard distribution than the standard rejection method. A brief description of the adaptive rejection sampling method appears in Appendix 3. Because the hazard rate is bounded, in each iteration, calculate the upper bound on the parameter by solving the following inequalities with other parameters as given

$$
\lambda e^{\beta_1 X_t + \beta_2 t + \beta_3 tX_t} < 1
$$
Lower bound only exists for $\lambda$ that $\lambda>0$. The common practice is to initialize the sampler with two values right on the lower bound and the upper bound. However, when the data size grows, such initial choice can create numeric overflow. Therefore, the state effect $\beta_1$ is bounded between $[-0.8, \min(0.8, \bar{\beta_1})]$ (either half or double when exponentiated), the duration dependence($\beta_2$) is bounded between$[-0.3, \min(0.5, \bar{\beta_2})]$, and the interaction effect($\beta_3$) is bounded between $[-0.3, \min(0.3, \bar{\beta_3})]$. If such bound still causes overflow, shrink the bound symmetricaly by an increment of 0.05 until the initial proposal is accepted. To further enhance the accuracy of the draws, draw five times from the posterior distribution before update the parameter. Consequently, the empirical analysis rarely draws value close to the bound.




## Appendix

### 1. Derivation of the Gibbs Sampler
**To be modified**
Given the latent state $X$ and parameter $\theta$, the likelihood function for the observed data is

$$
\begin{aligned}
P(D|\theta,X) &=  \prod_{i=1}^N \\
&= \prod_{i=1}^N \prod_{t=1}^{T_i} [h_{t,0}^{(E^i_t=1,Y^i_t=0)}(1-h_{t,0})^{(E^i_t=0,Y^i_t=0)}h_{t,1}^{(E^i_t=1,Y^i_t=1)}(1-h_{t,1})^{(E^i_t=0,Y^i_t=1)}]\\
&\hspace{1.8cm}[(1-s)^{(X^i_t=1,Y^i_t=1)}s^{(X^i_t=1,Y^i_t=0)}(1-g)^{(X^i_t=0,Y^i_t=0)}g^{(X^i_t=0,Y^i_t=1)}]\\ &\hspace{1.8cm}\{[\ell^{(X^i_{t-1}=0,X^i_t=1)}(1-\ell)^{(X^i_{t-1}=0,X^i_t=0})]^{t!=1}[\pi^{X^i_1=1}(1-pi)^{X^i_1=0}]^{t=1}\}\\
&= [h_{t,0}^{\sum_i\sum_t(E^i_t=1,Y^i_t=0)}(1-h_{t,0})^{\sum_i\sum_t(E^i_t=0,Y^i_t=0)}h_{t,1}^{\sum_i\sum_t(E^i_t=1,Y^i_t=1)}(1-h_{t,1})^{\sum_i\sum_t(E^i_t=0,Y^i_t=1)}]\\
&\hspace{1.8cm}[(1-s)^{\sum_i\sum_t(X^i_t=1,Y^i_t=1)}s^{\sum_i\sum_t(X^i_t=1,Y^i_t=0)}(1-g)^{\sum_i\sum_t(X^i_t=0,Y^i_t=0)}g^{\sum_i\sum_t(X^i_t=0,Y^i_t=1)}]\\ 
&\hspace{1.8cm}\{[\ell^{\sum_i\sum_{t=2}^{T_i}(X^i_{t-1}=0,X^i_t=1)}(1-\ell)^{\sum_i\sum_{t=2}^{T_i}(X^i_{t-1}=0,X^i_t=0})]^{t!=1}[\pi^{\sum_iX^i_1=1}(1-pi)^{\sum_iX^i_1=0}]^{t=1}\}
\end{aligned}
$$

Notice that since all parameters have a beta prior, it is easy to derive the Gibbs Sampler scheme from the here.

### 2. Log-Concavity of the Posterior Distribution of the Hazard Model Parameters

For $\lambda$

$$
\begin{aligned}
\frac{\partial \ell}{\partial \lambda} &= \sum_{i=1}^N \sum_{t=1}^{T_i} -\frac{(1-H_{i,t})e^{\mathbf{X}_{i,t}\beta}}{1-\lambda e^{\mathbf{X}_{i,t}\beta}}+\frac{H_{i,t}}{\lambda}\\
\frac{\partial^2 \ell}{\partial \lambda^2} &= \sum_{i=1}^N \sum_{t=1}^{T_i}-\frac{(1-H_{i,t})e^{2\mathbf{X}_{i,t}\beta}}{(1-\lambda e^{\mathbf{X}_{i,t}\beta})^2}-\frac{H_{i,t}}{\lambda^2}
\end{aligned}
$$

Because $H_{i,t}\geq 0$, $1-H_{i,t}\geq 0$, $H_{i,t}\geq 0$, $e^{\mathbf{X}_{i,t}\beta}\geq0$. Therefore, $\frac{\partial^2 \ell}{\partial \lambda^2} <0 \quad \forall \beta_j$.

For $\beta_j$
$$
\begin{aligned}
\frac{\partial \ell}{\partial \beta_k} &= \sum_{i=1}^N \sum_{t=1}^{T_i} [-\frac{(1-H_{i,t})\lambda e^{\mathbf{X}_{i,t}\beta}}{1-\lambda e^{\mathbf{X}_{i,t}\beta}}+H_{i,t}]X_{i,t,k}\\
\frac{\partial^2 \ell}{\partial \beta_k^2} &= \sum_{i=1}^N \sum_{t=1}^{T_i}-[\frac{1}{1-\lambda e^{\mathbf{X}_{i,t}\beta}}+\frac{e^{\mathbf{X}_{i,t}\beta}\lambda}{(1-\lambda e^{\mathbf{X}_{i,t}\beta})^2}]e^{\mathbf{X}_{i,t}\beta}X_{i,t,k}^2(1-H_{i,t})\lambda
\end{aligned}
$$
Because $1-H_{i,t}\geq 0$, $\lambda\geq 0$, $\frac{\partial^2 \ell}{\partial \beta_k^2} <0 \quad \forall \beta_j$.


### 3. Adaptive Rejection Sampling Method


(1) Choose a few values $x_j$ from the domain. Construct the upper hull and the lower hull of the target distribution function $f(x)$ by piecewise linear functions of 

$$
\begin{aligned}
u(x) &= f(x_j)+f'(x_j)(x-x_j)\\
l(x) &= \frac{(x_{j+1}-x)f(x_j)+(x-x_j)f(x_{j+1})}{x_{j+1}-x_j}
\end{aligned}
$$

defined over intervals $x\in(z_{j-1},z_j)$ where 

$$
z_j = \frac{f(x_j)-f(x_{j+1})-x_{j+1}f'(x_{j+1})+x_jf'(x_j)}{f'(x_j)-f'(x_{j+1})}
$$

(2) Sample new value of $x^*$ by the probability of $s(x)$ where

$$
s(x) =\frac{exp(u(x))}{\int_{D_x} exp(u(x)) dx} 
$$

(3) Sample $w$ independently from uniform(0,1). Accept the new value$x^*$ if

$$
w \leq e^{l(x^*)-u(x^*)}
$$
Otherwise, accept the new value$x^*$ if 
$$
w \leq e^{f(x^*)-u(x^*)}
$$
Otherwise reject $x^*$ and draw again.

(4) If $x^*$ is accepted, add to the list of $x_j$ for the next draw.

Because the target distribution function is concave, it follows that $f'(x_1)>0$ and $f'(x_J)<0$. The initial value thus cannot be sampled randomly.

### 4. Convergence of the MCMC
**To be added**
